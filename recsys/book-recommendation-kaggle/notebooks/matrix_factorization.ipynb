{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization for Book Recommendations\n",
    "\n",
    "This notebook implements classical matrix factorization for collaborative filtering to recommend similar books based on user ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DATA_DIR = \"../data\"\n",
    "\n",
    "# Initialize wandb\n",
    "run = wandb.init(\n",
    "    project=\"book-recommendation-kaggle\",\n",
    "    group=\"matrix-factorization\",\n",
    "    job_type=\"train\",\n",
    "    save_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "books = pd.read_csv(f\"{DATA_DIR}/Books.csv.zip\", compression=\"zip\")\n",
    "ratings = pd.read_csv(f\"{DATA_DIR}/Ratings.csv.zip\", compression=\"zip\")\n",
    "users = pd.read_csv(f\"{DATA_DIR}/Users.csv.zip\", compression=\"zip\")\n",
    "\n",
    "print(f\"Books: {len(books):,}\")\n",
    "print(f\"Ratings: {len(ratings):,}\")\n",
    "print(f\"Users: {len(users):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for users and books with at least 5 interactions\n",
    "print(\"\\n=== Filtering for Active Users and Popular Books ===\")\n",
    "\n",
    "# Count interactions per user and book\n",
    "user_counts = ratings.groupby(\"User-ID\").size()\n",
    "book_counts = ratings.groupby(\"ISBN\").size()\n",
    "\n",
    "min_interactions = 1\n",
    "\n",
    "# Get active users and popular books\n",
    "active_users = user_counts[user_counts >= min_interactions].index\n",
    "popular_books = book_counts[book_counts >= min_interactions].index\n",
    "\n",
    "print(\n",
    "    f\"Active users (≥{min_interactions} ratings): {len(active_users):,} / {len(user_counts):,}\"\n",
    ")\n",
    "print(\n",
    "    f\"Popular books (≥{min_interactions} ratings): {len(popular_books):,} / {len(book_counts):,}\"\n",
    ")\n",
    "\n",
    "# Filter ratings\n",
    "filtered_ratings = ratings[\n",
    "    (ratings[\"User-ID\"].isin(active_users)) & (ratings[\"ISBN\"].isin(popular_books))\n",
    "].copy()\n",
    "\n",
    "print(\n",
    "    f\"Filtered ratings: {len(filtered_ratings):,} / {len(ratings):,} ({len(filtered_ratings) / len(ratings) * 100:.1f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create compact user and book indices\n",
    "print(\"\\n=== Creating Compact Indices ===\")\n",
    "\n",
    "# Create mappings\n",
    "unique_users = sorted(filtered_ratings[\"User-ID\"].unique())\n",
    "unique_books = sorted(filtered_ratings[\"ISBN\"].unique())\n",
    "\n",
    "user_to_idx = {user_id: idx for idx, user_id in enumerate(unique_users)}\n",
    "book_to_idx = {book_id: idx for idx, book_id in enumerate(unique_books)}\n",
    "idx_to_user = {idx: user_id for user_id, idx in user_to_idx.items()}\n",
    "idx_to_book = {idx: book_id for book_id, idx in book_to_idx.items()}\n",
    "\n",
    "# Map to indices\n",
    "filtered_ratings[\"user_idx\"] = filtered_ratings[\"User-ID\"].map(user_to_idx)\n",
    "filtered_ratings[\"book_idx\"] = filtered_ratings[\"ISBN\"].map(book_to_idx)\n",
    "\n",
    "n_users = len(unique_users)\n",
    "n_books = len(unique_books)\n",
    "\n",
    "print(f\"Matrix dimensions: {n_users:,} users × {n_books:,} books\")\n",
    "print(f\"Sparsity: {(1 - len(filtered_ratings) / (n_users * n_books)) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create book metadata mapping for recommendations\n",
    "print(\"\\n=== Creating Book Metadata Mapping ===\")\n",
    "\n",
    "# Filter books to only those in our filtered dataset\n",
    "book_metadata = books[books[\"ISBN\"].isin(unique_books)].copy()\n",
    "book_metadata[\"book_idx\"] = book_metadata[\"ISBN\"].map(book_to_idx)\n",
    "\n",
    "# Create title to index mapping\n",
    "title_to_idx = book_metadata.set_index(\"Book-Title\")[\"book_idx\"].to_dict()\n",
    "idx_to_title = book_metadata.set_index(\"book_idx\")[\"Book-Title\"].to_dict()\n",
    "idx_to_author = book_metadata.set_index(\"book_idx\")[\"Book-Author\"].to_dict()\n",
    "\n",
    "print(f\"Book metadata for {len(book_metadata):,} books\")\n",
    "print(\"\\nSample books:\")\n",
    "for i, (title, author) in enumerate(\n",
    "    zip(book_metadata[\"Book-Title\"].head(), book_metadata[\"Book-Author\"].head())\n",
    "):\n",
    "    print(f\"  {title} by {author}\")\n",
    "    if i >= 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rating Matrix Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feedback matrix\n",
    "print(\"\\n=== Creating Rating Matrix ===\")\n",
    "\n",
    "# Create sparse matrix\n",
    "user_indices = filtered_ratings[\"user_idx\"].values\n",
    "book_indices = filtered_ratings[\"book_idx\"].values\n",
    "rating_values = filtered_ratings[\"Book-Rating\"].values\n",
    "\n",
    "# Create sparse rating matrix\n",
    "rating_matrix_sparse = csr_matrix(\n",
    "    (rating_values, (user_indices, book_indices)), shape=(n_users, n_books)\n",
    ")\n",
    "\n",
    "print(f\"Sparse rating matrix: {rating_matrix_sparse.shape}\")\n",
    "print(f\"Non-zero entries: {rating_matrix_sparse.nnz:,}\")\n",
    "print(f\"Memory usage: {rating_matrix_sparse.data.nbytes / 1024**2:.1f} MB\")\n",
    "\n",
    "# Analyze feedback distribution\n",
    "print(f\"\\nRating distribution:\")\n",
    "print(filtered_ratings[\"Book-Rating\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_size=128, dropout=0.1):\n",
    "        super(GMF, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_size)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_size)\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Linear(embedding_size, embedding_size // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embedding_size // 4, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, user_ids, item_ids):\n",
    "        user_embed = self.user_embedding(user_ids)\n",
    "        item_embed = self.item_embedding(item_ids)\n",
    "        element_product = user_embed * item_embed\n",
    "        output = self.sequential(element_product)\n",
    "        output = torch.sigmoid(output)\n",
    "        return output.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_users,\n",
    "        num_items,\n",
    "        embedding_size=128,\n",
    "        hidden_layers=[64, 32],\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super(MLP, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_size)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_size)\n",
    "        mlp_layers = []\n",
    "        input_size = embedding_size * 2\n",
    "        for hidden_size in hidden_layers:\n",
    "            mlp_layers.append(nn.Linear(input_size, hidden_size))\n",
    "            mlp_layers.append(nn.ReLU())\n",
    "            mlp_layers.append(nn.Dropout(dropout))\n",
    "            input_size = hidden_size\n",
    "        mlp_layers.append(nn.Linear(hidden_layers[-1], 1))\n",
    "        self.sequential = nn.Sequential(*mlp_layers)\n",
    "\n",
    "    def forward(self, user_ids, item_ids):\n",
    "        user_embed = self.user_embedding(user_ids)\n",
    "        item_embed = self.item_embedding(item_ids)\n",
    "        concat_embed = torch.cat((user_embed, item_embed), dim=1)\n",
    "        output = self.sequential(concat_embed)\n",
    "        output = torch.sigmoid(output)\n",
    "        return output.view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralCollaborativeFiltering(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        gmf,\n",
    "        mlp,\n",
    "    ):\n",
    "        super(NeuralCollaborativeFiltering, self).__init__()\n",
    "        self.gmf = gmf\n",
    "        self.mlp = mlp\n",
    "\n",
    "    def forward(self, user_ids, item_ids):\n",
    "        gmf_output = self.gmf(user_ids, item_ids)\n",
    "        mlp_output = self.mlp(user_ids, item_ids)\n",
    "        combined_output = (gmf_output + mlp_output) / 2\n",
    "        return combined_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "print(\"\\n=== Preparing Training Data ===\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "user_ids = torch.LongTensor(filtered_ratings[\"user_idx\"].values)\n",
    "book_ids = torch.LongTensor(filtered_ratings[\"book_idx\"].values)\n",
    "target_tensor = torch.FloatTensor(filtered_ratings[\"Book-Rating\"].values) / 10.0\n",
    "\n",
    "# Create train/validation split (80/20)\n",
    "n_total = len(user_ids)\n",
    "n_train = int(0.8 * n_total)\n",
    "\n",
    "# Random shuffle\n",
    "indices = torch.randperm(n_total)\n",
    "train_indices = indices[:n_train]\n",
    "val_indices = indices[n_train:]\n",
    "\n",
    "# Split data\n",
    "train_users = user_ids[train_indices]\n",
    "train_books = book_ids[train_indices]\n",
    "train_ratings = target_tensor[train_indices]\n",
    "\n",
    "val_users = user_ids[val_indices]\n",
    "val_books = book_ids[val_indices]\n",
    "val_ratings = target_tensor[val_indices]\n",
    "\n",
    "print(f\"Training samples: {len(train_users):,}\")\n",
    "print(f\"Validation samples: {len(val_users):,}\")\n",
    "\n",
    "# Model parameters\n",
    "n_factors = 64\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4\n",
    "batch_size = 8192\n",
    "n_epochs = 50\n",
    "\n",
    "print(f\"\\nModel configuration:\")\n",
    "print(f\"  Factors: {n_factors}\")\n",
    "print(f\"  Learning rate: {learning_rate}\")\n",
    "print(f\"  Batch size: {batch_size:,}\")\n",
    "print(f\"  Epochs: {n_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "gmf = GMF(n_users, n_books).to(device)\n",
    "mlp = MLP(n_users, n_books, hidden_layers=[128, 64, 32]).to(device)\n",
    "gmf_optimizer = optim.AdamW(\n",
    "    gmf.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    ")\n",
    "mlp_optimizer = optim.AdamW(\n",
    "    mlp.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    ")\n",
    "gmf_scheduler = optim.lr_scheduler.StepLR(gmf_optimizer, step_size=15, gamma=0.5)\n",
    "mlp_scheduler = optim.lr_scheduler.StepLR(mlp_optimizer, step_size=15, gamma=0.5)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(f\"GMF model parameters: {sum(p.numel() for p in gmf.parameters()):,}\")\n",
    "print(f\"MLP model parameters: {sum(p.numel() for p in mlp.parameters()):,}\")\n",
    "\n",
    "# Move data to device\n",
    "train_users = train_users.to(device)\n",
    "train_books = train_books.to(device)\n",
    "train_ratings = train_ratings.to(device)\n",
    "\n",
    "val_users = val_users.to(device)\n",
    "val_books = val_books.to(device)\n",
    "val_ratings = val_ratings.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gmf(model, criterion, optimizer, batch_size=64, num_epochs=5):\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        i = 0\n",
    "\n",
    "        for i in range(0, len(train_users), batch_size):\n",
    "            # Get batch\n",
    "            batch_users = train_users[i : i + batch_size]\n",
    "            batch_books = train_books[i : i + batch_size]\n",
    "            batch_targets = train_ratings[i : i + batch_size]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(batch_users, batch_books)\n",
    "            loss = criterion(predictions, batch_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                diff = torch.abs(predictions - batch_targets).sum().item()\n",
    "                print(\n",
    "                    f\"Batch [{i + 1}/{len(train_users)}], Loss: {loss.item()}, Avg. Diff: {(diff / len(batch_targets))}\"\n",
    "                )\n",
    "\n",
    "            i = i + 1\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(\n",
    "            f\"GMF Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss / len(train_users)}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp(model, criterion, optimizer, batch_size=64, num_epochs=5):\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        i = 0\n",
    "        for i in range(0, len(train_users), batch_size):\n",
    "            # Get batch\n",
    "            batch_users = train_users[i : i + batch_size]\n",
    "            batch_books = train_books[i : i + batch_size]\n",
    "            batch_targets = train_ratings[i : i + batch_size]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(batch_users, batch_books)\n",
    "            loss = criterion(predictions, batch_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                diff = torch.abs(predictions - batch_targets).sum().item()\n",
    "                print(\n",
    "                    f\"Batch [{i + 1}/{len(train_users)}], Loss: {loss.item()}, Avg. Diff: {(diff / len(batch_targets))}\"\n",
    "                )\n",
    "\n",
    "            i = i + 1\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(\n",
    "            f\"MLP Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss / len(train_users)}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "print(\"Training GMF...\")\n",
    "train_gmf(gmf, criterion, gmf_optimizer)\n",
    "\n",
    "print(\"Training MLP...\")\n",
    "train_mlp(mlp, criterion, mlp_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, users, books, ratings, batch_size=8192, criterion=criterion):\n",
    "    \"\"\"Evaluate model on given data\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    n_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(users), batch_size):\n",
    "            batch_users = users[i : i + batch_size]\n",
    "            batch_books = books[i : i + batch_size]\n",
    "            batch_targets = ratings[i : i + batch_size]\n",
    "\n",
    "            predictions = model(batch_users, batch_books)\n",
    "            loss = criterion(predictions, batch_targets)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "    return total_loss / n_batches if n_batches > 0 else 0\n",
    "\n",
    "\n",
    "# Training loop\n",
    "print(\"\\n=== Starting Training ===\")\n",
    "\n",
    "model = NeuralCollaborativeFiltering(gmf, mlp)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    ")\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.5)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float(\"inf\")\n",
    "patience_counter = 0\n",
    "patience = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    n_batches = 0\n",
    "\n",
    "    # Training batches\n",
    "    for i in range(0, len(train_users), batch_size):\n",
    "        # Get batch\n",
    "        batch_users = train_users[i : i + batch_size]\n",
    "        batch_books = train_books[i : i + batch_size]\n",
    "        batch_targets = train_ratings[i : i + batch_size]\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch_users, batch_books)\n",
    "        loss = criterion(predictions, batch_targets)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    # Calculate losses\n",
    "    train_loss = epoch_loss / n_batches\n",
    "    val_loss = evaluate_model(model, val_users, val_books, val_ratings, batch_size)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    scheduler.step()\n",
    "\n",
    "    # Log to wandb\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Print progress\n",
    "    if (epoch + 1) % 5 == 0 or epoch < 5:\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1:2d}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, LR = {optimizer.param_groups[0]['lr']:.2e}\"\n",
    "        )\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save(model.state_dict(), \"best_matrix_factorization.pth\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "print(f\"\\nTraining completed! Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label=\"Train Loss\", alpha=0.7)\n",
    "plt.plot(val_losses, label=\"Validation Loss\", alpha=0.7)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Training Progress\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_losses[5:], label=\"Train Loss (after epoch 5)\", alpha=0.7)\n",
    "plt.plot(val_losses[5:], label=\"Validation Loss (after epoch 5)\", alpha=0.7)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Training Progress (Zoomed)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Log plot to wandb\n",
    "wandb.log({\"training_progress\": wandb.Image(plt)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book Similarity Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(\"best_matrix_factorization.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Extract book embeddings\n",
    "print(\"\\n=== Creating Book Similarity Index ===\")\n",
    "with torch.no_grad():\n",
    "    book_embeddings = model.get_book_embeddings().cpu().numpy()\n",
    "\n",
    "print(f\"Book embeddings shape: {book_embeddings.shape}\")\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "print(\"Computing book similarity matrix...\")\n",
    "book_similarity_matrix = cosine_similarity(book_embeddings)\n",
    "print(f\"Similarity matrix shape: {book_similarity_matrix.shape}\")\n",
    "print(f\"Memory usage: {book_similarity_matrix.nbytes / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_similar_books(book_title, k=10):\n",
    "    \"\"\"\n",
    "    Recommend k most similar books to the given book title\n",
    "\n",
    "    Args:\n",
    "        book_title (str): Title of the book\n",
    "        k (int): Number of recommendations to return\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples (title, author, similarity_score)\n",
    "    \"\"\"\n",
    "    # Check if book exists\n",
    "    if book_title not in title_to_idx:\n",
    "        # Try partial matching\n",
    "        matching_titles = [\n",
    "            title\n",
    "            for title in title_to_idx.keys()\n",
    "            if book_title.lower() in title.lower()\n",
    "        ]\n",
    "        if matching_titles:\n",
    "            print(f\"Book '{book_title}' not found. Did you mean one of these?\")\n",
    "            for i, title in enumerate(matching_titles[:5]):\n",
    "                print(f\"  {i + 1}. {title}\")\n",
    "            return []\n",
    "        else:\n",
    "            print(f\"Book '{book_title}' not found in the dataset.\")\n",
    "            return []\n",
    "\n",
    "    # Get book index\n",
    "    book_idx = title_to_idx[book_title]\n",
    "\n",
    "    # Get similarity scores for this book\n",
    "    similarities = book_similarity_matrix[book_idx]\n",
    "\n",
    "    # Get top k+1 most similar books (excluding the book itself)\n",
    "    top_indices = np.argsort(similarities)[::-1][1 : k + 1]  # Skip first (itself)\n",
    "\n",
    "    # Build recommendations\n",
    "    recommendations = []\n",
    "    for idx in top_indices:\n",
    "        try:\n",
    "            title = idx_to_title[idx]\n",
    "            author = idx_to_author.get(idx, \"Unknown Author\")\n",
    "            similarity = similarities[idx]\n",
    "            recommendations.append((title, author, similarity))\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "def display_recommendations(book_title, recommendations, k=10):\n",
    "    \"\"\"\n",
    "    Display book recommendations in a nice format\n",
    "    \"\"\"\n",
    "    if not recommendations:\n",
    "        return\n",
    "\n",
    "    # Get info about the query book\n",
    "    if book_title in title_to_idx:\n",
    "        query_idx = title_to_idx[book_title]\n",
    "        query_author = idx_to_author.get(query_idx, \"Unknown Author\")\n",
    "        print(f\"\\n📚 Books similar to: '{book_title}' by {query_author}\")\n",
    "    else:\n",
    "        print(f\"\\n📚 Books similar to: '{book_title}'\")\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for i, (title, author, similarity) in enumerate(recommendations[:k], 1):\n",
    "        print(f\"{i:2d}. {title}\")\n",
    "        print(f\"    by {author}\")\n",
    "        print(f\"    Similarity: {similarity:.3f}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "def search_books(query, max_results=10):\n",
    "    \"\"\"\n",
    "    Search for books by title (case-insensitive partial matching)\n",
    "    \"\"\"\n",
    "    query_lower = query.lower()\n",
    "    matching_books = []\n",
    "\n",
    "    for title in title_to_idx.keys():\n",
    "        if query_lower in title.lower():\n",
    "            book_idx = title_to_idx[title]\n",
    "            author = idx_to_author.get(book_idx, \"Unknown Author\")\n",
    "            matching_books.append((title, author))\n",
    "\n",
    "    return matching_books[:max_results]\n",
    "\n",
    "\n",
    "print(\"✅ Recommendation system ready!\")\n",
    "print(\"\\nAvailable functions:\")\n",
    "print(\"  - recommend_similar_books(book_title, k=10)\")\n",
    "print(\"  - display_recommendations(book_title, recommendations)\")\n",
    "print(\"  - search_books(query)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Search for Harry Potter books\n",
    "print(\"=== Searching for Harry Potter books ===\")\n",
    "harry_potter_books = search_books(\"harry potter\")\n",
    "for i, (title, author) in enumerate(harry_potter_books[:5], 1):\n",
    "    print(f\"{i}. {title} by {author}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Get recommendations for a Harry Potter book\n",
    "if harry_potter_books:\n",
    "    sample_book = harry_potter_books[0][0]  # Take the first Harry Potter book\n",
    "    recommendations = recommend_similar_books(sample_book, k=10)\n",
    "    display_recommendations(sample_book, recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Try different genres\n",
    "test_books = [\n",
    "    \"The Lord of the Rings\",\n",
    "    \"To Kill a Mockingbird\",\n",
    "    \"1984\",\n",
    "    \"Pride and Prejudice\",\n",
    "    \"The Great Gatsby\",\n",
    "]\n",
    "\n",
    "for book_title in test_books:\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Testing recommendations for: {book_title}\")\n",
    "\n",
    "    # First try exact match\n",
    "    recommendations = recommend_similar_books(book_title, k=5)\n",
    "\n",
    "    if recommendations:\n",
    "        display_recommendations(book_title, recommendations, k=5)\n",
    "    else:\n",
    "        # Try partial search\n",
    "        matches = search_books(book_title, max_results=3)\n",
    "        if matches:\n",
    "            print(f\"\\nFound similar titles:\")\n",
    "            for title, author in matches:\n",
    "                print(f\"  - {title} by {author}\")\n",
    "        else:\n",
    "            print(f\"No books found matching '{book_title}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Recommendation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_book_recommendations():\n",
    "    \"\"\"\n",
    "    Interactive function for getting book recommendations\n",
    "    \"\"\"\n",
    "    print(\"\\n🔍 Interactive Book Recommendation System\")\n",
    "    print(\"Type 'quit' to exit, 'search <query>' to search for books\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"\\nEnter a book title (or command): \").strip()\n",
    "\n",
    "        if user_input.lower() == \"quit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        if user_input.lower().startswith(\"search \"):\n",
    "            query = user_input[7:]  # Remove 'search ' prefix\n",
    "            matches = search_books(query, max_results=10)\n",
    "            if matches:\n",
    "                print(f\"\\nFound {len(matches)} books matching '{query}':\")\n",
    "                for i, (title, author) in enumerate(matches, 1):\n",
    "                    print(f\"  {i:2d}. {title} by {author}\")\n",
    "            else:\n",
    "                print(f\"No books found matching '{query}'\")\n",
    "            continue\n",
    "\n",
    "        # Try to get recommendations\n",
    "        recommendations = recommend_similar_books(user_input, k=8)\n",
    "        display_recommendations(user_input, recommendations, k=8)\n",
    "\n",
    "\n",
    "# Uncomment to run interactive session\n",
    "# interactive_book_recommendations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Analysis and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the learned embeddings\n",
    "print(\"\\n=== Model Analysis ===\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    book_embeddings = model.get_book_embeddings().cpu().numpy()\n",
    "    user_embeddings = model.get_user_embeddings().cpu().numpy()\n",
    "\n",
    "print(f\"Book embedding statistics:\")\n",
    "print(f\"  Mean: {book_embeddings.mean():.4f}\")\n",
    "print(f\"  Std: {book_embeddings.std():.4f}\")\n",
    "print(f\"  Min: {book_embeddings.min():.4f}\")\n",
    "print(f\"  Max: {book_embeddings.max():.4f}\")\n",
    "\n",
    "print(f\"\\nUser embedding statistics:\")\n",
    "print(f\"  Mean: {user_embeddings.mean():.4f}\")\n",
    "print(f\"  Std: {user_embeddings.std():.4f}\")\n",
    "print(f\"  Min: {user_embeddings.min():.4f}\")\n",
    "print(f\"  Max: {user_embeddings.max():.4f}\")\n",
    "\n",
    "# Analyze similarity distribution\n",
    "print(f\"\\nSimilarity matrix statistics:\")\n",
    "upper_triangle = np.triu(\n",
    "    book_similarity_matrix, k=1\n",
    ")  # Exclude diagonal and lower triangle\n",
    "non_zero_similarities = upper_triangle[upper_triangle != 0]\n",
    "\n",
    "print(f\"  Mean similarity: {non_zero_similarities.mean():.4f}\")\n",
    "print(f\"  Std similarity: {non_zero_similarities.std():.4f}\")\n",
    "print(f\"  Min similarity: {non_zero_similarities.min():.4f}\")\n",
    "print(f\"  Max similarity: {non_zero_similarities.max():.4f}\")\n",
    "\n",
    "# Plot similarity distribution\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(non_zero_similarities, bins=50, alpha=0.7, edgecolor=\"black\")\n",
    "plt.xlabel(\"Cosine Similarity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Book Similarities\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Show top 10 most similar book pairs\n",
    "similarity_pairs = []\n",
    "for i in range(len(book_similarity_matrix)):\n",
    "    for j in range(i + 1, len(book_similarity_matrix)):\n",
    "        similarity_pairs.append((i, j, book_similarity_matrix[i, j]))\n",
    "\n",
    "# Sort by similarity\n",
    "similarity_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print(f\"\\nTop 10 most similar book pairs:\")\n",
    "for i, (book1_idx, book2_idx, similarity) in enumerate(similarity_pairs[:10], 1):\n",
    "    try:\n",
    "        title1 = idx_to_title[book1_idx]\n",
    "        title2 = idx_to_title[book2_idx]\n",
    "        author1 = idx_to_author.get(book1_idx, \"Unknown\")\n",
    "        author2 = idx_to_author.get(book2_idx, \"Unknown\")\n",
    "        print(f\"{i:2d}. {similarity:.3f}: '{title1}' by {author1}\")\n",
    "        print(f\"     ↔ '{title2}' by {author2}\")\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "# Plot top similarities\n",
    "top_similarities = [pair[2] for pair in similarity_pairs[:100]]\n",
    "plt.plot(range(1, 101), top_similarities, \"o-\", alpha=0.7)\n",
    "plt.xlabel(\"Rank\")\n",
    "plt.ylabel(\"Similarity Score\")\n",
    "plt.title(\"Top 100 Book Similarities\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "wandb.log({\"similarity_analysis\": wandb.Image(plt)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model and Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save everything needed for deployment\n",
    "print(\"\\n=== Saving Model Components ===\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save model state\n",
    "torch.save(\n",
    "    {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"model_config\": {\n",
    "            \"n_users\": n_users,\n",
    "            \"n_books\": n_books,\n",
    "            \"n_factors\": n_factors,\n",
    "            \"dropout\": 0.1,\n",
    "        },\n",
    "        \"training_config\": {\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"n_epochs\": n_epochs,\n",
    "        },\n",
    "    },\n",
    "    \"matrix_factorization_complete.pth\",\n",
    ")\n",
    "\n",
    "# Save embeddings and similarity matrix\n",
    "np.save(\"book_embeddings.npy\", book_embeddings)\n",
    "np.save(\"book_similarity_matrix.npy\", book_similarity_matrix)\n",
    "\n",
    "# Save mappings\n",
    "mappings = {\n",
    "    \"title_to_idx\": title_to_idx,\n",
    "    \"idx_to_title\": idx_to_title,\n",
    "    \"idx_to_author\": idx_to_author,\n",
    "    \"book_to_idx\": book_to_idx,\n",
    "    \"idx_to_book\": idx_to_book,\n",
    "    \"user_to_idx\": user_to_idx,\n",
    "    \"idx_to_user\": idx_to_user,\n",
    "}\n",
    "\n",
    "with open(\"book_mappings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mappings, f)\n",
    "\n",
    "print(\"✅ Saved:\")\n",
    "print(\"  - matrix_factorization_complete.pth\")\n",
    "print(\"  - book_embeddings.npy\")\n",
    "print(\"  - book_similarity_matrix.npy\")\n",
    "print(\"  - book_mappings.pkl\")\n",
    "\n",
    "print(f\"\\n📊 Final Model Summary:\")\n",
    "print(f\"  Users: {n_users:,}\")\n",
    "print(f\"  Books: {n_books:,}\")\n",
    "print(f\"  Ratings: {len(filtered_ratings):,}\")\n",
    "print(f\"  Embedding dimensions: {n_factors}\")\n",
    "print(f\"  Final validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"  Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation Evaluation Metrics\n",
    "\n",
    "Now let's evaluate the recommendation system using standard ranking metrics: Precision@K, Recall@K, and mAP (mean Average Precision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(recommended_items, relevant_items, k):\n",
    "    \"\"\"\n",
    "    Calculate Precision@K for a single user\n",
    "\n",
    "    Args:\n",
    "        recommended_items (list): List of recommended item indices (ranked)\n",
    "        relevant_items (set): Set of relevant/positive item indices for the user\n",
    "        k (int): Number of top recommendations to consider\n",
    "\n",
    "    Returns:\n",
    "        float: Precision@K score\n",
    "    \"\"\"\n",
    "    if k == 0 or len(recommended_items) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    recommended_at_k = set(recommended_items[:k])\n",
    "    relevant_recommended = recommended_at_k.intersection(relevant_items)\n",
    "\n",
    "    return len(relevant_recommended) / min(k, len(recommended_items))\n",
    "\n",
    "\n",
    "def recall_at_k(recommended_items, relevant_items, k):\n",
    "    \"\"\"\n",
    "    Calculate Recall@K for a single user\n",
    "\n",
    "    Args:\n",
    "        recommended_items (list): List of recommended item indices (ranked)\n",
    "        relevant_items (set): Set of relevant/positive item indices for the user\n",
    "        k (int): Number of top recommendations to consider\n",
    "\n",
    "    Returns:\n",
    "        float: Recall@K score\n",
    "    \"\"\"\n",
    "    if len(relevant_items) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    recommended_at_k = set(recommended_items[:k])\n",
    "    relevant_recommended = recommended_at_k.intersection(relevant_items)\n",
    "\n",
    "    return len(relevant_recommended) / len(relevant_items)\n",
    "\n",
    "\n",
    "def average_precision(recommended_items, relevant_items):\n",
    "    \"\"\"\n",
    "    Calculate Average Precision for a single user\n",
    "\n",
    "    Args:\n",
    "        recommended_items (list): List of recommended item indices (ranked)\n",
    "        relevant_items (set): Set of relevant/positive item indices for the user\n",
    "\n",
    "    Returns:\n",
    "        float: Average Precision score\n",
    "    \"\"\"\n",
    "    if len(relevant_items) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i, item in enumerate(recommended_items):\n",
    "        if item in relevant_items:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "\n",
    "    return score / len(relevant_items)\n",
    "\n",
    "\n",
    "def get_user_recommendations(model, user_idx, user_items, n_recommendations=100):\n",
    "    \"\"\"\n",
    "    Get top N recommendations for a user\n",
    "\n",
    "    Args:\n",
    "        model: Trained matrix factorization model\n",
    "        user_idx (int): User index\n",
    "        user_items (set): Set of items the user has already interacted with\n",
    "        n_recommendations (int): Number of recommendations to generate\n",
    "\n",
    "    Returns:\n",
    "        list: List of recommended book indices (excluding already seen items)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get user embedding\n",
    "        user_tensor = torch.LongTensor([user_idx]).to(device)\n",
    "        user_emb = model.user_embedding(user_tensor)\n",
    "\n",
    "        # Get all book embeddings\n",
    "        book_embs = model.get_book_embeddings()\n",
    "\n",
    "        # Compute scores for all books\n",
    "        scores = torch.mm(user_emb, book_embs.t()).squeeze()\n",
    "\n",
    "        # Add biases\n",
    "        user_bias = model.user_bias(user_tensor).squeeze()\n",
    "        book_biases = model.book_bias.weight.squeeze()\n",
    "        global_bias = model.global_bias\n",
    "\n",
    "        scores = scores + user_bias + book_biases + global_bias\n",
    "\n",
    "        # Apply sigmoid to get probabilities\n",
    "        scores = torch.sigmoid(scores)\n",
    "\n",
    "        # Convert to numpy and get top items\n",
    "        scores_np = scores.cpu().numpy()\n",
    "\n",
    "        # Get all book indices sorted by score (descending)\n",
    "        all_book_indices = np.argsort(scores_np)[::-1]\n",
    "\n",
    "        # Filter out items the user has already seen\n",
    "        recommended_items = []\n",
    "        for book_idx in all_book_indices:\n",
    "            if book_idx not in user_items:\n",
    "                recommended_items.append(book_idx)\n",
    "                if len(recommended_items) >= n_recommendations:\n",
    "                    break\n",
    "\n",
    "        return recommended_items\n",
    "\n",
    "\n",
    "print(\"✅ Evaluation functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Test Data for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test set for evaluation\n",
    "print(\"\\n=== Preparing Test Data for Evaluation ===\")\n",
    "\n",
    "# We'll use the validation split as our test set for evaluation\n",
    "test_users_tensor = val_users\n",
    "test_books_tensor = val_books\n",
    "test_ratings_tensor = val_ratings\n",
    "\n",
    "# Convert validation data to user-item format for evaluation\n",
    "test_user_items = {}  # user_idx -> set of positive book indices\n",
    "test_user_all_items = {}  # user_idx -> set of all book indices (for filtering)\n",
    "\n",
    "# Group by user\n",
    "for i in range(len(test_users_tensor)):\n",
    "    user_idx = test_users_tensor[i].item()\n",
    "    book_idx = test_books_tensor[i].item()\n",
    "    rating = test_ratings_tensor[i].item()\n",
    "\n",
    "    if user_idx not in test_user_items:\n",
    "        test_user_items[user_idx] = set()\n",
    "        test_user_all_items[user_idx] = set()\n",
    "\n",
    "    test_user_all_items[user_idx].add(book_idx)\n",
    "    if rating == 1:  # Positive feedback (rating >= 7.0)\n",
    "        test_user_items[user_idx].add(book_idx)\n",
    "\n",
    "# Also need training data to filter out seen items during recommendation\n",
    "train_user_items = {}  # user_idx -> set of book indices seen during training\n",
    "\n",
    "for i in range(len(train_users)):\n",
    "    user_idx = train_users[i].item()\n",
    "    book_idx = train_books[i].item()\n",
    "\n",
    "    if user_idx not in train_user_items:\n",
    "        train_user_items[user_idx] = set()\n",
    "\n",
    "    train_user_items[user_idx].add(book_idx)\n",
    "\n",
    "# Filter test users that have at least 1 positive item and appear in training\n",
    "valid_test_users = []\n",
    "for user_idx in test_user_items:\n",
    "    if len(test_user_items[user_idx]) > 0 and user_idx in train_user_items:\n",
    "        valid_test_users.append(user_idx)\n",
    "\n",
    "print(f\"Total test users: {len(test_user_items)}\")\n",
    "print(f\"Valid test users (with positive items & in training): {len(valid_test_users)}\")\n",
    "\n",
    "# Sample a subset for faster evaluation\n",
    "max_test_users = 1000  # Adjust this based on computational resources\n",
    "if len(valid_test_users) > max_test_users:\n",
    "    np.random.seed(42)\n",
    "    valid_test_users = np.random.choice(\n",
    "        valid_test_users, max_test_users, replace=False\n",
    "    ).tolist()\n",
    "    print(f\"Sampled {max_test_users} users for evaluation\")\n",
    "\n",
    "print(f\"Users for evaluation: {len(valid_test_users)}\")\n",
    "\n",
    "# Show statistics\n",
    "positive_counts = [len(test_user_items[user_idx]) for user_idx in valid_test_users]\n",
    "print(f\"Average positive items per test user: {np.mean(positive_counts):.2f}\")\n",
    "print(f\"Min positive items: {np.min(positive_counts)}\")\n",
    "print(f\"Max positive items: {np.max(positive_counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(\"\\n=== Running Evaluation ===\")\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load(\"best_matrix_factorization.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Evaluation parameters\n",
    "k_values = [5, 10, 20, 50]\n",
    "n_recommendations = 100  # Generate top-100 to calculate metrics at different k values\n",
    "\n",
    "# Store results\n",
    "results = {f\"precision@{k}\": [] for k in k_values}\n",
    "results.update({f\"recall@{k}\": [] for k in k_values})\n",
    "results[\"mAP\"] = []\n",
    "\n",
    "print(f\"Evaluating {len(valid_test_users)} users...\")\n",
    "\n",
    "# Evaluate each user\n",
    "for i, user_idx in enumerate(tqdm(valid_test_users, desc=\"Evaluating users\")):\n",
    "    # Get user's training items (to filter out)\n",
    "    user_train_items = train_user_items.get(user_idx, set())\n",
    "\n",
    "    # Get user's positive test items\n",
    "    user_positive_items = test_user_items[user_idx]\n",
    "\n",
    "    # Generate recommendations\n",
    "    recommended_items = get_user_recommendations(\n",
    "        model, user_idx, user_train_items, n_recommendations\n",
    "    )\n",
    "\n",
    "    # Calculate metrics for different k values\n",
    "    for k in k_values:\n",
    "        prec_k = precision_at_k(recommended_items, user_positive_items, k)\n",
    "        recall_k = recall_at_k(recommended_items, user_positive_items, k)\n",
    "\n",
    "        results[f\"precision@{k}\"].append(prec_k)\n",
    "        results[f\"recall@{k}\"].append(recall_k)\n",
    "\n",
    "    # Calculate mAP (using all recommendations)\n",
    "    map_score = average_precision(recommended_items, user_positive_items)\n",
    "    results[\"mAP\"].append(map_score)\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_results = {}\n",
    "for metric, values in results.items():\n",
    "    avg_results[metric] = np.mean(values)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"Precision@{k:2d}: {avg_results[f'precision@{k}']:.4f}\")\n",
    "\n",
    "print()\n",
    "for k in k_values:\n",
    "    print(f\"Recall@{k:2d}:    {avg_results[f'recall@{k}']:.4f}\")\n",
    "\n",
    "print(f\"\\nmAP:          {avg_results['mAP']:.4f}\")\n",
    "\n",
    "# Log to wandb\n",
    "wandb.log(avg_results)\n",
    "\n",
    "print(f\"\\nEvaluation completed on {len(valid_test_users)} users\")\n",
    "print(\n",
    "    f\"Average positive items per user: {np.mean([len(test_user_items[u]) for u in valid_test_users]):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results in more detail\n",
    "print(\"\\n=== Detailed Analysis ===\")\n",
    "\n",
    "# Create a detailed results dataframe\n",
    "detailed_results = []\n",
    "for i, user_idx in enumerate(valid_test_users):\n",
    "    user_result = {\"user_idx\": user_idx}\n",
    "    user_result[\"n_positive\"] = len(test_user_items[user_idx])\n",
    "\n",
    "    for k in k_values:\n",
    "        user_result[f\"precision@{k}\"] = results[f\"precision@{k}\"][i]\n",
    "        user_result[f\"recall@{k}\"] = results[f\"recall@{k}\"][i]\n",
    "\n",
    "    user_result[\"mAP\"] = results[\"mAP\"][i]\n",
    "    detailed_results.append(user_result)\n",
    "\n",
    "results_df = pd.DataFrame(detailed_results)\n",
    "\n",
    "# Show statistics by number of positive items\n",
    "print(\"Results by number of positive items:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for n_pos in sorted(results_df[\"n_positive\"].unique()):\n",
    "    if n_pos <= 20:  # Focus on users with reasonable number of positives\n",
    "        subset = results_df[results_df[\"n_positive\"] == n_pos]\n",
    "        if len(subset) >= 5:  # Only show if we have enough samples\n",
    "            print(f\"Users with {n_pos:2d} positive items ({len(subset):3d} users):\")\n",
    "            print(\n",
    "                f\"  Precision@10: {subset['precision@10'].mean():.4f} ± {subset['precision@10'].std():.4f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"  Recall@10:    {subset['recall@10'].mean():.4f} ± {subset['recall@10'].std():.4f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"  mAP:          {subset['mAP'].mean():.4f} ± {subset['mAP'].std():.4f}\"\n",
    "            )\n",
    "            print()\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Precision@K for different K values\n",
    "axes[0, 0].bar(range(len(k_values)), [avg_results[f\"precision@{k}\"] for k in k_values])\n",
    "axes[0, 0].set_xlabel(\"K\")\n",
    "axes[0, 0].set_ylabel(\"Precision@K\")\n",
    "axes[0, 0].set_title(\"Precision@K for Different K Values\")\n",
    "axes[0, 0].set_xticks(range(len(k_values)))\n",
    "axes[0, 0].set_xticklabels([f\"{k}\" for k in k_values])\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Recall@K for different K values\n",
    "axes[0, 1].bar(range(len(k_values)), [avg_results[f\"recall@{k}\"] for k in k_values])\n",
    "axes[0, 1].set_xlabel(\"K\")\n",
    "axes[0, 1].set_ylabel(\"Recall@K\")\n",
    "axes[0, 1].set_title(\"Recall@K for Different K Values\")\n",
    "axes[0, 1].set_xticks(range(len(k_values)))\n",
    "axes[0, 1].set_xticklabels([f\"{k}\" for k in k_values])\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Distribution of mAP scores\n",
    "axes[1, 0].hist(results[\"mAP\"], bins=30, alpha=0.7, edgecolor=\"black\")\n",
    "axes[1, 0].set_xlabel(\"mAP Score\")\n",
    "axes[1, 0].set_ylabel(\"Number of Users\")\n",
    "axes[1, 0].set_title(\"Distribution of mAP Scores\")\n",
    "axes[1, 0].axvline(\n",
    "    avg_results[\"mAP\"],\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Mean: {avg_results['mAP']:.4f}\",\n",
    ")\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Precision@10 vs number of positive items\n",
    "# Group users by number of positive items for better visualization\n",
    "pos_item_groups = (\n",
    "    results_df.groupby(\"n_positive\")[\"precision@10\"]\n",
    "    .agg([\"mean\", \"count\"])\n",
    "    .reset_index()\n",
    ")\n",
    "pos_item_groups = pos_item_groups[\n",
    "    pos_item_groups[\"count\"] >= 5\n",
    "]  # Only groups with 5+ users\n",
    "\n",
    "axes[1, 1].scatter(\n",
    "    pos_item_groups[\"n_positive\"],\n",
    "    pos_item_groups[\"mean\"],\n",
    "    s=pos_item_groups[\"count\"] * 5,\n",
    "    alpha=0.6,\n",
    ")\n",
    "axes[1, 1].set_xlabel(\"Number of Positive Items\")\n",
    "axes[1, 1].set_ylabel(\"Average Precision@10\")\n",
    "axes[1, 1].set_title(\n",
    "    \"Precision@10 vs Number of Positive Items\\n(Bubble size = number of users)\"\n",
    ")\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Log the visualization\n",
    "wandb.log({\"evaluation_results\": wandb.Image(fig)})\n",
    "\n",
    "# Show top and bottom performers\n",
    "print(\"\\nTop 10 users by mAP:\")\n",
    "top_users = results_df.nlargest(10, \"mAP\")\n",
    "for _, row in top_users.iterrows():\n",
    "    print(\n",
    "        f\"User {row['user_idx']:5d}: mAP={row['mAP']:.4f}, {row['n_positive']} positive items\"\n",
    "    )\n",
    "\n",
    "print(\"\\nBottom 10 users by mAP:\")\n",
    "bottom_users = results_df.nsmallest(10, \"mAP\")\n",
    "for _, row in bottom_users.iterrows():\n",
    "    print(\n",
    "        f\"User {row['user_idx']:5d}: mAP={row['mAP']:.4f}, {row['n_positive']} positive items\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL EVALUATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n📊 MODEL PERFORMANCE:\")\n",
    "print(f\"   Matrix Factorization with {n_factors} factors\")\n",
    "print(f\"   Trained on {len(train_users):,} interactions\")\n",
    "print(f\"   Evaluated on {len(valid_test_users)} users\")\n",
    "print(\n",
    "    f\"   Average {np.mean([len(test_user_items[u]) for u in valid_test_users]):.1f} positive items per test user\"\n",
    ")\n",
    "\n",
    "print(f\"\\n📈 RANKING METRICS:\")\n",
    "for k in k_values:\n",
    "    print(f\"   Precision@{k:2d}: {avg_results[f'precision@{k}']:.4f}\")\n",
    "    print(f\"   Recall@{k:2d}:    {avg_results[f'recall@{k}']:.4f}\")\n",
    "\n",
    "print(f\"   mAP:          {avg_results['mAP']:.4f}\")\n",
    "\n",
    "print(f\"\\n🔍 KEY INSIGHTS:\")\n",
    "max_similarity = book_similarity_matrix.max()\n",
    "mean_similarity = np.mean(\n",
    "    book_similarity_matrix[np.triu_indices_from(book_similarity_matrix, k=1)]\n",
    ")\n",
    "\n",
    "print(f\"   • Maximum book similarity: {max_similarity:.4f}\")\n",
    "print(f\"   • Average book similarity: {mean_similarity:.4f}\")\n",
    "print(f\"   • Training validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "if avg_results[\"precision@10\"] < 0.05:\n",
    "    print(f\"\\n⚠️  PERFORMANCE NOTES:\")\n",
    "    print(f\"   • Low precision suggests the model may need improvement\")\n",
    "    print(\n",
    "        f\"   • Consider: higher embedding dimensions, more data, or different loss functions\"\n",
    "    )\n",
    "    print(\n",
    "        f\"   • The similarity scores (max {max_similarity:.3f}) indicate embeddings may benefit from\"\n",
    "    )\n",
    "    print(f\"     different initialization or regularization strategies\")\n",
    "\n",
    "print(f\"\\n💡 RECOMMENDATIONS FOR IMPROVEMENT:\")\n",
    "print(f\"   • Try increasing embedding dimensions (current: {n_factors})\")\n",
    "print(f\"   • Experiment with different loss functions (BPR, WARP)\")\n",
    "print(f\"   • Add negative sampling for better implicit feedback learning\")\n",
    "print(f\"   • Consider user/item side information if available\")\n",
    "print(f\"   • Use learning rate scheduling or different optimizers\")\n",
    "\n",
    "# Save detailed results\n",
    "results_df.to_csv(\"recommendation_evaluation_results.csv\", index=False)\n",
    "print(f\"\\n💾 Detailed results saved to: recommendation_evaluation_results.csv\")\n",
    "\n",
    "# Create summary for wandb\n",
    "wandb_summary = {\n",
    "    \"final_precision@5\": avg_results[\"precision@5\"],\n",
    "    \"final_precision@10\": avg_results[\"precision@10\"],\n",
    "    \"final_recall@10\": avg_results[\"recall@10\"],\n",
    "    \"final_mAP\": avg_results[\"mAP\"],\n",
    "    \"max_similarity\": float(max_similarity),\n",
    "    \"mean_similarity\": float(mean_similarity),\n",
    "    \"n_test_users\": len(valid_test_users),\n",
    "    \"avg_positive_per_user\": np.mean(\n",
    "        [len(test_user_items[u]) for u in valid_test_users]\n",
    "    ),\n",
    "}\n",
    "\n",
    "wandb.log(wandb_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish wandb run\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
