{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b0a90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "DATA_DIR = \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db5be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    project=\"book-recommendation-kaggle\",\n",
    "    group=\"dev\",\n",
    "    job_type=\"train\",\n",
    "    save_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab243d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.read_csv(f\"{DATA_DIR}/Books.csv.zip\", compression=\"zip\")\n",
    "ratings = pd.read_csv(f\"{DATA_DIR}/Ratings.csv.zip\", compression=\"zip\")\n",
    "users = pd.read_csv(f\"{DATA_DIR}/Users.csv.zip\", compression=\"zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e69614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73630ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272e8fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c11c912",
   "metadata": {},
   "outputs": [],
   "source": [
    "users[\"User-ID\"].nunique() / len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80ebf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings[\"target\"] = (ratings[\"Book-Rating\"] >= 7).astype(int)\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06021e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mappings for categorical features\n",
    "\n",
    "books[\"Year-Of-Publication\"] = pd.to_numeric(\n",
    "    books[\"Year-Of-Publication\"], errors=\"coerce\"\n",
    ")\n",
    "books[\"year_bucket\"] = pd.cut(\n",
    "    books[\"Year-Of-Publication\"],\n",
    "    bins=[0, 1980, 1990, 2000, 2010, 2025],\n",
    "    labels=[\"<1980\", \"1980s\", \"1990s\", \"2000s\", \"2010s+\"],\n",
    ")\n",
    "\n",
    "# Create vocabs\n",
    "title_vocab = {title: idx for idx, title in enumerate(books[\"Book-Title\"].unique())}\n",
    "author_vocab = {author: idx for idx, author in enumerate(books[\"Book-Author\"].unique())}\n",
    "publisher_vocab = {pub: idx for idx, pub in enumerate(books[\"Publisher\"].unique())}\n",
    "year_vocab = {year: idx for idx, year in enumerate(books[\"year_bucket\"].cat.categories)}\n",
    "\n",
    "# Process user features\n",
    "users[\"age_bucket\"] = pd.cut(\n",
    "    users[\"Age\"],\n",
    "    bins=[0, 18, 25, 35, 50, 100],\n",
    "    labels=[\"<18\", \"18-25\", \"25-35\", \"35-50\", \"50+\"],\n",
    ")\n",
    "\n",
    "# Simple location processing\n",
    "users[\"location_simple\"] = (\n",
    "    users[\"Location\"].str.split(\",\").str[-1].str.strip().str.lower()\n",
    ")\n",
    "location_vocab = {loc: idx for idx, loc in enumerate(users[\"location_simple\"].unique())}\n",
    "age_vocab = {age: idx for idx, age in enumerate(users[\"age_bucket\"].cat.categories)}\n",
    "\n",
    "feature_mappings = {\n",
    "    \"title_vocab\": title_vocab,\n",
    "    \"author_vocab\": author_vocab,\n",
    "    \"publisher_vocab\": publisher_vocab,\n",
    "    \"year_vocab\": year_vocab,\n",
    "    \"location_vocab\": location_vocab,\n",
    "    \"age_vocab\": age_vocab,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46619b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better data preparation - fix the float issue and missing mappings\n",
    "print(\"=== Data Mapping and Cleaning ===\")\n",
    "\n",
    "# Create proper integer mappings\n",
    "user_id_to_idx = users.reset_index().set_index(\"User-ID\")[\"index\"]\n",
    "book_id_to_idx = books.reset_index().set_index(\"ISBN\")[\"index\"]\n",
    "\n",
    "print(f\"User mapping size: {len(user_id_to_idx)}\")\n",
    "print(f\"Book mapping size: {len(book_id_to_idx)}\")\n",
    "\n",
    "# Add the target column\n",
    "ratings = ratings.copy()\n",
    "ratings[\"target\"] = (ratings[\"Book-Rating\"] >= 7.0).astype(int)\n",
    "\n",
    "# Map user and book IDs, keeping track of missing values\n",
    "ratings[\"user_idx\"] = ratings[\"User-ID\"].map(user_id_to_idx)\n",
    "ratings[\"book_idx\"] = ratings[\"ISBN\"].map(book_id_to_idx)\n",
    "\n",
    "# Check for missing mappings\n",
    "missing_users = ratings[\"user_idx\"].isnull().sum()\n",
    "missing_books = ratings[\"book_idx\"].isnull().sum()\n",
    "print(f\"Missing user mappings: {missing_users}\")\n",
    "print(f\"Missing book mappings: {missing_books}\")\n",
    "\n",
    "# Only keep rows with valid mappings\n",
    "ratings_clean = ratings.dropna(subset=[\"user_idx\", \"book_idx\"]).copy()\n",
    "\n",
    "# Convert to proper integers\n",
    "ratings_clean[\"user_idx\"] = ratings_clean[\"user_idx\"].astype(\"int32\")\n",
    "ratings_clean[\"book_idx\"] = ratings_clean[\"book_idx\"].astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b1e750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add feature indices\n",
    "ratings_clean = ratings_clean.merge(\n",
    "    books[[\"ISBN\", \"Book-Title\", \"Book-Author\", \"Publisher\", \"year_bucket\"]], on=\"ISBN\"\n",
    ")\n",
    "\n",
    "ratings_clean = ratings_clean.merge(\n",
    "    users[[\"User-ID\", \"age_bucket\", \"location_simple\"]],\n",
    "    left_on=\"User-ID\",\n",
    "    right_on=\"User-ID\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# Map to indices\n",
    "ratings_clean[\"title_idx\"] = (\n",
    "    ratings_clean[\"Book-Title\"].map(feature_mappings[\"title_vocab\"]).fillna(0)\n",
    ")\n",
    "ratings_clean[\"author_idx\"] = (\n",
    "    ratings_clean[\"Book-Author\"].map(feature_mappings[\"author_vocab\"]).fillna(0)\n",
    ")\n",
    "ratings_clean[\"publisher_idx\"] = (\n",
    "    ratings_clean[\"Publisher\"].map(feature_mappings[\"publisher_vocab\"]).fillna(0)\n",
    ")\n",
    "ratings_clean[\"year_idx\"] = (\n",
    "    ratings_clean[\"year_bucket\"].map(feature_mappings[\"year_vocab\"]).fillna(0)\n",
    ")\n",
    "ratings_clean[\"location_idx\"] = (\n",
    "    ratings_clean[\"location_simple\"].map(feature_mappings[\"location_vocab\"]).fillna(0)\n",
    ")\n",
    "ratings_clean[\"age_idx\"] = (\n",
    "    ratings_clean[\"age_bucket\"].map(feature_mappings[\"age_vocab\"]).fillna(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15236ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_clean = ratings_clean.drop(\n",
    "    columns=[col for col in ratings_clean if \"Image-URL\" in col]\n",
    ")\n",
    "print(f\"Final clean dataset size: {len(ratings_clean)} (from {len(ratings)} original)\")\n",
    "print(\n",
    "    f\"Data types: user_idx={ratings_clean['user_idx'].dtype}, book_idx={ratings_clean['book_idx'].dtype}\"\n",
    ")\n",
    "\n",
    "ratings_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82334a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridContrastiveDataset(Dataset):\n",
    "    def __init__(self, df, num_random_negatives=7):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.num_random_negatives = num_random_negatives\n",
    "\n",
    "        # Separate positives and explicit negatives\n",
    "        self.positives = df[df[\"target\"] == 1].reset_index(drop=True)\n",
    "        self.explicit_negatives = df[df[\"target\"] == 0].reset_index(drop=True)\n",
    "\n",
    "        print(\"Dataset composition:\")\n",
    "        print(f\"  Positives: {len(self.positives):,}\")\n",
    "        print(f\"  Explicit negatives: {len(self.explicit_negatives):,}\")\n",
    "        print(f\"  Random negatives per positive: {num_random_negatives}\")\n",
    "\n",
    "        # Create efficient lookup structures\n",
    "        self.user_positive_books = (\n",
    "            self.positives.groupby(\"user_idx_new\")[\"book_idx_new\"].apply(set).to_dict()\n",
    "        )\n",
    "        self.user_negative_books = (\n",
    "            self.explicit_negatives.groupby(\"user_idx_new\")[\"book_idx_new\"]\n",
    "            .apply(set)\n",
    "            .to_dict()\n",
    "        )\n",
    "        self.all_books = set(df[\"book_idx_new\"].unique())\n",
    "\n",
    "        # Pre-compute features - ensure ALL required features are available\n",
    "        self.user_features = (\n",
    "            df.groupby(\"user_idx_new\")\n",
    "            .first()[[\"age_idx\", \"location_idx\"]]\n",
    "            .fillna(0)  # Fill missing values\n",
    "            .to_dict(\"index\")\n",
    "        )\n",
    "\n",
    "        self.book_features = (\n",
    "            df.groupby(\"book_idx_new\")\n",
    "            .first()[[\"title_idx\", \"author_idx\", \"publisher_idx\", \"year_idx\"]]\n",
    "            .fillna(0)  # Fill missing values\n",
    "            .to_dict(\"index\")\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.positives)  # One sample per positive interaction\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the positive interaction\n",
    "        pos_row = self.positives.iloc[idx]\n",
    "        user_id = pos_row[\"user_idx_new\"]\n",
    "        pos_book_id = pos_row[\"book_idx_new\"]\n",
    "\n",
    "        # Get user features\n",
    "        user_feats = self.user_features[user_id]\n",
    "\n",
    "        # Get positive book features\n",
    "        pos_book_feats = self.book_features[pos_book_id]\n",
    "\n",
    "        # Collect all negatives for this user\n",
    "        negatives = []\n",
    "        negative_features = []\n",
    "\n",
    "        # 1. Add explicit negatives (from actual user interactions with target=0)\n",
    "        if user_id in self.user_negative_books:\n",
    "            explicit_negs = list(self.user_negative_books[user_id])\n",
    "            negatives.extend(explicit_negs)\n",
    "            negative_features.extend(\n",
    "                [self.book_features[book_id] for book_id in explicit_negs]\n",
    "            )\n",
    "\n",
    "        # 2. Add random negatives (books user never interacted with)\n",
    "        user_all_interactions = self.user_positive_books.get(\n",
    "            user_id, set()\n",
    "        ) | self.user_negative_books.get(user_id, set())\n",
    "\n",
    "        available_books = list(self.all_books - user_all_interactions)\n",
    "\n",
    "        if len(available_books) >= self.num_random_negatives:\n",
    "            random_negs = np.random.choice(\n",
    "                available_books, size=self.num_random_negatives, replace=False\n",
    "            )\n",
    "            negatives.extend(random_negs)\n",
    "            negative_features.extend(\n",
    "                [self.book_features[book_id] for book_id in random_negs]\n",
    "            )\n",
    "\n",
    "        # Ensure we have at least one negative\n",
    "        if len(negatives) == 0:\n",
    "            # Add one random negative if no explicit negatives exist\n",
    "            available_books = list(self.all_books - user_all_interactions)\n",
    "            if available_books:\n",
    "                random_neg = np.random.choice(available_books, size=1)[0]\n",
    "                negatives = [random_neg]\n",
    "                negative_features = [self.book_features[random_neg]]\n",
    "\n",
    "        return {\n",
    "            \"user_id\": int(user_id),\n",
    "            \"user_age_idx\": int(user_feats[\"age_idx\"]),\n",
    "            \"user_location_idx\": int(user_feats[\"location_idx\"]),\n",
    "            \"pos_book_id\": int(pos_book_id),\n",
    "            \"pos_title_idx\": int(pos_book_feats[\"title_idx\"]),\n",
    "            \"pos_author_idx\": int(pos_book_feats[\"author_idx\"]),\n",
    "            \"pos_publisher_idx\": int(pos_book_feats[\"publisher_idx\"]),\n",
    "            \"pos_year_idx\": int(pos_book_feats[\"year_idx\"]),\n",
    "            \"neg_book_ids\": torch.LongTensor(negatives),\n",
    "            \"neg_title_idx\": torch.LongTensor(\n",
    "                [int(f[\"title_idx\"]) for f in negative_features]\n",
    "            ),\n",
    "            \"neg_author_idx\": torch.LongTensor(\n",
    "                [int(f[\"author_idx\"]) for f in negative_features]\n",
    "            ),\n",
    "            \"neg_publisher_idx\": torch.LongTensor(\n",
    "                [int(f[\"publisher_idx\"]) for f in negative_features]\n",
    "            ),\n",
    "            \"neg_year_idx\": torch.LongTensor(\n",
    "                [int(f[\"year_idx\"]) for f in negative_features]\n",
    "            ),\n",
    "            # Keep track of negative types for analysis\n",
    "            \"num_explicit_negs\": len(self.user_negative_books.get(user_id, [])),\n",
    "            \"num_random_negs\": self.num_random_negatives,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a01620",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserTower(nn.Module):\n",
    "    def __init__(self, num_users, num_locations, num_age_buckets, embedding_dim=64):\n",
    "        super().__init__()\n",
    "\n",
    "        # Simpler approach - fewer feature embeddings to start\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.location_embedding = nn.Embedding(num_locations, 16)\n",
    "        self.age_embedding = nn.Embedding(num_age_buckets, 16)\n",
    "\n",
    "        # Simpler fusion\n",
    "        self.feature_fusion = nn.Sequential(\n",
    "            nn.Linear(embedding_dim + 32, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "\n",
    "    def forward(self, user_ids, location_ids, age_bucket_ids):\n",
    "        user_emb = self.user_embedding(user_ids)\n",
    "        location_emb = self.location_embedding(location_ids)\n",
    "        age_emb = self.age_embedding(age_bucket_ids)\n",
    "\n",
    "        # Simple concatenation and fusion\n",
    "        features = torch.cat([user_emb, location_emb, age_emb], dim=-1)\n",
    "        output = self.feature_fusion(features)\n",
    "\n",
    "        return F.normalize(output, p=2, dim=-1)\n",
    "\n",
    "\n",
    "class BookTower(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_books,\n",
    "        num_titles,\n",
    "        num_authors,\n",
    "        num_publishers,\n",
    "        num_year_buckets,\n",
    "        embedding_dim=64,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Simpler book model\n",
    "        self.book_embedding = nn.Embedding(num_books, embedding_dim)\n",
    "        self.title_embedding = nn.Embedding(num_titles, 32)\n",
    "        self.author_embedding = nn.Embedding(num_authors, 16)\n",
    "        self.publisher_embedding = nn.Embedding(num_publishers, 16)\n",
    "        self.year_embedding = nn.Embedding(num_year_buckets, 8)\n",
    "\n",
    "        # Simpler fusion\n",
    "        self.feature_fusion = nn.Sequential(\n",
    "            nn.Linear(embedding_dim + 72, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "\n",
    "    def forward(self, book_ids, title_ids, author_ids, publisher_ids, year_bucket_ids):\n",
    "        book_emb = self.book_embedding(book_ids)\n",
    "        title_emb = self.title_embedding(title_ids)\n",
    "        author_emb = self.author_embedding(author_ids)\n",
    "        publisher_emb = self.publisher_embedding(publisher_ids)\n",
    "        year_emb = self.year_embedding(year_bucket_ids)\n",
    "\n",
    "        features = torch.cat([book_emb, title_emb, author_emb, publisher_emb, year_emb], dim=-1)\n",
    "        output = self.feature_fusion(features)\n",
    "\n",
    "        return F.normalize(output, p=2, dim=-1)\n",
    "\n",
    "\n",
    "class ContrastiveTwoTowerModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_users,\n",
    "        num_books,\n",
    "        num_locations,\n",
    "        num_age_buckets,\n",
    "        num_titles,\n",
    "        num_authors,\n",
    "        num_publishers,\n",
    "        num_year_buckets,\n",
    "        embedding_dim=64,\n",
    "        temperature=0.07,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.user_tower = UserTower(\n",
    "            num_users, num_locations, num_age_buckets, embedding_dim\n",
    "        )\n",
    "        self.book_tower = BookTower(\n",
    "            num_books,\n",
    "            num_titles,\n",
    "            num_authors,\n",
    "            num_publishers,\n",
    "            num_year_buckets,\n",
    "            embedding_dim,\n",
    "        )\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, user_data, book_data):\n",
    "        user_emb = self.user_tower(\n",
    "            user_data[\"user_ids\"],\n",
    "            user_data[\"location_ids\"],\n",
    "            user_data[\"age_bucket_ids\"],\n",
    "        )\n",
    "\n",
    "        book_emb = self.book_tower(\n",
    "            book_data[\"book_ids\"],\n",
    "            book_data[\"title_ids\"],\n",
    "            book_data[\"author_ids\"],\n",
    "            book_data[\"publisher_ids\"],\n",
    "            book_data[\"year_bucket_ids\"],\n",
    "        )\n",
    "\n",
    "        return user_emb, book_emb\n",
    "\n",
    "    def compute_similarity(self, user_emb, book_emb):\n",
    "        return torch.sum(user_emb * book_emb, dim=-1) / self.temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8053b2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader=None, epochs=10, lr=1e-4):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.8)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch_size = len(batch[\"user_id\"])\n",
    "\n",
    "            # Convert to tensors\n",
    "            user_data = {\n",
    "                \"user_ids\": torch.LongTensor([batch[\"user_id\"][i] for i in range(batch_size)]),\n",
    "                \"location_ids\": torch.LongTensor([batch[\"user_location_idx\"][i] for i in range(batch_size)]),\n",
    "                \"age_bucket_ids\": torch.LongTensor([batch[\"user_age_idx\"][i] for i in range(batch_size)]),\n",
    "            }\n",
    "\n",
    "            # Positive book data\n",
    "            pos_book_data = {\n",
    "                \"book_ids\": torch.LongTensor([batch[\"pos_book_id\"][i] for i in range(batch_size)]),\n",
    "                \"title_ids\": torch.LongTensor([batch[\"pos_title_idx\"][i] for i in range(batch_size)]),\n",
    "                \"author_ids\": torch.LongTensor([batch[\"pos_author_idx\"][i] for i in range(batch_size)]),\n",
    "                \"publisher_ids\": torch.LongTensor([batch[\"pos_publisher_idx\"][i] for i in range(batch_size)]),\n",
    "                \"year_bucket_ids\": torch.LongTensor([batch[\"pos_year_idx\"][i] for i in range(batch_size)]),\n",
    "            }\n",
    "\n",
    "            # Handle negative books\n",
    "            all_neg_book_ids = []\n",
    "            all_neg_features = {\n",
    "                \"title_ids\": [],\n",
    "                \"author_ids\": [],\n",
    "                \"publisher_ids\": [],\n",
    "                \"year_bucket_ids\": [],\n",
    "            }\n",
    "            neg_counts = []\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                neg_ids = batch[\"neg_book_ids\"][i]\n",
    "                neg_counts.append(len(neg_ids))\n",
    "\n",
    "                all_neg_book_ids.extend(neg_ids.tolist())\n",
    "                all_neg_features[\"title_ids\"].extend(batch[\"neg_title_idx\"][i].tolist())\n",
    "                all_neg_features[\"author_ids\"].extend(batch[\"neg_author_idx\"][i].tolist())\n",
    "                all_neg_features[\"publisher_ids\"].extend(batch[\"neg_publisher_idx\"][i].tolist())\n",
    "                all_neg_features[\"year_bucket_ids\"].extend(batch[\"neg_year_idx\"][i].tolist())\n",
    "\n",
    "            # Get user and positive book embeddings\n",
    "            user_emb, pos_book_emb = model(user_data, pos_book_data)\n",
    "\n",
    "            # Skip batch if no negatives\n",
    "            if len(all_neg_book_ids) == 0:\n",
    "                continue\n",
    "\n",
    "            # Get negative book embeddings\n",
    "            neg_book_data = {\n",
    "                \"book_ids\": torch.LongTensor(all_neg_book_ids),\n",
    "                \"title_ids\": torch.LongTensor(all_neg_features[\"title_ids\"]),\n",
    "                \"author_ids\": torch.LongTensor(all_neg_features[\"author_ids\"]),\n",
    "                \"publisher_ids\": torch.LongTensor(all_neg_features[\"publisher_ids\"]),\n",
    "                \"year_bucket_ids\": torch.LongTensor(all_neg_features[\"year_bucket_ids\"]),\n",
    "            }\n",
    "            \n",
    "            neg_book_emb = model.book_tower(\n",
    "                neg_book_data[\"book_ids\"],\n",
    "                neg_book_data[\"title_ids\"],\n",
    "                neg_book_data[\"author_ids\"], \n",
    "                neg_book_data[\"publisher_ids\"],\n",
    "                neg_book_data[\"year_bucket_ids\"]\n",
    "            )\n",
    "\n",
    "            # Expand user embeddings to match negative books\n",
    "            user_emb_expanded = []\n",
    "            for i, count in enumerate(neg_counts):\n",
    "                user_emb_expanded.extend([user_emb[i]] * count)\n",
    "            user_emb_for_negs = torch.stack(user_emb_expanded)\n",
    "\n",
    "            # Compute similarities\n",
    "            pos_sim = model.compute_similarity(user_emb, pos_book_emb)\n",
    "            neg_sim = model.compute_similarity(user_emb_for_negs, neg_book_emb)\n",
    "\n",
    "            # Reshape negatives back to per-user format and pad\n",
    "            neg_sim_per_user = []\n",
    "            start_idx = 0\n",
    "            for count in neg_counts:\n",
    "                if count > 0:\n",
    "                    neg_sim_per_user.append(neg_sim[start_idx : start_idx + count])\n",
    "                else:\n",
    "                    neg_sim_per_user.append(torch.tensor([]))\n",
    "                start_idx += count\n",
    "\n",
    "            # Pad negatives to same length for batching\n",
    "            max_negs = max(neg_counts) if neg_counts else 1\n",
    "            padded_neg_sim = torch.full((batch_size, max_negs), float(\"-inf\"))\n",
    "\n",
    "            for i, neg_sims in enumerate(neg_sim_per_user):\n",
    "                if len(neg_sims) > 0:\n",
    "                    padded_neg_sim[i, : len(neg_sims)] = neg_sims\n",
    "\n",
    "            # InfoNCE loss: positive is always index 0\n",
    "            logits = torch.cat([pos_sim.unsqueeze(1), padded_neg_sim], dim=1)\n",
    "            labels = torch.zeros(batch_size, dtype=torch.long)\n",
    "\n",
    "            # Only compute loss for samples with valid negatives\n",
    "            valid_mask = torch.tensor([count > 0 for count in neg_counts])\n",
    "            if valid_mask.sum() > 0:\n",
    "                loss = F.cross_entropy(logits[valid_mask], labels[valid_mask])\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "\n",
    "                if batch_idx % 100 == 0:\n",
    "                    current_lr = optimizer.param_groups[0]['lr']\n",
    "                    print(f\"Batch {batch_idx}: loss = {loss.item():.4f}, lr = {current_lr:.2e}\")\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        if num_batches > 0:\n",
    "            avg_loss = total_loss / num_batches\n",
    "            print(f\"Epoch {epoch + 1}: Avg Loss = {avg_loss:.4f}\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch + 1}: No valid batches processed\")\n",
    "            \n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": avg_loss if num_batches > 0 else 0,\n",
    "            \"learning_rate\": optimizer.param_groups[0]['lr']\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orj7e1dhcn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proper collaborative filtering data preparation\n",
    "print(\"=== Proper Collaborative Filtering Data Preparation ===\")\n",
    "\n",
    "# Sample data for faster training (use full dataset later)\n",
    "sample_size = 1_000_000  # Adjust based on your needs\n",
    "if len(ratings_clean) > sample_size:\n",
    "    ratings_sample = ratings_clean.sample(n=sample_size, random_state=42)\n",
    "    print(f\"Sampled {sample_size} rows from {len(ratings_clean)} total ratings\")\n",
    "else:\n",
    "    ratings_sample = ratings_clean\n",
    "    print(f\"Using full dataset: {len(ratings_sample)} rows\")\n",
    "\n",
    "# Filter for users/books with sufficient interactions\n",
    "print(\"\\n=== Filtering for Active Users/Popular Books ===\")\n",
    "user_counts = ratings_sample.groupby('user_idx').size()\n",
    "book_counts = ratings_sample.groupby('book_idx').size()\n",
    "\n",
    "min_user_interactions = 5\n",
    "min_book_interactions = 5\n",
    "\n",
    "active_users = user_counts[user_counts >= min_user_interactions].index\n",
    "popular_books = book_counts[book_counts >= min_book_interactions].index\n",
    "\n",
    "print(f\"Active users: {len(active_users):,} / {len(user_counts):,}\")\n",
    "print(f\"Popular books: {len(popular_books):,} / {len(book_counts):,}\")\n",
    "\n",
    "# Filter dataset\n",
    "filtered_ratings = ratings_sample[\n",
    "    (ratings_sample['user_idx'].isin(active_users)) & \n",
    "    (ratings_sample['book_idx'].isin(popular_books))\n",
    "].copy()\n",
    "\n",
    "print(f\"Filtered dataset: {len(filtered_ratings):,} interactions\")\n",
    "\n",
    "# Create compact user/book indices for the filtered data\n",
    "user_idx_map = {old_idx: new_idx for new_idx, old_idx in enumerate(sorted(active_users))}\n",
    "book_idx_map = {old_idx: new_idx for new_idx, old_idx in enumerate(sorted(popular_books))}\n",
    "\n",
    "filtered_ratings['user_idx_new'] = filtered_ratings['user_idx'].map(user_idx_map)\n",
    "filtered_ratings['book_idx_new'] = filtered_ratings['book_idx'].map(book_idx_map)\n",
    "\n",
    "print(f\"New dimensions - Users: {len(user_idx_map)}, Books: {len(book_idx_map)}\")\n",
    "\n",
    "# Check class balance\n",
    "print(f\"\\nTarget distribution: {filtered_ratings['target'].value_counts().to_dict()}\")\n",
    "\n",
    "# User-based split (proper for collaborative filtering)\n",
    "def user_based_split(df, val_ratio=0.2, test_ratio=0.1):\n",
    "    train_list = []\n",
    "    val_list = []\n",
    "    test_list = []\n",
    "    \n",
    "    for user_id in df['user_idx_new'].unique():\n",
    "        user_data = df[df['user_idx_new'] == user_id]\n",
    "        \n",
    "        if len(user_data) >= 5:  # Only split if user has enough interactions\n",
    "            # First split: train vs (val+test)\n",
    "            n_test_val = max(2, int(len(user_data) * (val_ratio + test_ratio)))\n",
    "            train_data = user_data.iloc[:-n_test_val]\n",
    "            test_val_data = user_data.iloc[-n_test_val:]\n",
    "            \n",
    "            # Second split: val vs test\n",
    "            n_test = max(1, int(len(test_val_data) * (test_ratio / (val_ratio + test_ratio))))\n",
    "            val_data = test_val_data.iloc[:-n_test]\n",
    "            test_data = test_val_data.iloc[-n_test:]\n",
    "            \n",
    "            train_list.append(train_data)\n",
    "            val_list.append(val_data)\n",
    "            test_list.append(test_data)\n",
    "        else:\n",
    "            # Keep all data in training for users with few interactions\n",
    "            train_list.append(user_data)\n",
    "    \n",
    "    return pd.concat(train_list), pd.concat(val_list), pd.concat(test_list)\n",
    "\n",
    "X_train, X_val, X_test = user_based_split(filtered_ratings)\n",
    "\n",
    "print(\"Proper collaborative filtering splits:\")\n",
    "print(f\"Train: {len(X_train):,} interactions\")\n",
    "print(f\"Val: {len(X_val):,} interactions\") \n",
    "print(f\"Test: {len(X_test):,} interactions\")\n",
    "\n",
    "# Store dimensions for model creation\n",
    "num_users_filtered = len(user_idx_map)\n",
    "num_books_filtered = len(book_idx_map)\n",
    "\n",
    "print(f\"\\nModel dimensions: {num_users_filtered} users, {num_books_filtered} books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8wg3lwphd8l",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple dataset for validation (no negative sampling needed)\n",
    "class SimpleRatingsDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        return {\n",
    "            'user_id': torch.LongTensor([row['user_idx_new']]),\n",
    "            'book_id': torch.LongTensor([row['book_idx_new']]),\n",
    "            'target': torch.FloatTensor([row['target']])\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "print(\"\\n=== Creating Datasets ===\")\n",
    "train_dataset = HybridContrastiveDataset(X_train, num_random_negatives=5)\n",
    "val_dataset = SimpleRatingsDataset(X_val)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 128  # Smaller batch size due to more complex data\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"DataLoader sizes - Train: {len(train_loader)} batches, Val: {len(val_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c55f04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom collate function for variable-length negatives\n",
    "def contrastive_collate_fn(batch):\n",
    "    \"\"\"Collate function for HybridContrastiveDataset\"\"\"\n",
    "    collated = {}\n",
    "    \n",
    "    # Handle single values\n",
    "    for key in ['user_id', 'pos_book_id', 'pos_title_idx', 'pos_author_idx', \n",
    "                'pos_publisher_idx', 'pos_year_idx', 'user_age_idx', 'user_location_idx',\n",
    "                'num_explicit_negs', 'num_random_negs']:\n",
    "        if key in batch[0]:\n",
    "            values = [item[key] for item in batch]\n",
    "            collated[key] = values  # Keep as list for the training function\n",
    "    \n",
    "    # Handle variable-length negatives\n",
    "    neg_keys = ['neg_book_ids', 'neg_title_idx', 'neg_author_idx', 'neg_publisher_idx', 'neg_year_idx']\n",
    "    for key in neg_keys:\n",
    "        if key in batch[0]:\n",
    "            collated[key] = [item[key] for item in batch]\n",
    "    \n",
    "    return collated\n",
    "\n",
    "# Recreate datasets and model with fixes\n",
    "print(\"=== Recreating Datasets and Model ===\")\n",
    "\n",
    "# Create new dataset instances\n",
    "train_dataset = HybridContrastiveDataset(X_train, num_random_negatives=3)\n",
    "\n",
    "# Create model with improved settings\n",
    "model = ContrastiveTwoTowerModel(\n",
    "    num_users=num_users_filtered,\n",
    "    num_books=num_books_filtered, \n",
    "    num_locations=len(feature_mappings['location_vocab']),\n",
    "    num_age_buckets=len(feature_mappings['age_vocab']),\n",
    "    num_titles=len(feature_mappings['title_vocab']),\n",
    "    num_authors=len(feature_mappings['author_vocab']),\n",
    "    num_publishers=len(feature_mappings['publisher_vocab']),\n",
    "    num_year_buckets=len(feature_mappings['year_vocab']),\n",
    "    embedding_dim=64,\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "print(f\"Updated model created with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "# Create new data loader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=64,  # Reasonable batch size\n",
    "    shuffle=True, \n",
    "    collate_fn=contrastive_collate_fn\n",
    ")\n",
    "\n",
    "print(f\"Updated train_loader with {len(train_loader)} batches\")\n",
    "\n",
    "# Test the batch structure\n",
    "test_batch = next(iter(train_loader))\n",
    "print(f\"Batch keys: {sorted(test_batch.keys())}\")\n",
    "print(f\"Batch size: {len(test_batch['user_id'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7vog39v7yu7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the improved training setup\n",
    "print(\"=== Testing Improved Training Setup ===\")\n",
    "\n",
    "# Start with a few batches to verify everything works\n",
    "print(\"Running 5 test batches...\")\n",
    "model.train()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    if batch_idx >= 5:  # Only test first 5 batches\n",
    "        break\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    batch_size = len(batch[\"user_id\"])\n",
    "    \n",
    "    # Convert to tensors (fixed data handling)\n",
    "    user_data = {\n",
    "        \"user_ids\": torch.LongTensor([batch[\"user_id\"][i] for i in range(batch_size)]),\n",
    "        \"location_ids\": torch.LongTensor([batch[\"user_location_idx\"][i] for i in range(batch_size)]),\n",
    "        \"age_bucket_ids\": torch.LongTensor([batch[\"user_age_idx\"][i] for i in range(batch_size)]),\n",
    "    }\n",
    "\n",
    "    pos_book_data = {\n",
    "        \"book_ids\": torch.LongTensor([batch[\"pos_book_id\"][i] for i in range(batch_size)]),\n",
    "        \"title_ids\": torch.LongTensor([batch[\"pos_title_idx\"][i] for i in range(batch_size)]),\n",
    "        \"author_ids\": torch.LongTensor([batch[\"pos_author_idx\"][i] for i in range(batch_size)]),\n",
    "        \"publisher_ids\": torch.LongTensor([batch[\"pos_publisher_idx\"][i] for i in range(batch_size)]),\n",
    "        \"year_bucket_ids\": torch.LongTensor([batch[\"pos_year_idx\"][i] for i in range(batch_size)]),\n",
    "    }\n",
    "    \n",
    "    # Get embeddings and compute similarity\n",
    "    user_emb, pos_book_emb = model(user_data, pos_book_data)\n",
    "    pos_sim = model.compute_similarity(user_emb, pos_book_emb)\n",
    "    \n",
    "    print(f\"Batch {batch_idx}: user_emb shape={user_emb.shape}, pos_sim range=[{pos_sim.min():.3f}, {pos_sim.max():.3f}]\")\n",
    "    \n",
    "    # Simple test loss (just positive similarities)\n",
    "    test_loss = -pos_sim.mean()  # Simple test loss\n",
    "    test_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"  Test loss: {test_loss.item():.4f}\")\n",
    "\n",
    "print(\"\\n✅ Test successful! All features are working.\")\n",
    "print(\"\\nNow starting full training with improved setup...\")\n",
    "\n",
    "# Run full training with fixes\n",
    "train(model, train_loader, epochs=3, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159ba5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
