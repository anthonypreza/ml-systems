{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7866ebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wandb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb02c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb run\n",
    "run = wandb.init(\n",
    "    project=\"book-recommendation\", group=\"dev\", job_type=\"feat_eng\", save_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727d8dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_json_file(filepath: str, sample_size: int = 100_000) -> pd.DataFrame:\n",
    "    \"\"\"Sample a JSON lines file and return a DataFrame.\"\"\"\n",
    "    import random\n",
    "\n",
    "    sampled_lines = []\n",
    "    total_lines = 0\n",
    "\n",
    "    print(f\"Sampling {sample_size} lines from {filepath}...\")\n",
    "\n",
    "    print(\"Estimating total number of lines in the file...\")\n",
    "    with gzip.open(filepath, \"rt\") as f:\n",
    "        for i, _ in enumerate(f):\n",
    "            total_lines += 1\n",
    "            if i > 1_000_000:  # Limit to first million lines for speed\n",
    "                break\n",
    "\n",
    "    # Calculate sampling probability\n",
    "    if total_lines > sample_size:\n",
    "        sample_prob = sample_size / total_lines\n",
    "    else:\n",
    "        sample_prob = 1.0\n",
    "\n",
    "    print(\n",
    "        f\"Total lines estimated: {total_lines}. Sampling probability: {sample_prob:.6f}\"\n",
    "    )\n",
    "\n",
    "    # Collect sampled\n",
    "    with gzip.open(filepath, \"rt\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                if random.random() < sample_prob:\n",
    "                    sampled_lines.append(json.loads(line))\n",
    "                    if len(sampled_lines) >= sample_size:\n",
    "                        break\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "    print(f\"Sampled {len(sampled_lines)} lines.\")\n",
    "    return pd.DataFrame(sampled_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8cc4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_records = sample_json_file(\"../data/goodreads_interactions_dedup.json.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599fff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df = pd.DataFrame(interactions_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225a6070",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406c2342",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_records = sample_json_file(\"../data/goodreads_books.json.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e171d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df = pd.DataFrame(books_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7111942d",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f66cbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_records = sample_json_file(\n",
    "    \"../data/goodreads_book_genres_initial.json.gz\", sample_size=10_000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9b8cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_df = pd.DataFrame(genres_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791385fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bc78e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_records = sample_json_file(\n",
    "    \"../data/goodreads_book_authors.json.gz\", sample_size=10_000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d12e3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_df = pd.DataFrame(author_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3551fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e133c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "print(\"=== PREPROCESSING ===\")\n",
    "print(f\"Interactions columns before: {interactions_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040026d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73791442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep essential columns for positive interactions\n",
    "core_interactions = interactions_df[\n",
    "    [\n",
    "        \"user_id\",\n",
    "        \"book_id\",\n",
    "        \"rating\",\n",
    "        \"is_read\",\n",
    "        \"review_text_incomplete\",\n",
    "        \"date_added\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4d866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter users and books with at least 3 interactions\n",
    "user_counts = core_interactions[\"user_id\"].value_counts()\n",
    "book_counts = core_interactions[\"book_id\"].value_counts()\n",
    "\n",
    "active_users = user_counts[user_counts >= 3].index\n",
    "popular_books = book_counts[book_counts >= 3].index\n",
    "\n",
    "filtered_interactions = core_interactions[\n",
    "    core_interactions[\"user_id\"].isin(user_counts[user_counts >= 3].index)\n",
    "    & core_interactions[\"book_id\"].isin(book_counts[book_counts >= 3].index)\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cfa21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "print(\"=== FEATURE ENGINEERING ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4fa162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add derived features to positive interactions\n",
    "filtered_interactions[\"is_read\"] = filtered_interactions[\"is_read\"].astype(int)\n",
    "# Skip rating, explicit_rating, review_length - these are outcomes of reading, not predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b8de7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute user and book profiles from interactions only (no target leakage)\n",
    "user_features = filtered_interactions.groupby(\"user_id\").agg(\n",
    "    {\"rating\": [\"mean\", \"std\", \"count\"], \"book_id\": \"nunique\"}\n",
    ")\n",
    "\n",
    "book_features = filtered_interactions.groupby(\"book_id\").agg(\n",
    "    {\"rating\": [\"mean\", \"std\", \"count\"], \"user_id\": \"nunique\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92c8e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the MultiIndex columns\n",
    "user_features.columns = [\"_\".join(col).strip() for col in user_features.columns]\n",
    "book_features.columns = [\"_\".join(col).strip() for col in book_features.columns]\n",
    "\n",
    "# Reset index to make user_id/book_id regular columns\n",
    "user_features = user_features.reset_index()\n",
    "book_features = book_features.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a6e2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create completion prediction dataset (only real interactions)\n",
    "completion_samples = filtered_interactions[\n",
    "    [\n",
    "        \"user_id\",\n",
    "        \"book_id\",\n",
    "        \"is_read\",  # TARGET: Will user complete this book?\n",
    "        \"date_added\",  # Only temporal info, no interaction outcomes\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "print(\"=== COMPLETION PREDICTION DATASET ===\")\n",
    "print(f\"Dataset shape: {completion_samples.shape}\")\n",
    "print(\"Target distribution:\")\n",
    "print(completion_samples[\"is_read\"].value_counts())\n",
    "print(f\"Completion rate: {completion_samples['is_read'].mean():.3f}\")\n",
    "\n",
    "train_data = completion_samples.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ec9c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No synthetic negatives needed - using real interactions only\n",
    "all_samples = train_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cbcebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip negative sample creation\n",
    "# all_samples = pd.concat([positive_samples, negative_samples], ignore_index=True)\n",
    "\n",
    "# Add user and book profiles to our completion dataset\n",
    "train_data = all_samples.merge(\n",
    "    user_features, on=\"user_id\", how=\"left\", suffixes=(\"\", \"_user\")\n",
    ")\n",
    "train_data = train_data.merge(\n",
    "    book_features, on=\"book_id\", how=\"left\", suffixes=(\"\", \"_book\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f2634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip negative sample creation - using only real interactions now\n",
    "train_data = all_samples.merge(\n",
    "    user_features, on=\"user_id\", how=\"left\", suffixes=(\"\", \"_user\")\n",
    ")\n",
    "train_data = train_data.merge(\n",
    "    book_features, on=\"book_id\", how=\"left\", suffixes=(\"\", \"_book\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2815b3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity features\n",
    "train_data[\"rating_diff\"] = abs(\n",
    "    train_data[\"rating_mean\"] - train_data[\"rating_mean_book\"]\n",
    ")\n",
    "train_data[\"rating_similarity\"] = 1 / 1 + train_data[\"rating_diff\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b530777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_languages = train_data.merge(books_df[[\"book_id\", \"language_code\"]], on=\"book_id\")\n",
    "user_language_prefs = user_languages.groupby(\"user_id\")[\"language_code\"].apply(\n",
    "    lambda x: x.value_counts().index[0] if len(x) > 0 else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4c148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_languages = books_df.set_index(\"book_id\")[\"language_code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0c41ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"user_preferred_lang\"] = train_data[\"user_id\"].map(user_language_prefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a54982e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"book_language\"] = train_data[\"book_id\"].map(book_languages)\n",
    "train_data[\"language_match\"] = (\n",
    "    train_data[\"user_preferred_lang\"] == train_data[\"book_language\"]\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26989fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_books_with_genres = filtered_interactions.merge(\n",
    "    genres_df, on=\"book_id\", how=\"left\"\n",
    ")\n",
    "train_data = train_data.merge(genres_df, on=\"book_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c66bef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_genre_similarity(user_profile, book_genres):\n",
    "    \"\"\"Compute cosine similarity between user profile and book\"\"\"\n",
    "    if not isinstance(book_genres, dict) or not book_genres:\n",
    "        return 0\n",
    "\n",
    "    # Get common genres\n",
    "    common_genres = set(user_profile.keys()) & set(book_genres.keys())\n",
    "    if not common_genres:\n",
    "        return 0\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    dot_product = sum(\n",
    "        user_profile.get(genre, 0) * book_genres.get(genre, 0)\n",
    "        for genre in common_genres\n",
    "    )\n",
    "    user_norm = sum(v**2 for v in user_profile.values()) ** 0.5\n",
    "    book_norm = sum(v**2 for v in book_genres.values()) ** 0.5\n",
    "\n",
    "    if user_norm == 0 or book_norm == 0:\n",
    "        return 0\n",
    "\n",
    "    return dot_product / (user_norm * book_norm)\n",
    "\n",
    "\n",
    "user_genre_profiles = {}\n",
    "\n",
    "for user_id in user_books_with_genres[\"user_id\"].unique():\n",
    "    user_books = user_books_with_genres[user_books_with_genres[\"user_id\"] == user_id]\n",
    "    user_profile = {}\n",
    "\n",
    "    for _, row in user_books.iterrows():\n",
    "        genres_dict = row[\"genres\"]\n",
    "        if isinstance(genres_dict, dict) and len(genres_dict) > 0:\n",
    "            # Normalize weights within each book\n",
    "            total_weight = sum(genres_dict.values())\n",
    "            for genre, weight in genres_dict.items():\n",
    "                normalized_weight = weight / total_weight if total_weight > 0 else 0\n",
    "                user_profile[genre] = user_profile.get(genre, 0) + normalized_weight\n",
    "\n",
    "    user_genre_profiles[user_id] = user_profile\n",
    "\n",
    "\n",
    "# Now compute similarities with the corrected profiles\n",
    "def get_genre_similarity_final(row):\n",
    "    user_id = row[\"user_id\"]\n",
    "    book_genres = row[\"genres\"]\n",
    "\n",
    "    # Get user profile\n",
    "    user_profile = user_genre_profiles.get(user_id, {})\n",
    "\n",
    "    # Handle NaN book genres\n",
    "    if pd.isna(book_genres) or not isinstance(book_genres, dict):\n",
    "        return 0.0\n",
    "\n",
    "    # Compute similarity\n",
    "    return compute_genre_similarity(user_profile, book_genres)\n",
    "\n",
    "\n",
    "train_data[\"genre_similarity\"] = train_data.apply(get_genre_similarity_final, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68da8fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.merge(\n",
    "    books_df[[\"book_id\", \"publication_year\"]], on=\"book_id\", how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cqrfyi2oyx",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add book format features\n",
    "train_data = train_data.merge(\n",
    "    books_df[[\"book_id\", \"is_ebook\", \"format\"]], on=\"book_id\", how=\"left\"\n",
    ")\n",
    "\n",
    "# Clean and encode is_ebook\n",
    "train_data[\"is_ebook\"] = (\n",
    "    train_data[\"is_ebook\"]\n",
    "    .map({\"true\": 1, \"True\": 1, True: 1, \"false\": 0, \"False\": 0, False: 0})\n",
    "    .fillna(0)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "# Get user ebook preferences\n",
    "user_ebook_preference = filtered_interactions.merge(\n",
    "    books_df[[\"book_id\", \"is_ebook\"]], on=\"book_id\"\n",
    ")\n",
    "user_ebook_preference[\"is_ebook\"] = (\n",
    "    user_ebook_preference[\"is_ebook\"]\n",
    "    .map({\"true\": 1, \"True\": 1, True: 1, \"false\": 0, \"False\": 0, False: 0})\n",
    "    .fillna(0)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "user_ebook_pref = user_ebook_preference.groupby(\"user_id\")[\"is_ebook\"].mean()\n",
    "train_data[\"user_ebook_preference\"] = (\n",
    "    train_data[\"user_id\"].map(user_ebook_pref).fillna(0.5)\n",
    ")\n",
    "\n",
    "# Ebook preference match\n",
    "train_data[\"ebook_preference_match\"] = 1 - abs(\n",
    "    train_data[\"user_ebook_preference\"] - train_data[\"is_ebook\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x7raqvbtpjm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format feature engineering\n",
    "print(\"=== FORMAT FEATURE ENGINEERING ===\")\n",
    "\n",
    "# Check format distribution\n",
    "print(\"Format value counts:\")\n",
    "format_counts = books_df[\"format\"].value_counts().head(10)\n",
    "print(format_counts)\n",
    "\n",
    "# Clean format data - handle missing and normalize\n",
    "train_data[\"format\"] = train_data[\"format\"].fillna(\"Unknown\")\n",
    "\n",
    "# Get top formats (covers 80%+ of data)\n",
    "top_formats = [\n",
    "    \"Paperback\",\n",
    "    \"Hardcover\",\n",
    "    \"Kindle Edition\",\n",
    "    \"Mass Market Paperback\",\n",
    "    \"ebook\",\n",
    "    \"Unknown\",\n",
    "]\n",
    "\n",
    "\n",
    "# Create format categories\n",
    "def categorize_format(format_val):\n",
    "    if pd.isna(format_val) or format_val == \"\" or format_val == \"Unknown\":\n",
    "        return \"Unknown\"\n",
    "    elif \"Paperback\" in str(format_val):\n",
    "        return \"Paperback\"\n",
    "    elif \"Hardcover\" in str(format_val) or \"Hardback\" in str(format_val):\n",
    "        return \"Hardcover\"\n",
    "    elif (\n",
    "        \"Kindle\" in str(format_val)\n",
    "        or \"ebook\" in str(format_val)\n",
    "        or \"eBook\" in str(format_val)\n",
    "    ):\n",
    "        return \"Digital\"\n",
    "    elif \"Mass Market\" in str(format_val):\n",
    "        return \"Mass Market\"\n",
    "    elif \"Audio\" in str(format_val):\n",
    "        return \"Audio\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "\n",
    "train_data[\"format_category\"] = train_data[\"format\"].apply(categorize_format)\n",
    "\n",
    "print(\"\\nFormat categories:\")\n",
    "print(train_data[\"format_category\"].value_counts())\n",
    "\n",
    "# One-hot encode format categories (for top categories)\n",
    "format_dummies = pd.get_dummies(train_data[\"format_category\"], prefix=\"format\").astype(\n",
    "    int\n",
    ")\n",
    "train_data = pd.concat([train_data, format_dummies], axis=1)\n",
    "\n",
    "# Ensure boolean dummies are 0/1 ints\n",
    "for _c in [\n",
    "    c for c in train_data.columns if c.startswith(\"format_\") and c != \"format_category\"\n",
    "]:\n",
    "    train_data[_c] = train_data[_c].astype(int)\n",
    "\n",
    "# Get user format preferences\n",
    "user_format_data = filtered_interactions.merge(\n",
    "    books_df[[\"book_id\", \"format\"]], on=\"book_id\"\n",
    ")\n",
    "user_format_data[\"format_category\"] = user_format_data[\"format\"].apply(\n",
    "    categorize_format\n",
    ")\n",
    "\n",
    "# Calculate user's preferred format (most common format they interact with)\n",
    "user_format_prefs = user_format_data.groupby(\"user_id\")[\"format_category\"].apply(\n",
    "    lambda x: x.value_counts().index[0] if len(x) > 0 else \"Unknown\"\n",
    ")\n",
    "\n",
    "# Add user format preference match\n",
    "train_data[\"user_preferred_format\"] = (\n",
    "    train_data[\"user_id\"].map(user_format_prefs).fillna(\"Unknown\")\n",
    ")\n",
    "train_data[\"format_preference_match\"] = (\n",
    "    train_data[\"user_preferred_format\"] == train_data[\"format_category\"]\n",
    ").astype(int)\n",
    "\n",
    "print(\n",
    "    f\"✅ Added format features: {len(format_dummies.columns)} one-hot encoded categories\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355da897",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"publication_year\"] = pd.to_numeric(\n",
    "    train_data[\"publication_year\"], errors=\"coerce\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831ec488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_year_preferences(filtered_interactions, books_df):\n",
    "    user_book_years = filtered_interactions.merge(\n",
    "        books_df[[\"book_id\", \"publication_year\"]], on=\"book_id\"\n",
    "    )\n",
    "    user_book_years[\"publication_year\"] = pd.to_numeric(\n",
    "        user_book_years[\"publication_year\"], errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    user_year_prefs = (\n",
    "        user_book_years.groupby(\"user_id\")[\"publication_year\"]\n",
    "        .agg([\"mean\", \"std\"])\n",
    "        .fillna(0)\n",
    "    )\n",
    "    user_year_prefs.columns = [\"user_year_pref_mean\", \"user_year_pref_std\"]\n",
    "    return user_year_prefs\n",
    "\n",
    "\n",
    "user_year_prefs = compute_year_preferences(filtered_interactions, books_df)\n",
    "train_data = train_data.merge(\n",
    "    user_year_prefs, on=\"user_id\", how=\"left\", suffixes=(\"\", \"_user_pref\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488dc63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === AUTHOR-BASED FEATURES ===\n",
    "print(\"=== AUTHOR FEATURE ENGINEERING ===\")\n",
    "\n",
    "\n",
    "def extract_primary_author_id(authors_field):\n",
    "    try:\n",
    "        if isinstance(authors_field, list) and len(authors_field) > 0:\n",
    "            a0 = authors_field[0]\n",
    "            if isinstance(a0, dict) and \"author_id\" in a0:\n",
    "                return str(a0.get(\"author_id\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "# Map each book to a primary author id\n",
    "books_author_map = books_df[[\"book_id\", \"authors\"]].copy()\n",
    "books_author_map[\"primary_author_id\"] = books_author_map[\"authors\"].apply(\n",
    "    extract_primary_author_id\n",
    ")\n",
    "\n",
    "# Bring primary author onto train_data\n",
    "train_data = train_data.merge(\n",
    "    books_author_map[[\"book_id\", \"primary_author_id\"]], on=\"book_id\", how=\"left\"\n",
    ")\n",
    "\n",
    "# Prepare author stats\n",
    "author_cols = [\n",
    "    \"author_id\",\n",
    "    \"average_rating\",\n",
    "    \"ratings_count\",\n",
    "    \"text_reviews_count\",\n",
    "    \"name\",\n",
    "]\n",
    "authors_stats = authors_df[author_cols].copy()\n",
    "authors_stats[\"author_id\"] = authors_stats[\"author_id\"].astype(str)\n",
    "for c in [\"average_rating\", \"ratings_count\", \"text_reviews_count\"]:\n",
    "    authors_stats[c] = pd.to_numeric(authors_stats[c], errors=\"coerce\").fillna(0)\n",
    "\n",
    "# Merge author stats to training data\n",
    "train_data = train_data.merge(\n",
    "    authors_stats.rename(\n",
    "        columns={\n",
    "            \"author_id\": \"primary_author_id\",\n",
    "            \"average_rating\": \"author_avg_rating\",\n",
    "            \"ratings_count\": \"author_ratings_count\",\n",
    "            \"text_reviews_count\": \"author_text_reviews_count\",\n",
    "            \"name\": \"author_name\",\n",
    "        }\n",
    "    ),\n",
    "    on=\"primary_author_id\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# User–author familiarity signals\n",
    "ua = filtered_interactions.merge(\n",
    "    books_author_map[[\"book_id\", \"primary_author_id\"]], on=\"book_id\", how=\"left\"\n",
    ")\n",
    "ua_counts = (\n",
    "    ua.groupby([\"user_id\", \"primary_author_id\"])\n",
    "    .size()\n",
    "    .rename(\"user_author_interactions\")\n",
    ")\n",
    "user_total = ua.groupby(\"user_id\").size().rename(\"user_total_interactions\")\n",
    "\n",
    "tmp = train_data[[\"user_id\", \"primary_author_id\"]].copy()\n",
    "tmp = tmp.merge(\n",
    "    ua_counts.reset_index(), on=[\"user_id\", \"primary_author_id\"], how=\"left\"\n",
    ")\n",
    "tmp = tmp.merge(user_total.reset_index(), on=\"user_id\", how=\"left\")\n",
    "train_data[\"user_author_interaction_count\"] = (\n",
    "    tmp[\"user_author_interactions\"].fillna(0).astype(float)\n",
    ")\n",
    "train_data[\"user_author_interaction_ratio\"] = (\n",
    "    (\n",
    "        train_data[\"user_author_interaction_count\"]\n",
    "        / tmp[\"user_total_interactions\"].replace({0: pd.NA})\n",
    "    )\n",
    "    .fillna(0)\n",
    "    .astype(float)\n",
    ")\n",
    "\n",
    "# Author popularity signals\n",
    "train_data[\"author_popularity_log\"] = np.log1p(\n",
    "    train_data[\"author_ratings_count\"].fillna(0)\n",
    ")\n",
    "\n",
    "print(\"✅ Added author features: avg_rating, popularity, and user-author familiarity\")\n",
    "# Simple flag: has the user read this author before?\n",
    "train_data[\"has_read_author_before\"] = (\n",
    "    train_data[\"user_author_interaction_count\"] > 0\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0adbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TEXT SIMILARITY FEATURES (REVIEW TEXT + TITLE) ===\n",
    "print(\"=== TEXT SIMILARITY FEATURES ===\")\n",
    "\n",
    "# Build corpora from interactions\n",
    "reviews = filtered_interactions[\n",
    "    [\"user_id\", \"book_id\", \"review_text_incomplete\"]\n",
    "].dropna()\n",
    "reviews[\"review_text_incomplete\"] = reviews[\"review_text_incomplete\"].astype(str)\n",
    "\n",
    "# Aggregate user and book review text\n",
    "user_review_text = (\n",
    "    reviews.groupby(\"user_id\")[\"review_text_incomplete\"]\n",
    "    .apply(lambda x: \" \".join(x.astype(str)))\n",
    "    .reset_index()\n",
    ")\n",
    "book_review_text = (\n",
    "    reviews.groupby(\"book_id\")[\"review_text_incomplete\"]\n",
    "    .apply(lambda x: \" \".join(x.astype(str)))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Fit a single TF-IDF on combined corpus to align spaces\n",
    "combined_corpus = pd.concat(\n",
    "    [\n",
    "        user_review_text[\"review_text_incomplete\"],\n",
    "        book_review_text[\"review_text_incomplete\"],\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "if len(combined_corpus) > 0:\n",
    "    tfidf_reviews = TfidfVectorizer(stop_words=\"english\", max_features=5000, min_df=5)\n",
    "    tfidf_reviews.fit(combined_corpus)\n",
    "\n",
    "    U_rev = normalize(\n",
    "        tfidf_reviews.transform(user_review_text[\"review_text_incomplete\"])\n",
    "    )\n",
    "    B_rev = normalize(\n",
    "        tfidf_reviews.transform(book_review_text[\"review_text_incomplete\"])\n",
    "    )\n",
    "\n",
    "    # Build lookup dicts of normalized sparse vectors\n",
    "    user_ids = user_review_text[\"user_id\"].tolist()\n",
    "    book_ids = book_review_text[\"book_id\"].tolist()\n",
    "    user_rev_vec = {uid: U_rev[i] for i, uid in enumerate(user_ids)}\n",
    "    book_rev_vec = {bid: B_rev[i] for i, bid in enumerate(book_ids)}\n",
    "\n",
    "    # Compute cosine similarity (dot of normalized vectors)\n",
    "    def review_text_similarity(row):\n",
    "        u = user_rev_vec.get(row[\"user_id\"])\n",
    "        b = book_rev_vec.get(row[\"book_id\"])\n",
    "        if u is None or b is None:\n",
    "            return 0.0\n",
    "        return float(u.multiply(b).sum())\n",
    "\n",
    "    train_data[\"review_text_similarity\"] = train_data.apply(\n",
    "        review_text_similarity, axis=1\n",
    "    )\n",
    "else:\n",
    "    train_data[\"review_text_similarity\"] = 0.0\n",
    "\n",
    "print(\"✅ Added review_text_similarity\")\n",
    "\n",
    "# Title similarity (word n-grams)\n",
    "book_titles = books_df[[\"book_id\", \"title\"]].dropna().copy()\n",
    "book_titles[\"title\"] = book_titles[\"title\"].astype(str)\n",
    "user_titles = filtered_interactions.merge(book_titles, on=\"book_id\", how=\"left\")\n",
    "user_titles = user_titles.dropna(subset=[\"title\"])\n",
    "user_title_agg = (\n",
    "    user_titles.groupby(\"user_id\")[\"title\"]\n",
    "    .apply(lambda x: \" \".join(x.astype(str)))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "combined_titles = pd.concat(\n",
    "    [user_title_agg[\"title\"], book_titles[\"title\"]], ignore_index=True\n",
    ")\n",
    "\n",
    "if len(combined_titles) > 0:\n",
    "    tfidf_titles = TfidfVectorizer(\n",
    "        stop_words=\"english\", ngram_range=(1, 2), max_features=3000, min_df=3\n",
    "    )\n",
    "    tfidf_titles.fit(combined_titles)\n",
    "\n",
    "    U_tit = normalize(tfidf_titles.transform(user_title_agg[\"title\"]))\n",
    "    B_tit = normalize(tfidf_titles.transform(book_titles[\"title\"]))\n",
    "\n",
    "    user_tit_vec = {\n",
    "        uid: U_tit[i] for i, uid in enumerate(user_title_agg[\"user_id\"].tolist())\n",
    "    }\n",
    "    book_tit_vec = {\n",
    "        bid: B_tit[i] for i, bid in enumerate(book_titles[\"book_id\"].tolist())\n",
    "    }\n",
    "\n",
    "    def title_text_similarity(row):\n",
    "        u = user_tit_vec.get(row[\"user_id\"])\n",
    "        b = book_tit_vec.get(row[\"book_id\"])\n",
    "        if u is None or b is None:\n",
    "            return 0.0\n",
    "        return float(u.multiply(b).sum())\n",
    "\n",
    "    train_data[\"title_similarity\"] = train_data.apply(title_text_similarity, axis=1)\n",
    "else:\n",
    "    train_data[\"title_similarity\"] = 0.0\n",
    "\n",
    "print(\"✅ Added title_similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaba5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_data[\"year_diff\"] = abs(\n",
    "    train_data[\"publication_year\"] - train_data[\"user_year_pref_mean\"]\n",
    ")\n",
    "train_data[\"year_similarity\"] = np.exp(\n",
    "    -train_data[\"year_diff\"] / (train_data[\"user_year_pref_std\"] + 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07590416",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_length_dict = pd.to_numeric(\n",
    "    books_df.set_index(\"book_id\")[\"num_pages\"], errors=\"coerce\"\n",
    ")\n",
    "train_data[\"book_length\"] = train_data[\"book_id\"].map(book_length_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe93094",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_length_prefs = filtered_interactions.merge(\n",
    "    books_df[[\"book_id\", \"num_pages\"]], on=\"book_id\"\n",
    ")\n",
    "user_length_prefs[\"num_pages\"] = pd.to_numeric(\n",
    "    user_length_prefs[\"num_pages\"], errors=\"coerce\"\n",
    ")\n",
    "user_avg_length = user_length_prefs.groupby(\"user_id\")[\"num_pages\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9cfafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"user_avg_book_length\"] = train_data[\"user_id\"].map(user_avg_length)\n",
    "train_data[\"length_similarity\"] = 1 / (\n",
    "    1 + abs(train_data[\"book_length\"] - train_data[\"user_avg_book_length\"]) / 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ae6138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading frequency patterns\n",
    "train_data[\"date_added\"] = pd.to_datetime(\n",
    "    train_data[\"date_added\"], format=\"%Y-%m-%d %H:%M:%S%z\", errors=\"coerce\"\n",
    ")\n",
    "train_data[\"hour_added\"] = train_data[\"date_added\"].dt.hour\n",
    "train_data[\"day_of_week\"] = train_data[\"date_added\"].dt.dayofweek\n",
    "train_data[\"month_added\"] = train_data[\"date_added\"].dt.month\n",
    "\n",
    "# TODO: Explore seasonal patterns at some point, for now this approach doesn' work\n",
    "# # Seasonal reading patterns - simpler approach\n",
    "# user_seasonal_patterns = filtered_interactions.copy()\n",
    "# user_seasonal_patterns['date_added'] = pd.to_datetime(user_seasonal_patterns['date_added'], format='%a %b %d %H:%M:%S %z %Y', errors='coerce')\n",
    "\n",
    "# # Create season column with NaN for invalid dates\n",
    "# user_seasonal_patterns['season'] = user_seasonal_patterns['date_added'].dt.month % 12 // 3\n",
    "\n",
    "# # User's preferred reading season (dropna handles NaN values automatically)\n",
    "# user_season_prefs = user_seasonal_patterns.dropna(subset=['season']).groupby('user_id')['season'].apply(\n",
    "#     lambda x: x.value_counts().index[0] if len(x) > 0 else None,\n",
    "#     include_groups=False\n",
    "# )\n",
    "\n",
    "# train_data['book_season'] = train_data['month_added'] % 12 // 3\n",
    "# train_data['user_preferred_season'] = train_data['user_id'].map(user_season_prefs)\n",
    "# train_data['season_match'] = (train_data['user_preferred_season'] == train_data['book_season']).astype(int)\n",
    "\n",
    "# TODO: FUTURE IMPROVEMENT - Add scalable leave-one-out completion rates\n",
    "# Current challenge: Preventing target leakage while maintaining computational efficiency\n",
    "#\n",
    "# COMMENTED OUT: Computationally expensive leave-one-out approach\n",
    "# - User completion rate (excluding current book)\n",
    "# - Book completion rate (excluding current user)\n",
    "#\n",
    "# Potential optimizations for future implementation:\n",
    "# 1. Pre-compute global completion rates and subtract current interaction\n",
    "# 2. Use vectorized operations instead of apply()\n",
    "# 3. Implement efficient caching for repeated calculations\n",
    "# 4. Consider approximate methods (e.g., sample-based estimates)\n",
    "#\n",
    "# For now, skipping these features to maintain training speed and scalability\n",
    "\n",
    "# def compute_user_completion_rate_loo(row):\n",
    "#     \"\"\"Compute user completion rate excluding current book (leave-one-out)\"\"\"\n",
    "#     user_id = row['user_id']\n",
    "#     book_id = row['book_id']\n",
    "#\n",
    "#     # Get all interactions for this user EXCEPT current book\n",
    "#     user_other_books = filtered_interactions[\n",
    "#         (filtered_interactions['user_id'] == user_id) &\n",
    "#         (filtered_interactions['book_id'] != book_id)\n",
    "#     ]\n",
    "#\n",
    "#     if len(user_other_books) == 0:\n",
    "#         return 0.5  # Default if no other books (neutral prior)\n",
    "#\n",
    "#     return user_other_books['is_read'].mean()\n",
    "\n",
    "# print(\"Computing leave-one-out user completion rates (no target leakage)...\")\n",
    "# train_data[\"user_completion_rate\"] = train_data.apply(compute_user_completion_rate_loo, axis=1)\n",
    "\n",
    "# def compute_book_completion_rate_loo(row):\n",
    "#     \"\"\"Compute book completion rate excluding current user (leave-one-out)\"\"\"\n",
    "#     user_id = row['user_id']\n",
    "#     book_id = row['book_id']\n",
    "#\n",
    "#     # Get all interactions for this book EXCEPT current user\n",
    "#     book_other_users = filtered_interactions[\n",
    "#         (filtered_interactions['book_id'] == book_id) &\n",
    "#         (filtered_interactions['user_id'] != user_id)\n",
    "#     ]\n",
    "#\n",
    "#     if len(book_other_users) == 0:\n",
    "#         return 0.5  # Default if no other users (neutral prior)\n",
    "#\n",
    "#     return book_other_users['is_read'].mean()\n",
    "\n",
    "# print(\"Computing leave-one-out book completion rates (no target leakage)...\")\n",
    "# train_data[\"book_typical_completion\"] = train_data.apply(compute_book_completion_rate_loo, axis=1)\n",
    "\n",
    "# train_data[\"completion_alignment\"] = 1 - abs(\n",
    "#     train_data[\"user_completion_rate\"] - train_data[\"book_typical_completion\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3904a7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "def get_sentiment_polarity(text):\n",
    "    \"\"\"Get sentiment polarity (-1 to 1)\"\"\"\n",
    "    if pd.isna(text) or len(str(text).strip()) == 0:\n",
    "        return 0.0\n",
    "    try:\n",
    "        return TextBlob(str(text)).sentiment.polarity\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "# User sentiment patterns\n",
    "user_sentiment_profiles = filtered_interactions.groupby(\"user_id\")[\n",
    "    \"review_text_incomplete\"\n",
    "].apply(lambda reviews: np.mean([get_sentiment_polarity(review) for review in reviews]))\n",
    "\n",
    "# Book sentiment patterns\n",
    "book_sentiment_profiles = filtered_interactions.groupby(\"book_id\")[\n",
    "    \"review_text_incomplete\"\n",
    "].apply(lambda reviews: np.mean([get_sentiment_polarity(review) for review in reviews]))\n",
    "\n",
    "# Add sentiment similarity\n",
    "train_data[\"user_avg_sentiment\"] = train_data[\"user_id\"].map(user_sentiment_profiles)\n",
    "train_data[\"book_avg_sentiment\"] = train_data[\"book_id\"].map(book_sentiment_profiles)\n",
    "train_data[\"sentiment_similarity\"] = (\n",
    "    1 - abs(train_data[\"user_avg_sentiment\"] - train_data[\"book_avg_sentiment\"]) / 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q8cmysi0z7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historical rating patterns (no leakage - uses OTHER books user has rated)\n",
    "print(\"=== HISTORICAL RATING PATTERNS ===\")\n",
    "\n",
    "# TODO: FUTURE IMPROVEMENT - Add temporal filtering\n",
    "# Currently using all ratings from user's history and others' ratings of books\n",
    "# For production, should only use ratings that occurred BEFORE current interaction\n",
    "# This prevents future information leakage but adds complexity\n",
    "\n",
    "\n",
    "# User's rating behavior on OTHER books (not current one)\n",
    "def compute_user_rating_history(interactions):\n",
    "    \"\"\"Compute user rating patterns from their history (excluding current book)\"\"\"\n",
    "    user_rating_patterns = {}\n",
    "\n",
    "    for user_id in interactions[\"user_id\"].unique():\n",
    "        user_books = interactions[interactions[\"user_id\"] == user_id]\n",
    "\n",
    "        if len(user_books) > 1:  # Need multiple books for patterns\n",
    "            # Only use ratings (NOT is_read which is our target)\n",
    "            rated_books = user_books[user_books[\"rating\"] > 0]\n",
    "\n",
    "            if len(rated_books) > 0:\n",
    "                user_rating_patterns[user_id] = {\n",
    "                    \"user_avg_rating_historical\": rated_books[\"rating\"].mean(),\n",
    "                    \"user_rating_std_historical\": rated_books[\"rating\"].std()\n",
    "                    if len(rated_books) > 1\n",
    "                    else 0,\n",
    "                    \"user_harsh_rater\": (rated_books[\"rating\"] <= 2).mean(),\n",
    "                    \"user_generous_rater\": (rated_books[\"rating\"] >= 4).mean(),\n",
    "                    \"user_rating_count_historical\": len(rated_books),\n",
    "                }\n",
    "\n",
    "    return user_rating_patterns\n",
    "\n",
    "\n",
    "# Compute historical patterns\n",
    "user_rating_history = compute_user_rating_history(filtered_interactions)\n",
    "\n",
    "# Add to training data (only rating-based features, NO is_read features)\n",
    "for feature in [\n",
    "    \"user_avg_rating_historical\",\n",
    "    \"user_rating_std_historical\",\n",
    "    \"user_harsh_rater\",\n",
    "    \"user_generous_rater\",\n",
    "    \"user_rating_count_historical\",\n",
    "]:\n",
    "    train_data[feature] = train_data[\"user_id\"].map(\n",
    "        lambda uid: user_rating_history.get(uid, {}).get(feature, 0)\n",
    "    )\n",
    "\n",
    "print(\"✅ Added historical rating features (no target leakage)\")\n",
    "print(\"⚠️  Note: Temporal leakage possible - future improvement needed\")\n",
    "print(\"Users with rating history: {len(user_rating_history)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d80979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Book shelf features\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def count_shelves_in_list(shelves):\n",
    "    c = Counter()\n",
    "    for s in shelves:\n",
    "        c[s[\"name\"]] += int(s[\"count\"])\n",
    "    return c\n",
    "\n",
    "\n",
    "# Global shelf popularity across all books\n",
    "global_shelf_counts = Counter()\n",
    "for shelves in books_df[\"popular_shelves\"]:\n",
    "    global_shelf_counts.update(count_shelves_in_list(shelves))\n",
    "\n",
    "TOP_N = 100  # tune as you like (50/100/200)\n",
    "top_shelves = [name for name, _ in global_shelf_counts.most_common(TOP_N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23250394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_shelf_count(popular_shelves: list):\n",
    "    cnt = 0\n",
    "    for shelf in popular_shelves:\n",
    "        cnt += int(shelf[\"count\"])\n",
    "    return cnt\n",
    "\n",
    "\n",
    "def compute_top_n_shelf_props(popular_shelves: list, top_shelves=top_shelves):\n",
    "    # total count for this book\n",
    "    total = sum(int(s[\"count\"]) for s in popular_shelves)\n",
    "    if total == 0:\n",
    "        return {shelf: 0.0 for shelf in top_shelves}\n",
    "\n",
    "    # build dict for this book\n",
    "    counts = {s[\"name\"]: int(s[\"count\"]) for s in popular_shelves}\n",
    "    return {shelf: counts.get(shelf, 0) / total for shelf in top_shelves}\n",
    "\n",
    "\n",
    "shelves_df = books_df[[\"book_id\", \"popular_shelves\"]].copy()\n",
    "shelves_df[\"total_shelf_count\"] = shelves_df[\"popular_shelves\"].apply(\n",
    "    compute_total_shelf_count\n",
    ")\n",
    "\n",
    "# compute proportions dict\n",
    "shelves_df[\"shelf_props\"] = shelves_df[\"popular_shelves\"].apply(\n",
    "    compute_top_n_shelf_props\n",
    ")\n",
    "\n",
    "# expand dict into separate columns\n",
    "shelf_prop_df = shelves_df[\"shelf_props\"].apply(pd.Series)\n",
    "\n",
    "# merge back\n",
    "shelves_df = pd.concat(\n",
    "    [shelves_df[[\"book_id\", \"total_shelf_count\"]], shelf_prop_df], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7b6da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User shelf features\n",
    "\n",
    "user_shelves_df = filtered_interactions[[\"user_id\", \"book_id\"]].merge(\n",
    "    books_df[[\"book_id\", \"popular_shelves\"]], on=\"book_id\"\n",
    ")\n",
    "\n",
    "\n",
    "def aggregate_user_shelves(shelf_lists):\n",
    "    counter = Counter()\n",
    "    for shelves in shelf_lists:\n",
    "        for s in shelves:\n",
    "            counter[s[\"name\"]] += int(s[\"count\"])\n",
    "    return dict(counter)\n",
    "\n",
    "\n",
    "user_shelf_profiles = user_shelves_df.groupby(\"user_id\")[\"popular_shelves\"].apply(\n",
    "    aggregate_user_shelves\n",
    ")\n",
    "\n",
    "user_shelf_profiles_wide = user_shelf_profiles.unstack(fill_value=0)\n",
    "\n",
    "# restrict/reorder user shelves to the same top_shelves\n",
    "user_shelf_profiles_aligned = user_shelf_profiles_wide.reindex(\n",
    "    columns=top_shelves, fill_value=0\n",
    ")\n",
    "\n",
    "# normalize to proportions\n",
    "user_shelf_profiles_aligned = user_shelf_profiles_aligned.div(\n",
    "    user_shelf_profiles_aligned.sum(axis=1).replace(0, 1), axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d91a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(u, v):\n",
    "    nu, nv = np.linalg.norm(u), np.linalg.norm(v)\n",
    "    if nu == 0 or nv == 0:\n",
    "        return 0.0\n",
    "    return float(np.dot(u, v) / (nu * nv))\n",
    "\n",
    "\n",
    "user_profiles = user_shelf_profiles_aligned.reset_index().rename(\n",
    "    columns={s: s + \"_user\" for s in top_shelves}\n",
    ")\n",
    "book_profiles = shelves_df[[\"book_id\"] + top_shelves].rename(\n",
    "    columns={s: s + \"_book\" for s in top_shelves}\n",
    ")\n",
    "\n",
    "user_profiles = user_profiles.fillna(0.0)\n",
    "book_profiles = book_profiles.fillna(0.0)\n",
    "\n",
    "interactions_with_sim = (\n",
    "    filtered_interactions[[\"user_id\", \"book_id\"]]\n",
    "    .merge(user_profiles, on=\"user_id\", how=\"left\")\n",
    "    .merge(book_profiles, on=\"book_id\", how=\"left\")\n",
    ")\n",
    "\n",
    "u_cols = [s + \"_user\" for s in top_shelves]\n",
    "b_cols = [s + \"_book\" for s in top_shelves]\n",
    "\n",
    "# user and book matrices\n",
    "U = interactions_with_sim[u_cols].to_numpy(dtype=float)\n",
    "B = interactions_with_sim[b_cols].to_numpy(dtype=float)\n",
    "\n",
    "# dot product row-wise\n",
    "dot = np.einsum(\"ij,ij->i\", U, B)\n",
    "\n",
    "# norms\n",
    "U_norm = np.linalg.norm(U, axis=1)\n",
    "B_norm = np.linalg.norm(B, axis=1)\n",
    "\n",
    "# avoid div by 0\n",
    "denom = U_norm * B_norm\n",
    "denom[denom == 0] = 1.0\n",
    "\n",
    "# cosine similarity\n",
    "cos_sim = dot / denom\n",
    "\n",
    "# assign back\n",
    "interactions_with_sim[\"shelf_cosine_similarity\"] = cos_sim\n",
    "interactions_with_sim[u_cols + b_cols] = interactions_with_sim[u_cols + b_cols].fillna(\n",
    "    0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3630d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.merge(interactions_with_sim, on=[\"user_id\", \"book_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeb67d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"shelf_cosine_similarity\"] = train_data[\"shelf_cosine_similarity\"].fillna(\n",
    "    0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c518cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following features are dropped as their representations are already captured\n",
    "features_to_drop_for_ml = [\n",
    "    \"user_id\",\n",
    "    \"book_id\",\n",
    "    \"genres\",\n",
    "    \"date_added\",\n",
    "    \"user_preferred_lang\",\n",
    "    \"book_language\",\n",
    "    \"format\",\n",
    "    \"format_category\",\n",
    "    \"user_preferred_format\",\n",
    "    \"primary_author_id\",\n",
    "    \"author_name\",\n",
    "]\n",
    "train_data = train_data.drop(columns=features_to_drop_for_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa8b13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No null interaction features to fill - we didn't create leaky features\n",
    "# Only need to handle missing values in legitimate features\n",
    "\n",
    "# Fill temporal features\n",
    "datetime_cols = [\"hour_added\", \"day_of_week\", \"month_added\"]\n",
    "train_data[datetime_cols] = train_data[datetime_cols].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209025af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity features (0 = neutral/unknown)\n",
    "similarity_cols = [\n",
    "    \"year_similarity\",\n",
    "    \"length_similarity\",\n",
    "    \"sentiment_similarity\",\n",
    "    \"genre_similarity\",\n",
    "    \"review_text_similarity\",\n",
    "    \"title_similarity\",\n",
    "]\n",
    "for col in similarity_cols:\n",
    "    train_data[col] = train_data[col].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b7d954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill remaining nulls for year and length-based features\n",
    "print(\"=== Filling remaining nulls for year/length features ===\")\n",
    "\n",
    "# Ensure publication_year is numeric\n",
    "train_data[\"publication_year\"] = pd.to_numeric(\n",
    "    train_data[\"publication_year\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Compute robust medians\n",
    "med_year = train_data[\"publication_year\"].median()\n",
    "med_len = train_data[\"book_length\"].median()\n",
    "med_user_year_std = train_data[\"user_year_pref_std\"].median()\n",
    "\n",
    "# Fill NaNs\n",
    "train_data[\"publication_year\"] = train_data[\"publication_year\"].fillna(med_year)\n",
    "train_data[\"user_year_pref_mean\"] = train_data[\"user_year_pref_mean\"].fillna(med_year)\n",
    "train_data[\"user_year_pref_std\"] = train_data[\"user_year_pref_std\"].fillna(\n",
    "    med_user_year_std if pd.notnull(med_user_year_std) else 0.0\n",
    ")\n",
    "train_data[\"book_length\"] = train_data[\"book_length\"].fillna(med_len)\n",
    "train_data[\"user_avg_book_length\"] = train_data[\"user_avg_book_length\"].fillna(med_len)\n",
    "\n",
    "# Recompute dependent features\n",
    "if \"year_diff\" in train_data.columns:\n",
    "    train_data[\"year_diff\"] = (\n",
    "        train_data[\"publication_year\"] - train_data[\"user_year_pref_mean\"]\n",
    "    )\n",
    "    train_data[\"year_similarity\"] = 1 - abs(train_data[\"year_diff\"]) / (\n",
    "        train_data[\"user_year_pref_std\"] + 1\n",
    "    )\n",
    "if \"length_similarity\" in train_data.columns:\n",
    "    train_data[\"length_similarity\"] = 1 / (\n",
    "        1 + abs(train_data[\"book_length\"] - train_data[\"user_avg_book_length\"]) / 100\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca2002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill author-related features\n",
    "for col, val in {\n",
    "    \"author_avg_rating\": 0.0,\n",
    "    \"author_ratings_count\": 0.0,\n",
    "    \"author_text_reviews_count\": 0.0,\n",
    "    \"author_popularity_log\": 0.0,\n",
    "    \"user_author_interaction_count\": 0.0,\n",
    "    \"user_author_interaction_ratio\": 0.0,\n",
    "    \"has_read_author_before\": 0,\n",
    "}.items():\n",
    "    if col in train_data.columns:\n",
    "        train_data[col] = train_data[col].fillna(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7070f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Fill user profile features (removed completion rate features to avoid leakage)\n",
    "user_profile_cols = [\"rating_mean\", \"rating_std\", \"rating_count\", \"book_id_nunique\"]\n",
    "\n",
    "for col in user_profile_cols:\n",
    "    train_data[col].fillna(train_data[col].median(), inplace=True)\n",
    "\n",
    "# Recalculate dependent features\n",
    "train_data[\"rating_diff\"] = abs(\n",
    "    train_data[\"rating_mean\"] - train_data[\"rating_mean_book\"]\n",
    ")\n",
    "train_data[\"rating_similarity\"] = 1 / (1 + train_data[\"rating_diff\"])\n",
    "\n",
    "# Fix year features\n",
    "train_data[\"year_diff\"].fillna(train_data[\"year_diff\"].median(), inplace=True)\n",
    "\n",
    "# Verify no nulls remain\n",
    "print(\"Remaining nulls:\")\n",
    "print(train_data.isnull().sum()[train_data.isnull().sum() > 0])\n",
    "\n",
    "# Final check\n",
    "print(f\"Total nulls remaining: {train_data.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c063737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After dropping raw columns, verify everything is numeric\n",
    "print(\"Final feature types:\")\n",
    "print(train_data.dtypes)\n",
    "\n",
    "# Should only see int64, float64, bool - no object/dict types\n",
    "non_numeric = train_data.select_dtypes(exclude=[\"number\", \"bool\"]).columns\n",
    "if len(non_numeric) > 0:\n",
    "    print(f\"Warning: Non-numeric columns remaining: {non_numeric.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d449e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceac4166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update user mapping for completion prediction\n",
    "\n",
    "user_book_mapping = all_samples[[\"user_id\", \"book_id\"]].copy()\n",
    "user_book_mapping[\"sample_index\"] = range(len(user_book_mapping))\n",
    "user_book_mapping_filename = \"completion_user_book_mapping.csv\"\n",
    "user_book_mapping_path = f\"../data/{user_book_mapping_filename}\"\n",
    "user_book_mapping.to_csv(user_book_mapping_path, index=False)\n",
    "\n",
    "artifact = wandb.Artifact(\n",
    "    name=user_book_mapping_filename,\n",
    "    type=\"supplemental_data\",\n",
    "    description=\"Mapping of (user_id, book_id) to sample index\",\n",
    ")\n",
    "artifact.add_file(user_book_mapping_path)\n",
    "run.log_artifact(artifact)\n",
    "\n",
    "print(\"✅ User-book mapping saved for completion prediction evaluation!\")\n",
    "print(f\"Mapping shape: {user_book_mapping.shape}\")\n",
    "print(\"Task: Predicting book completion (is_read)\")\n",
    "print(\"Clean dataset: No leaky features, no synthetic negatives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b73e0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final completion prediction dataset\n",
    "train_data_filename = \"completion_prediction.csv\"\n",
    "train_data_path = f\"../data/{train_data_filename}\"\n",
    "train_data.to_csv(\"../data/completion_prediction.csv\", index=False)\n",
    "\n",
    "artifact = wandb.Artifact(\n",
    "    name=train_data_filename,\n",
    "    type=\"training_data\",\n",
    "    description=\"Book completion prediction training data\",\n",
    ")\n",
    "artifact.add_file(train_data_path)\n",
    "run.log_artifact(artifact)\n",
    "\n",
    "print(\"✅ Saved completion prediction dataset to ../data/completion_prediction.csv\")\n",
    "\n",
    "# Summary of what we built\n",
    "print(\"\\n=== FINAL DATASET SUMMARY ===\")\n",
    "print(f\"Dataset shape: {train_data.shape}\")\n",
    "print(\"Target: is_read (completion prediction)\")\n",
    "print(f\"Features: {train_data.shape[1] - 1}\")\n",
    "print(f\"Completion rate: {train_data['is_read'].mean():.3f}\")\n",
    "print(\"Clean dataset: No target leakage, no synthetic negatives\")\n",
    "print(\"Ready for model training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2918ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb15f4ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
