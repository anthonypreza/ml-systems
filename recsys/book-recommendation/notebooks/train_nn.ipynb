{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23195c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly.graph_objects as go\n",
    "import wandb\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ff9aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb run\n",
    "# Expanded parameter dictionary for production-style training visibility\n",
    "param = {\n",
    "    \"hidden_layers\": (256, 128),\n",
    "    \"blocks_per_layer\": 2,\n",
    "    \"dropout\": 0.2,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"batch_size\": 512,\n",
    "    \"epochs\": 100,\n",
    "    \"early_stopping_rounds\": 10,\n",
    "}\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"book-recommendation\",\n",
    "    group=\"dev\",\n",
    "    job_type=\"train\",\n",
    "    save_code=True,\n",
    "    config=param,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3eebd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact = run.use_artifact(\"book-recommendation/completion_prediction.csv:latest\")\n",
    "artifact_path = artifact.file()\n",
    "\n",
    "user_book_mapping_artifact = run.use_artifact(\n",
    "    \"book-recommendation/completion_user_book_mapping.csv:latest\"\n",
    ")\n",
    "user_book_mapping_path = user_book_mapping_artifact.file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85da447",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(artifact_path)\n",
    "user_book_mapping = pd.read_csv(user_book_mapping_path)\n",
    "\n",
    "print(\"Completion prediction model\")\n",
    "print(\"Task: Will user complete books they interact with?\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"Target: is_read (completion)\")\n",
    "print(f\"Features: {df.shape[1] - 1}\")\n",
    "\n",
    "# Check target distribution\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(df[\"is_read\"].value_counts())\n",
    "completion_rate = df[\"is_read\"].mean()\n",
    "print(f\"Overall completion rate: {completion_rate:.3f}\")\n",
    "\n",
    "df = df.merge(user_book_mapping, left_index=True, right_on=\"sample_index\")\n",
    "\n",
    "user_enc = LabelEncoder().fit(df[\"user_id\"])\n",
    "book_enc = LabelEncoder().fit(df[\"book_id\"])\n",
    "\n",
    "df[\"user_id_idx\"] = user_enc.transform(df[\"user_id\"])\n",
    "df[\"book_id_idx\"] = book_enc.transform(df[\"book_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3532a076",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d256952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "TARGET = \"is_read\"\n",
    "ID_USER = \"user_id_idx\"\n",
    "ID_BOOK = \"book_id_idx\"\n",
    "\n",
    "dense_cols = [\n",
    "    c for c in df.columns if c not in [TARGET, \"user_id\", \"book_id\", ID_USER, ID_BOOK]\n",
    "]\n",
    "\n",
    "skewed = [\n",
    "    \"rating_count\",\n",
    "    \"rating_count_book\",\n",
    "    \"book_id_nunique\",\n",
    "    \"author_ratings_count\",\n",
    "    \"author_text_reviews_count\",\n",
    "    \"user_id_nunique\",\n",
    "    \"user_rating_count_historical\",\n",
    "    \"user_author_interaction_count\",\n",
    "]\n",
    "for col in skewed:\n",
    "    if col in dense_cols:  # guard\n",
    "        df[col] = np.log1p(df[col])\n",
    "\n",
    "test_size = 0.2\n",
    "n = len(df)\n",
    "split_idx = int(n * (1 - test_size))\n",
    "train_df = df.iloc[:split_idx].copy()\n",
    "test_df = df.iloc[split_idx:].copy()\n",
    "\n",
    "X_train = train_df[dense_cols]\n",
    "y_train = train_df[TARGET]\n",
    "X_test = test_df[dense_cols]\n",
    "y_test = test_df[TARGET]\n",
    "\n",
    "X_tr, X_val, y_tr, y_val, idx_tr, idx_val = train_test_split(\n",
    "    X_train, y_train, train_df.index, test_size=0.2, stratify=y_train, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize features for NN\n",
    "scaler = StandardScaler().fit(X_tr)\n",
    "X_tr = scaler.transform(X_tr)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Align ID tensors\n",
    "u_tr = train_df.loc[idx_tr, ID_USER].to_numpy()\n",
    "b_tr = train_df.loc[idx_tr, ID_BOOK].to_numpy()\n",
    "u_val = train_df.loc[idx_val, ID_USER].to_numpy()\n",
    "b_val = train_df.loc[idx_val, ID_BOOK].to_numpy()\n",
    "u_test = test_df[ID_USER].to_numpy()\n",
    "b_test = test_df[ID_BOOK].to_numpy()\n",
    "\n",
    "\n",
    "# Torch dataset/dataloader\n",
    "class InteractionsDataset(Dataset):\n",
    "    def __init__(self, X_dense, y, user_ids, book_ids):\n",
    "        self.X = X_dense.astype(np.float32)\n",
    "        self.y = y.to_numpy().astype(np.float32)\n",
    "        self.u = user_ids.astype(np.int64)\n",
    "        self.b = book_ids.astype(np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.u[i], self.b[i], self.y[i]\n",
    "\n",
    "\n",
    "batch_size = int(param.get(\"batch_size\", 512))\n",
    "train_ds = InteractionsDataset(X_tr, y_tr, u_tr, b_tr)\n",
    "val_ds = InteractionsDataset(X_val, y_val, u_val, b_val)\n",
    "\n",
    "pin_mem = torch.cuda.is_available()\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=pin_mem\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=pin_mem\n",
    ")\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.norm(x + self.block(x))\n",
    "\n",
    "\n",
    "class UserBookRecommenderModel(nn.Module):\n",
    "    \"\"\"Neural network with residual MLP blocks for pointwise book recommendations.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_users: int,\n",
    "        num_books: int,\n",
    "        input_dim: int,\n",
    "        user_cols: list[int],\n",
    "        book_cols: list[int],\n",
    "        blocks_per_layer: int = 2,\n",
    "        hidden_layers=(256, 128),\n",
    "        emb_dim=32,\n",
    "        dropout=0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, emb_dim)\n",
    "        self.book_emb = nn.Embedding(num_books, emb_dim)\n",
    "\n",
    "        self.user_idx = user_cols\n",
    "        self.book_idx = book_cols\n",
    "\n",
    "        # crossed features = same length as user/book shelf vectors\n",
    "        cross_dim = len(self.user_idx)\n",
    "        total_in = input_dim + cross_dim + 2 * emb_dim\n",
    "\n",
    "        layers = []\n",
    "        prev = total_in\n",
    "\n",
    "        for h in hidden_layers:\n",
    "            # projection into new hidden dim\n",
    "            layers += [nn.Linear(prev, h), nn.BatchNorm1d(h), nn.GELU()]\n",
    "            # add several residual blocks at this dimension\n",
    "            for _ in range(blocks_per_layer):\n",
    "                layers.append(ResidualBlock(h, dropout=dropout))\n",
    "            prev = h\n",
    "\n",
    "        # output layer\n",
    "        layers.append(nn.Linear(prev, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, user_ids, book_ids):\n",
    "        u_shelves = x[:, self.user_idx]\n",
    "        b_shelves = x[:, self.book_idx]\n",
    "        cross = u_shelves * b_shelves\n",
    "\n",
    "        # look up embeddings\n",
    "        u = self.user_emb(user_ids)\n",
    "        b = self.book_emb(book_ids)\n",
    "\n",
    "        x_in = torch.cat([x, cross, u, b], dim=1)\n",
    "        return self.net(x_in).squeeze(-1)\n",
    "\n",
    "\n",
    "num_users = int(df[ID_USER].max() + 1)\n",
    "num_books = int(df[ID_BOOK].max() + 1)\n",
    "\n",
    "dense_index = {c: i for i, c in enumerate(dense_cols)}\n",
    "u_cols = [dense_index[c] for c in dense_cols if c.endswith(\"_user\")]\n",
    "b_cols = [\n",
    "    dense_index[c]\n",
    "    for c in dense_cols\n",
    "    if c.endswith(\"_book\")\n",
    "    and c not in (\"rating_mean_book\", \"rating_std_book\", \"rating_count_book\")\n",
    "]\n",
    "\n",
    "device = \"cpu\" if not torch.cuda.is_available() else \"gpu\"\n",
    "model = UserBookRecommenderModel(\n",
    "    num_users=num_users,\n",
    "    num_books=num_books,\n",
    "    input_dim=len(dense_cols),\n",
    "    user_cols=u_cols,\n",
    "    book_cols=b_cols,\n",
    "    hidden_layers=param.get(\"hidden_layers\", (256, 128)),\n",
    "    blocks_per_layer=param.get(\"blocks_per_layer\", 2),\n",
    "    emb_dim=int(param.get(\"emb_dim\", 32)),\n",
    "    dropout=float(param.get(\"dropout\", 0.2)),\n",
    ").to(device)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "wandb.watch(model, log=\"all\", log_freq=50)\n",
    "\n",
    "# Class imbalance handling: pos_weight = neg/pos on training split\n",
    "pos = float((y_tr == 1).sum())\n",
    "neg = float((y_tr == 0).sum())\n",
    "pos_weight = torch.tensor([neg / max(pos, 1.0)], device=device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "# Optimization\n",
    "lr = float(param.get(\"lr\", 1e-3))\n",
    "weight_decay = float(param.get(\"weight_decay\", 1e-4))\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=float(param.get(\"lr\", 1e-3)),\n",
    "    weight_decay=float(param.get(\"weight_decay\", 1e-5)),\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=float(param.get(\"lr\", 1e-3)),\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    epochs=int(param.get(\"epochs\", 30)),\n",
    ")\n",
    "\n",
    "epochs = int(param.get(\"epochs\", 30))\n",
    "patience = int(param.get(\"early_stopping_rounds\", 5))\n",
    "best_val = -float(\"inf\")\n",
    "best_state = None\n",
    "no_improve = 0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for xb, ub, bb, yb in train_loader:\n",
    "        xb = torch.as_tensor(xb, device=device)\n",
    "        ub = torch.as_tensor(ub, device=device)\n",
    "        bb = torch.as_tensor(bb, device=device)\n",
    "        yb = torch.as_tensor(yb, device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb, ub.long(), bb.long())\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "    train_loss /= len(train_ds)\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_probs = []\n",
    "    val_targets = []\n",
    "    with torch.no_grad():\n",
    "        for xb, ub, bb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            ub = ub.to(device)\n",
    "            bb = bb.to(device)\n",
    "            logits = model(xb, ub.long(), bb.long())\n",
    "            val_probs.append(torch.sigmoid(logits).cpu())\n",
    "            val_targets.append(yb)\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import average_precision_score\n",
    "\n",
    "    val_probs = np.concatenate([v.numpy() for v in val_probs])\n",
    "    val_targets = np.concatenate([t.numpy() for t in val_targets])\n",
    "    val_ap = float(average_precision_score(val_targets, val_probs))\n",
    "\n",
    "    run.log({\"nn/train_loss\": train_loss, \"nn/val_aucpr\": val_ap, \"epoch\": epoch})\n",
    "\n",
    "    if val_ap > best_val + 1e-6:\n",
    "        best_val = val_ap\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch} (best val AP {best_val:.4f})\")\n",
    "            break\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "\n",
    "# Inference on test\n",
    "test_ds = InteractionsDataset(X_test, y_test, u_test, b_test)\n",
    "test_loader = DataLoader(\n",
    "    test_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "probs = []\n",
    "with torch.no_grad():\n",
    "    for xb, ub, bb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        ub = ub.to(device)\n",
    "        bb = bb.to(device)\n",
    "        logits = model(xb, ub.long(), bb.long())\n",
    "        probs.append(torch.sigmoid(logits).cpu())\n",
    "y_proba = torch.cat(probs).numpy().ravel()\n",
    "preds = (y_proba >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d535d6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic classification metrics\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    average_precision_score,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "print(\"Basic classification metrics\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, preds):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, preds):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, preds):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, preds):.4f}\")\n",
    "\n",
    "# Get prediction probabilities for ranking\n",
    "# y_proba was computed by the NN above\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n",
    "print(f\"PR-AUC: {average_precision_score(y_test, y_proba):.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, preds))\n",
    "\n",
    "# Confusion matrix with plotly\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Create confusion matrix heatmap\n",
    "fig_cm = go.Figure(\n",
    "    data=go.Heatmap(\n",
    "        z=cm,\n",
    "        x=[\"Predicted 0\", \"Predicted 1\"],\n",
    "        y=[\"Actual 0\", \"Actual 1\"],\n",
    "        colorscale=\"Blues\",\n",
    "        text=cm,\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 20},\n",
    "        showscale=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_cm.update_layout(\n",
    "    title=\"Confusion Matrix\",\n",
    "    xaxis_title=\"Predicted Label\",\n",
    "    yaxis_title=\"Actual Label\",\n",
    "    width=500,\n",
    "    height=400,\n",
    ")\n",
    "\n",
    "fig_cm.show()\n",
    "run.log({\"confusion_matrix\": fig_cm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bg6qpey6jp7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC and precision-recall curves\n",
    "\n",
    "# Calculate curves\n",
    "fpr, tpr, roc_thresholds = roc_curve(y_test, y_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "precision, recall, pr_thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = average_precision_score(y_test, y_proba)\n",
    "\n",
    "# Create subplots for ROC and PR curves\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=[\"ROC Curve\", \"Precision-Recall Curve\"],\n",
    "    specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}]],\n",
    ")\n",
    "\n",
    "# ROC curve\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=fpr,\n",
    "        y=tpr,\n",
    "        mode=\"lines\",\n",
    "        name=f\"ROC Curve (AUC = {roc_auc:.3f})\",\n",
    "        line=dict(color=\"blue\", width=2),\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "# Diagonal line for ROC\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[0, 1],\n",
    "        y=[0, 1],\n",
    "        mode=\"lines\",\n",
    "        name=\"Random Classifier\",\n",
    "        line=dict(color=\"red\", width=1, dash=\"dash\"),\n",
    "        showlegend=False,\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "# Precision-recall curve\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=recall,\n",
    "        y=precision,\n",
    "        mode=\"lines\",\n",
    "        name=f\"PR Curve (AUC = {pr_auc:.3f})\",\n",
    "        line=dict(color=\"green\", width=2),\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "# Baseline for PR curve\n",
    "baseline = y_test.sum() / len(y_test)  # Positive rate\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[0, 1],\n",
    "        y=[baseline, baseline],\n",
    "        mode=\"lines\",\n",
    "        name=f\"Random Baseline ({baseline:.3f})\",\n",
    "        line=dict(color=\"red\", width=1, dash=\"dash\"),\n",
    "        showlegend=False,\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_xaxes(title_text=\"False Positive Rate\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"True Positive Rate\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Recall\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Precision\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Model Performance Curves\", width=1000, height=400, showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "run.log({\"roc_pr_curves\": fig})\n",
    "\n",
    "print(\"Model performance summary:\")\n",
    "print(\n",
    "    f\"ROC-AUC: {roc_auc:.4f} ({'Excellent' if roc_auc > 0.9 else 'Good' if roc_auc > 0.8 else 'Fair' if roc_auc > 0.7 else 'Poor'})\"\n",
    ")\n",
    "print(\n",
    "    f\"PR-AUC: {pr_auc:.4f} ({'Excellent' if pr_auc > 0.8 else 'Good' if pr_auc > 0.6 else 'Fair' if pr_auc > 0.4 else 'Poor'})\"\n",
    ")\n",
    "print(f\"Baseline (random): {baseline:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7amzlkemjla",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction distribution analysis\n",
    "\n",
    "# Create prediction distribution plots\n",
    "fig_dist = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=[\n",
    "        \"Prediction Probability Distribution\",\n",
    "        \"Prediction Probabilities by Class\",\n",
    "    ],\n",
    "    specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}]],\n",
    ")\n",
    "\n",
    "# Overall distribution\n",
    "fig_dist.add_trace(\n",
    "    go.Histogram(\n",
    "        x=y_proba,\n",
    "        nbinsx=50,\n",
    "        name=\"All Predictions\",\n",
    "        opacity=0.7,\n",
    "        marker_color=\"lightblue\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "# Distribution by class\n",
    "fig_dist.add_trace(\n",
    "    go.Histogram(\n",
    "        x=y_proba[y_test == 0],\n",
    "        nbinsx=30,\n",
    "        name=\"Negative Class (0)\",\n",
    "        opacity=0.7,\n",
    "        marker_color=\"red\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "fig_dist.add_trace(\n",
    "    go.Histogram(\n",
    "        x=y_proba[y_test == 1],\n",
    "        nbinsx=30,\n",
    "        name=\"Positive Class (1)\",\n",
    "        opacity=0.7,\n",
    "        marker_color=\"green\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "fig_dist.update_xaxes(title_text=\"Prediction Probability\", row=1, col=1)\n",
    "fig_dist.update_yaxes(title_text=\"Count\", row=1, col=1)\n",
    "fig_dist.update_xaxes(title_text=\"Prediction Probability\", row=1, col=2)\n",
    "fig_dist.update_yaxes(title_text=\"Count\", row=1, col=2)\n",
    "\n",
    "fig_dist.update_layout(\n",
    "    title=\"Prediction Probability Distributions\",\n",
    "    width=1000,\n",
    "    height=400,\n",
    "    showlegend=True,\n",
    "    barmode=\"overlay\",  # For overlapping histograms in the second subplot\n",
    ")\n",
    "\n",
    "fig_dist.show()\n",
    "run.log({\"prediction_distributions\": fig_dist})\n",
    "\n",
    "# Calculate separation metrics\n",
    "mean_pos = y_proba[y_test == 1].mean()\n",
    "mean_neg = y_proba[y_test == 0].mean()\n",
    "separation = abs(mean_pos - mean_neg)\n",
    "\n",
    "print(\"Prediction analysis:\")\n",
    "print(f\"Mean probability for positive class: {mean_pos:.4f}\")\n",
    "print(f\"Mean probability for negative class: {mean_neg:.4f}\")\n",
    "print(\n",
    "    f\"Class separation: {separation:.4f} ({'Good' if separation > 0.3 else 'Moderate' if separation > 0.1 else 'Poor'})\"\n",
    ")\n",
    "print(\"Optimal threshold (balanced): 0.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59mybjjffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load completion prediction mappings\n",
    "user_book_mapping = pd.read_csv(\"../data/completion_user_book_mapping.csv\")\n",
    "\n",
    "print(\"=== Completion Prediction Evaluation ===\")\n",
    "print(\"Task: Predicting if users complete books they interact with\")\n",
    "print(f\"Mapping shape: {user_book_mapping.shape}\")\n",
    "\n",
    "# Create test mapping for completion prediction\n",
    "test_indices = test_df.index\n",
    "test_mapping = user_book_mapping.iloc[test_indices].copy()\n",
    "test_mapping[\"prediction_score\"] = y_proba\n",
    "test_mapping[\"true_completion\"] = y_test.values  # is_read target\n",
    "\n",
    "print(f\"Test mapping created: {len(test_mapping)} samples\")\n",
    "print(f\"Users in test: {test_mapping['user_id'].nunique()}\")\n",
    "print(f\"Books completed in test: {test_mapping['true_completion'].sum()}\")\n",
    "\n",
    "test_mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfqmty6xr0m",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking evaluation functions\n",
    "\n",
    "\n",
    "def compute_precision_at_k(user_data, k):\n",
    "    \"\"\"Compute Precision@k for a single user\"\"\"\n",
    "    top_k = user_data.head(k)\n",
    "    return top_k[\"true_completion\"].sum() / k\n",
    "\n",
    "\n",
    "def compute_recall_at_k(user_data, k):\n",
    "    \"\"\"Compute Recall@k for a single user\"\"\"\n",
    "    top_k = user_data.head(k)\n",
    "    total_relevant = user_data[\"true_completion\"].sum()\n",
    "    if total_relevant == 0:\n",
    "        return 0.0\n",
    "    return top_k[\"true_completion\"].sum() / total_relevant\n",
    "\n",
    "\n",
    "def compute_ap_at_k(user_data, k):\n",
    "    \"\"\"Compute Average Precision@k for a single user\"\"\"\n",
    "    top_k = user_data.head(k)\n",
    "    y_true = top_k[\"true_completion\"].values\n",
    "    y_scores = top_k[\"prediction_score\"].values\n",
    "\n",
    "    if y_true.sum() == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return average_precision_score(y_true, y_scores)\n",
    "\n",
    "\n",
    "def compute_ndcg_at_k(user_data, k):\n",
    "    \"\"\"Compute NDCG@k for a single user\"\"\"\n",
    "    from sklearn.metrics import ndcg_score\n",
    "\n",
    "    top_k = user_data.head(k)\n",
    "    y_true = top_k[\"true_completion\"].values.reshape(1, -1)\n",
    "    y_scores = top_k[\"prediction_score\"].values.reshape(1, -1)\n",
    "\n",
    "    if len(y_true[0]) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return ndcg_score(y_true, y_scores, k=k)\n",
    "\n",
    "\n",
    "def evaluate_ranking_metrics(test_mapping, k_values=[5, 10, 20, 50]):\n",
    "    \"\"\"Compute comprehensive ranking metrics\"\"\"\n",
    "\n",
    "    metrics = {f\"precision@{k}\": [] for k in k_values}\n",
    "    metrics.update({f\"recall@{k}\": [] for k in k_values})\n",
    "    metrics.update({f\"map@{k}\": [] for k in k_values})\n",
    "    metrics.update({f\"ndcg@{k}\": [] for k in k_values})\n",
    "\n",
    "    users_evaluated = 0\n",
    "\n",
    "    for user_id in test_mapping[\"user_id\"].unique():\n",
    "        user_data = test_mapping[test_mapping[\"user_id\"] == user_id]\n",
    "\n",
    "        # Skip users with no positive items in test\n",
    "        if user_data[\"true_completion\"].sum() == 0:\n",
    "            continue\n",
    "\n",
    "        # Sort by prediction score (highest first)\n",
    "        user_data = user_data.sort_values(\"prediction_score\", ascending=False)\n",
    "        users_evaluated += 1\n",
    "\n",
    "        # Compute metrics for each k\n",
    "        for k in k_values:\n",
    "            if len(user_data) >= k:  # Only compute if user has enough items\n",
    "                metrics[f\"precision@{k}\"].append(compute_precision_at_k(user_data, k))\n",
    "                metrics[f\"recall@{k}\"].append(compute_recall_at_k(user_data, k))\n",
    "                metrics[f\"map@{k}\"].append(compute_ap_at_k(user_data, k))\n",
    "                metrics[f\"ndcg@{k}\"].append(compute_ndcg_at_k(user_data, k))\n",
    "\n",
    "    # Average across users\n",
    "    results = {}\n",
    "    for metric_name, values in metrics.items():\n",
    "        if values:  # Only compute average if we have values\n",
    "            results[metric_name] = np.mean(values)\n",
    "        else:\n",
    "            results[metric_name] = 0.0\n",
    "\n",
    "    results[\"users_evaluated\"] = users_evaluated\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Computing ranking metrics\")\n",
    "ranking_results = evaluate_ranking_metrics(test_mapping, k_values=[5, 10, 20, 50])\n",
    "\n",
    "print(f\"Users evaluated: {ranking_results['users_evaluated']}\")\n",
    "print(\"\\nRanking metrics:\")\n",
    "for metric, value in ranking_results.items():\n",
    "    if metric != \"users_evaluated\":\n",
    "        print(f\"{metric.upper()}: {value:.4f}\")\n",
    "# Log ranking metrics to Weights & Biases\n",
    "_rank_metrics = {\"ranking/users_evaluated\": ranking_results.get(\"users_evaluated\", 0)}\n",
    "for _k in [5, 10, 20, 50]:\n",
    "    _rank_metrics.update(\n",
    "        {\n",
    "            f\"ranking/precision@{_k}\": float(\n",
    "                ranking_results.get(f\"precision@{_k}\", 0.0)\n",
    "            ),\n",
    "            f\"ranking/recall@{_k}\": float(ranking_results.get(f\"recall@{_k}\", 0.0)),\n",
    "            f\"ranking/map@{_k}\": float(ranking_results.get(f\"map@{_k}\", 0.0)),\n",
    "            f\"ranking/ndcg@{_k}\": float(ranking_results.get(f\"ndcg@{_k}\", 0.0)),\n",
    "        }\n",
    "    )\n",
    "run.log(_rank_metrics)\n",
    "\n",
    "# Also log a compact table\n",
    "rank_table = wandb.Table(columns=[\"k\", \"precision\", \"recall\", \"map\", \"ndcg\"])\n",
    "for _k in [5, 10, 20, 50]:\n",
    "    rank_table.add_data(\n",
    "        _k,\n",
    "        float(ranking_results.get(f\"precision@{_k}\", 0.0)),\n",
    "        float(ranking_results.get(f\"recall@{_k}\", 0.0)),\n",
    "        float(ranking_results.get(f\"map@{_k}\", 0.0)),\n",
    "        float(ranking_results.get(f\"ndcg@{_k}\", 0.0)),\n",
    "    )\n",
    "run.log({\"ranking/metrics_table\": rank_table})\n",
    "\n",
    "# Promote commonly tracked ones to summary\n",
    "for _key in [\"precision@20\", \"recall@20\", \"map@20\", \"ndcg@20\"]:\n",
    "    run.summary[f\"ranking/{_key}\"] = float(ranking_results.get(_key, 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0r24w9sx2ese",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance report\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BOOK RECOMMENDATION SYSTEM - PERFORMANCE REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\"\"\n",
    "DATASET OVERVIEW:\n",
    "   • Total samples: {len(df):,}\n",
    "   • Training samples: {len(X_train):,}\n",
    "   • Test samples: {len(X_test):,}\n",
    "   • Features: {X_train.shape[1]}\n",
    "   • Unique users in test: {test_mapping[\"user_id\"].nunique():,}\n",
    "   • Users evaluated for ranking: {ranking_results[\"users_evaluated\"]:,}\n",
    "\n",
    "CLASSIFICATION PERFORMANCE:\n",
    "   • Accuracy: {accuracy_score(y_test, preds):.4f}\n",
    "   • Precision: {precision_score(y_test, preds):.4f}\n",
    "   • Recall: {recall_score(y_test, preds):.4f}\n",
    "   • F1-Score: {f1_score(y_test, preds):.4f}\n",
    "   • ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\n",
    "\n",
    "RANKING PERFORMANCE:\"\"\")\n",
    "\n",
    "# Format ranking metrics nicely\n",
    "metrics_by_k = {}\n",
    "for k in [5, 10, 20, 50]:\n",
    "    metrics_by_k[k] = {\n",
    "        \"Precision\": ranking_results[f\"precision@{k}\"],\n",
    "        \"Recall\": ranking_results[f\"recall@{k}\"],\n",
    "        \"mAP\": ranking_results[f\"map@{k}\"],\n",
    "        \"NDCG\": ranking_results[f\"ndcg@{k}\"],\n",
    "    }\n",
    "\n",
    "print(\"   ┌─────────┬──────────┬─────────┬─────────┬─────────┐\")\n",
    "print(\"   │    k    │ Precision│  Recall │   mAP   │  NDCG   │\")\n",
    "print(\"   ├─────────┼──────────┼─────────┼─────────┼─────────┤\")\n",
    "for k in [5, 10, 20, 50]:\n",
    "    metrics = metrics_by_k[k]\n",
    "    print(\n",
    "        f\"   │   @{k:2d}   │  {metrics['Precision']:.4f}  │ {metrics['Recall']:.4f}  │ {metrics['mAP']:.4f}  │ {metrics['NDCG']:.4f}  │\"\n",
    "    )\n",
    "print(\"   └─────────┴──────────┴─────────┴─────────┴─────────┘\")\n",
    "\n",
    "\n",
    "print(f\"\"\"\n",
    "RECOMMENDATION QUALITY INSIGHTS:\n",
    "   • mAP@20 of {ranking_results[\"map@20\"]:.4f} indicates {\"excellent\" if ranking_results[\"map@20\"] > 0.3 else \"good\" if ranking_results[\"map@20\"] > 0.1 else \"moderate\"} ranking quality\n",
    "   • Precision@20 of {ranking_results[\"precision@20\"]:.4f} means {ranking_results[\"precision@20\"] * 100:.1f}% of top-20 recommendations are relevant\n",
    "   • Model successfully learns from {X_train.shape[1]} engineered features\n",
    "\n",
    "BUSINESS IMPACT:\n",
    "   • For every 20 books recommended, ~{ranking_results[\"precision@20\"] * 20:.0f} will be relevant to the user\n",
    "   • {ranking_results[\"recall@20\"] * 100:.1f}% of relevant books are captured in top-20 recommendations\n",
    "   • Strong classification performance (AUC: {roc_auc_score(y_test, y_proba):.3f}) enables confident ranking\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save report to file\n",
    "report_text = f\"\"\"Book Recommendation System Performance Report\n",
    "Generated: {pd.Timestamp.now()}\n",
    "\n",
    "Dataset Overview:\n",
    "- Total samples: {len(df):,}\n",
    "- Training samples: {len(X_train):,}\n",
    "- Test samples: {len(X_test):,}\n",
    "- Features: {X_train.shape[1]}\n",
    "- Users evaluated: {ranking_results[\"users_evaluated\"]:,}\n",
    "\n",
    "Classification Metrics:\n",
    "- Accuracy: {accuracy_score(y_test, preds):.4f}\n",
    "- Precision: {precision_score(y_test, preds):.4f}\n",
    "- Recall: {recall_score(y_test, preds):.4f}\n",
    "- F1-Score: {f1_score(y_test, preds):.4f}\n",
    "- ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\n",
    "\n",
    "Ranking Metrics:\n",
    "\"\"\"\n",
    "\n",
    "for k in [5, 10, 20, 50]:\n",
    "    report_text += f\"@{k}: Precision={ranking_results[f'precision@{k}']:.4f}, Recall={ranking_results[f'recall@{k}']:.4f}, mAP={ranking_results[f'map@{k}']:.4f}, NDCG={ranking_results[f'ndcg@{k}']:.4f}\\n\"\n",
    "\n",
    "# Save to file\n",
    "with open(\"../data/model_performance_report_nn.txt\", \"w\") as f:\n",
    "    f.write(report_text)\n",
    "\n",
    "print(\"Detailed report saved to: ../data/model_performance_report_nn.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fgqd81r4ugu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and artifacts\n",
    "import os\n",
    "\n",
    "print(\"Saving model and artifacts\")\n",
    "\n",
    "# Save the trained PyTorch model (state dict)\n",
    "model_path = \"../models/nn_recommender_model.pt\"\n",
    "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Save test results for further analysis\n",
    "test_results_path = \"../data/test_results_nn.csv\"\n",
    "test_mapping.to_csv(test_results_path, index=False)\n",
    "print(f\"Test results saved to: {test_results_path}\")\n",
    "\n",
    "# Save ranking metrics summary\n",
    "ranking_summary = pd.DataFrame([ranking_results]).T\n",
    "ranking_summary.columns = [\"value\"]\n",
    "ranking_summary_path = \"../data/ranking_metrics_summary_nn.csv\"\n",
    "ranking_summary.to_csv(ranking_summary_path)\n",
    "print(f\"Ranking metrics saved to: {ranking_summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98395a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759a2eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
