{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23195c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import plotly.graph_objects as go\n",
    "import wandb\n",
    "import numpy as np\n",
    "from wandb.integration.xgboost import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ff9aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb run\n",
    "# Expanded parameter dictionary for production-style training visibility\n",
    "param = {\n",
    "    # Objective and evaluation\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": [\"aucpr\", \"logloss\"],\n",
    "    # Capacity and regularization\n",
    "    \"n_estimators\": 2000,  # training budget (used with early stopping)\n",
    "    \"eta\": 0.03,  # alias: learning_rate\n",
    "    \"max_depth\": 6,\n",
    "    \"min_child_weight\": 3,\n",
    "    \"gamma\": 0.0,  # min loss reduction to split\n",
    "    \"reg_lambda\": 1.0,  # L2 regularization\n",
    "    \"reg_alpha\": 0.0,  # L1 regularization\n",
    "    # Sampling for generalization\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"colsample_bylevel\": 1.0,\n",
    "    \"colsample_bynode\": 1.0,\n",
    "    # Tree construction\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"tree_method\": \"hist\",  # use \"gpu_hist\" if GPU is available\n",
    "    \"grow_policy\": \"depthwise\",  # consider \"lossguide\" for very large data\n",
    "    \"max_bin\": 256,\n",
    "    # Class imbalance and constraints\n",
    "    \"scale_pos_weight\": 1.0,  # set to (neg/pos) if imbalanced\n",
    "    \"monotone_constraints\": {},  # e.g., {\"feature_name\": 1/-1}\n",
    "    \"interaction_constraints\": [],\n",
    "    # Training controls\n",
    "    \"early_stopping_rounds\": 100,\n",
    "    \"verbosity\": 1,\n",
    "    \"seed\": 42,\n",
    "    \"nthread\": 4,  # alias: n_jobs\n",
    "}\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"book-recommendation\",\n",
    "    group=\"dev\",\n",
    "    job_type=\"train\",\n",
    "    save_code=True,\n",
    "    config=param,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3eebd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact = run.use_artifact(\"book-recommendation/completion_prediction.csv:latest\")\n",
    "artifact_path = artifact.file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85da447",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(artifact_path)\n",
    "\n",
    "print(\"Completion prediction model\")\n",
    "print(\"Task: Will user complete books they interact with?\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"Target: is_read (completion)\")\n",
    "print(f\"Features: {df.shape[1] - 1}\")\n",
    "\n",
    "# Check target distribution\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(df[\"is_read\"].value_counts())\n",
    "completion_rate = df[\"is_read\"].mean()\n",
    "print(f\"Overall completion rate: {completion_rate:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3532a076",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d256952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = df.iloc[:, 1:]\n",
    "y = df.iloc[:, 0]\n",
    "sz = df.shape\n",
    "\n",
    "# simple sequential split to keep any temporal order (if present)\n",
    "test_size = 0.2\n",
    "seed = 42\n",
    "split_idx = int(sz[0] * (1 - test_size))\n",
    "\n",
    "X_train = X.iloc[:split_idx, :]\n",
    "X_test = X.iloc[split_idx:, :]\n",
    "y_train = y.iloc[:split_idx]\n",
    "y_test = y.iloc[split_idx:]\n",
    "\n",
    "# Validation split from training (do not use test for early stopping)\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, stratify=y_train, random_state=seed\n",
    ")\n",
    "\n",
    "# Mirror key params into W&B (optional, param is already in wandb.config)\n",
    "wandb.config[\"data_shape\"] = sz\n",
    "\n",
    "print(\"Clean completion prediction setup with early stopping (from param)\")\n",
    "print(\"No leaky features (rating, review_length removed)\")\n",
    "print(\"No synthetic negatives\")\n",
    "print(\"Focus: Will user complete books they choose to interact with?\")\n",
    "\n",
    "# Map param dict to sklearn API\n",
    "eval_metric = param.get(\"eval_metric\", [\"aucpr\", \"logloss\"])\n",
    "if isinstance(eval_metric, str):\n",
    "    eval_metric_list = [eval_metric]\n",
    "else:\n",
    "    eval_metric_list = list(eval_metric)\n",
    "\n",
    "# Early stopping settings from param\n",
    "early_rounds = int(param.get(\"early_stopping_rounds\", 100))\n",
    "\n",
    "# Choose metric to early stop on (prefer aucpr if present)\n",
    "if \"aucpr\" in eval_metric_list:\n",
    "    es_metric = \"aucpr\"\n",
    "else:\n",
    "    es_metric = eval_metric_list[0] if len(eval_metric_list) > 0 else \"logloss\"\n",
    "maximize = es_metric not in [\n",
    "    \"rmse\",\n",
    "    \"mae\",\n",
    "    \"logloss\",\n",
    "    \"merror\",\n",
    "    \"mlogloss\",\n",
    "    \"poisson-nloglik\",\n",
    "]\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    WandbCallback(),\n",
    "    xgb.callback.EarlyStopping(\n",
    "        rounds=early_rounds,\n",
    "        metric_name=es_metric,\n",
    "        data_name=\"validation_1\",\n",
    "        save_best=True,\n",
    "        maximize=maximize,\n",
    "    ),\n",
    "]\n",
    "\n",
    "xgb_params = {\n",
    "    \"objective\": param.get(\"objective\", \"binary:logistic\"),\n",
    "    \"n_estimators\": int(param.get(\"n_estimators\", 2000)),\n",
    "    \"learning_rate\": float(param.get(\"eta\", 0.03)),\n",
    "    \"max_depth\": int(param.get(\"max_depth\", 6)),\n",
    "    \"min_child_weight\": float(param.get(\"min_child_weight\", 3)),\n",
    "    \"gamma\": float(param.get(\"gamma\", 0.0)),\n",
    "    \"reg_lambda\": float(param.get(\"reg_lambda\", 1.0)),\n",
    "    \"reg_alpha\": float(param.get(\"reg_alpha\", 0.0)),\n",
    "    \"subsample\": float(param.get(\"subsample\", 0.8)),\n",
    "    \"colsample_bytree\": float(param.get(\"colsample_bytree\", 0.8)),\n",
    "    \"colsample_bylevel\": float(param.get(\"colsample_bylevel\", 1.0)),\n",
    "    \"colsample_bynode\": float(param.get(\"colsample_bynode\", 1.0)),\n",
    "    \"booster\": param.get(\"booster\", \"gbtree\"),\n",
    "    \"tree_method\": param.get(\"tree_method\", \"hist\"),\n",
    "    \"grow_policy\": param.get(\"grow_policy\", \"depthwise\"),\n",
    "    \"max_bin\": int(param.get(\"max_bin\", 256)),\n",
    "    \"scale_pos_weight\": float(param.get(\"scale_pos_weight\", 1.0)),\n",
    "    \"n_jobs\": int(param.get(\"nthread\", -1)),\n",
    "    \"random_state\": int(param.get(\"seed\", seed)),\n",
    "    \"eval_metric\": eval_metric_list,\n",
    "    \"callbacks\": callbacks,\n",
    "}\n",
    "\n",
    "cls = xgb.XGBClassifier(**xgb_params)\n",
    "cls.fit(\n",
    "    X_tr,\n",
    "    y_tr,\n",
    "    eval_set=[(X_tr, y_tr), (X_val, y_val)],\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# get prediction on held-out test\n",
    "pred = cls.predict(X_test)\n",
    "error_rate = (pred != y_test).mean()\n",
    "\n",
    "# log metrics to wandb\n",
    "wandb.summary[\"Error Rate\"] = float(error_rate)\n",
    "# best iteration and score\n",
    "bst = cls.get_booster()\n",
    "if hasattr(bst, \"best_iteration\") and bst.best_iteration is not None:\n",
    "    run.summary[\"best_iteration\"] = int(bst.best_iteration)\n",
    "ev = cls.evals_result()\n",
    "if \"validation_1\" in ev and es_metric in ev[\"validation_1\"]:\n",
    "    run.summary[\"best_\" + es_metric] = float(\n",
    "        max(ev[\"validation_1\"][es_metric])\n",
    "        if maximize\n",
    "        else min(ev[\"validation_1\"][es_metric])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddbee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = cls.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d535d6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic classification metrics\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    average_precision_score,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "print(\"Basic classification metrics\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, preds):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, preds):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, preds):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, preds):.4f}\")\n",
    "\n",
    "# Get prediction probabilities for ranking\n",
    "y_proba = cls.predict_proba(X_test)[:, 1]\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n",
    "print(f\"PR-AUC: {average_precision_score(y_test, y_proba):.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, preds))\n",
    "\n",
    "# Confusion matrix with plotly\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Create confusion matrix heatmap\n",
    "fig_cm = go.Figure(\n",
    "    data=go.Heatmap(\n",
    "        z=cm,\n",
    "        x=[\"Predicted 0\", \"Predicted 1\"],\n",
    "        y=[\"Actual 0\", \"Actual 1\"],\n",
    "        colorscale=\"Blues\",\n",
    "        text=cm,\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 20},\n",
    "        showscale=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_cm.update_layout(\n",
    "    title=\"Confusion Matrix\",\n",
    "    xaxis_title=\"Predicted Label\",\n",
    "    yaxis_title=\"Actual Label\",\n",
    "    width=500,\n",
    "    height=400,\n",
    ")\n",
    "\n",
    "fig_cm.show()\n",
    "run.log({\"confusion_matrix\": fig_cm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bg6qpey6jp7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC and precision-recall curves\n",
    "\n",
    "# Calculate curves\n",
    "fpr, tpr, roc_thresholds = roc_curve(y_test, y_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "precision, recall, pr_thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = average_precision_score(y_test, y_proba)\n",
    "\n",
    "# Create subplots for ROC and PR curves\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=[\"ROC Curve\", \"Precision-Recall Curve\"],\n",
    "    specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}]],\n",
    ")\n",
    "\n",
    "# ROC curve\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=fpr,\n",
    "        y=tpr,\n",
    "        mode=\"lines\",\n",
    "        name=f\"ROC Curve (AUC = {roc_auc:.3f})\",\n",
    "        line=dict(color=\"blue\", width=2),\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "# Diagonal line for ROC\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[0, 1],\n",
    "        y=[0, 1],\n",
    "        mode=\"lines\",\n",
    "        name=\"Random Classifier\",\n",
    "        line=dict(color=\"red\", width=1, dash=\"dash\"),\n",
    "        showlegend=False,\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "# Precision-recall curve\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=recall,\n",
    "        y=precision,\n",
    "        mode=\"lines\",\n",
    "        name=f\"PR Curve (AUC = {pr_auc:.3f})\",\n",
    "        line=dict(color=\"green\", width=2),\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "# Baseline for PR curve\n",
    "baseline = y_test.sum() / len(y_test)  # Positive rate\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[0, 1],\n",
    "        y=[baseline, baseline],\n",
    "        mode=\"lines\",\n",
    "        name=f\"Random Baseline ({baseline:.3f})\",\n",
    "        line=dict(color=\"red\", width=1, dash=\"dash\"),\n",
    "        showlegend=False,\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_xaxes(title_text=\"False Positive Rate\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"True Positive Rate\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Recall\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Precision\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Model Performance Curves\", width=1000, height=400, showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "run.log({\"roc_pr_curves\": fig})\n",
    "\n",
    "print(\"Model performance summary:\")\n",
    "print(\n",
    "    f\"ROC-AUC: {roc_auc:.4f} ({'Excellent' if roc_auc > 0.9 else 'Good' if roc_auc > 0.8 else 'Fair' if roc_auc > 0.7 else 'Poor'})\"\n",
    ")\n",
    "print(\n",
    "    f\"PR-AUC: {pr_auc:.4f} ({'Excellent' if pr_auc > 0.8 else 'Good' if pr_auc > 0.6 else 'Fair' if pr_auc > 0.4 else 'Poor'})\"\n",
    ")\n",
    "print(f\"Baseline (random): {baseline:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7amzlkemjla",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction distribution analysis\n",
    "\n",
    "# Create prediction distribution plots\n",
    "fig_dist = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    subplot_titles=[\n",
    "        \"Prediction Probability Distribution\",\n",
    "        \"Prediction Probabilities by Class\",\n",
    "    ],\n",
    "    specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}]],\n",
    ")\n",
    "\n",
    "# Overall distribution\n",
    "fig_dist.add_trace(\n",
    "    go.Histogram(\n",
    "        x=y_proba,\n",
    "        nbinsx=50,\n",
    "        name=\"All Predictions\",\n",
    "        opacity=0.7,\n",
    "        marker_color=\"lightblue\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "# Distribution by class\n",
    "fig_dist.add_trace(\n",
    "    go.Histogram(\n",
    "        x=y_proba[y_test == 0],\n",
    "        nbinsx=30,\n",
    "        name=\"Negative Class (0)\",\n",
    "        opacity=0.7,\n",
    "        marker_color=\"red\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "fig_dist.add_trace(\n",
    "    go.Histogram(\n",
    "        x=y_proba[y_test == 1],\n",
    "        nbinsx=30,\n",
    "        name=\"Positive Class (1)\",\n",
    "        opacity=0.7,\n",
    "        marker_color=\"green\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "fig_dist.update_xaxes(title_text=\"Prediction Probability\", row=1, col=1)\n",
    "fig_dist.update_yaxes(title_text=\"Count\", row=1, col=1)\n",
    "fig_dist.update_xaxes(title_text=\"Prediction Probability\", row=1, col=2)\n",
    "fig_dist.update_yaxes(title_text=\"Count\", row=1, col=2)\n",
    "\n",
    "fig_dist.update_layout(\n",
    "    title=\"Prediction Probability Distributions\",\n",
    "    width=1000,\n",
    "    height=400,\n",
    "    showlegend=True,\n",
    "    barmode=\"overlay\",  # For overlapping histograms in the second subplot\n",
    ")\n",
    "\n",
    "fig_dist.show()\n",
    "run.log({\"prediction_distributions\": fig_dist})\n",
    "\n",
    "# Calculate separation metrics\n",
    "mean_pos = y_proba[y_test == 1].mean()\n",
    "mean_neg = y_proba[y_test == 0].mean()\n",
    "separation = abs(mean_pos - mean_neg)\n",
    "\n",
    "print(\"Prediction analysis:\")\n",
    "print(f\"Mean probability for positive class: {mean_pos:.4f}\")\n",
    "print(f\"Mean probability for negative class: {mean_neg:.4f}\")\n",
    "print(\n",
    "    f\"Class separation: {separation:.4f} ({'Good' if separation > 0.3 else 'Moderate' if separation > 0.1 else 'Poor'})\"\n",
    ")\n",
    "print(\"Optimal threshold (balanced): 0.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59mybjjffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load completion prediction mappings\n",
    "user_book_mapping = pd.read_csv(\"../data/completion_user_book_mapping.csv\")\n",
    "\n",
    "print(\"=== Completion Prediction Evaluation ===\")\n",
    "print(\"Task: Predicting if users complete books they interact with\")\n",
    "print(f\"Mapping shape: {user_book_mapping.shape}\")\n",
    "\n",
    "# Create test mapping for completion prediction\n",
    "test_indices = X_test.index\n",
    "test_mapping = user_book_mapping.iloc[test_indices].copy()\n",
    "test_mapping[\"prediction_score\"] = y_proba\n",
    "test_mapping[\"true_completion\"] = y_test.values  # is_read target\n",
    "\n",
    "print(f\"Test mapping created: {len(test_mapping)} samples\")\n",
    "print(f\"Users in test: {test_mapping['user_id'].nunique()}\")\n",
    "print(f\"Books completed in test: {test_mapping['true_completion'].sum()}\")\n",
    "\n",
    "test_mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfqmty6xr0m",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking evaluation functions\n",
    "\n",
    "\n",
    "def compute_precision_at_k(user_data, k):\n",
    "    \"\"\"Compute Precision@k for a single user\"\"\"\n",
    "    top_k = user_data.head(k)\n",
    "    return top_k[\"true_completion\"].sum() / k\n",
    "\n",
    "\n",
    "def compute_recall_at_k(user_data, k):\n",
    "    \"\"\"Compute Recall@k for a single user\"\"\"\n",
    "    top_k = user_data.head(k)\n",
    "    total_relevant = user_data[\"true_completion\"].sum()\n",
    "    if total_relevant == 0:\n",
    "        return 0.0\n",
    "    return top_k[\"true_completion\"].sum() / total_relevant\n",
    "\n",
    "\n",
    "def compute_ap_at_k(user_data, k):\n",
    "    \"\"\"Compute Average Precision@k for a single user\"\"\"\n",
    "    top_k = user_data.head(k)\n",
    "    y_true = top_k[\"true_completion\"].values\n",
    "    y_scores = top_k[\"prediction_score\"].values\n",
    "\n",
    "    if y_true.sum() == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return average_precision_score(y_true, y_scores)\n",
    "\n",
    "\n",
    "def compute_ndcg_at_k(user_data, k):\n",
    "    \"\"\"Compute NDCG@k for a single user\"\"\"\n",
    "    from sklearn.metrics import ndcg_score\n",
    "\n",
    "    top_k = user_data.head(k)\n",
    "    y_true = top_k[\"true_completion\"].values.reshape(1, -1)\n",
    "    y_scores = top_k[\"prediction_score\"].values.reshape(1, -1)\n",
    "\n",
    "    if len(y_true[0]) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return ndcg_score(y_true, y_scores, k=k)\n",
    "\n",
    "\n",
    "def evaluate_ranking_metrics(test_mapping, k_values=[5, 10, 20, 50]):\n",
    "    \"\"\"Compute comprehensive ranking metrics\"\"\"\n",
    "\n",
    "    metrics = {f\"precision@{k}\": [] for k in k_values}\n",
    "    metrics.update({f\"recall@{k}\": [] for k in k_values})\n",
    "    metrics.update({f\"map@{k}\": [] for k in k_values})\n",
    "    metrics.update({f\"ndcg@{k}\": [] for k in k_values})\n",
    "\n",
    "    users_evaluated = 0\n",
    "\n",
    "    for user_id in test_mapping[\"user_id\"].unique():\n",
    "        user_data = test_mapping[test_mapping[\"user_id\"] == user_id]\n",
    "\n",
    "        # Skip users with no positive items in test\n",
    "        if user_data[\"true_completion\"].sum() == 0:\n",
    "            continue\n",
    "\n",
    "        # Sort by prediction score (highest first)\n",
    "        user_data = user_data.sort_values(\"prediction_score\", ascending=False)\n",
    "        users_evaluated += 1\n",
    "\n",
    "        # Compute metrics for each k\n",
    "        for k in k_values:\n",
    "            if len(user_data) >= k:  # Only compute if user has enough items\n",
    "                metrics[f\"precision@{k}\"].append(compute_precision_at_k(user_data, k))\n",
    "                metrics[f\"recall@{k}\"].append(compute_recall_at_k(user_data, k))\n",
    "                metrics[f\"map@{k}\"].append(compute_ap_at_k(user_data, k))\n",
    "                metrics[f\"ndcg@{k}\"].append(compute_ndcg_at_k(user_data, k))\n",
    "\n",
    "    # Average across users\n",
    "    results = {}\n",
    "    for metric_name, values in metrics.items():\n",
    "        if values:  # Only compute average if we have values\n",
    "            results[metric_name] = np.mean(values)\n",
    "        else:\n",
    "            results[metric_name] = 0.0\n",
    "\n",
    "    results[\"users_evaluated\"] = users_evaluated\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Computing ranking metrics\")\n",
    "ranking_results = evaluate_ranking_metrics(test_mapping, k_values=[5, 10, 20, 50])\n",
    "\n",
    "print(f\"Users evaluated: {ranking_results['users_evaluated']}\")\n",
    "print(\"\\nRanking metrics:\")\n",
    "for metric, value in ranking_results.items():\n",
    "    if metric != \"users_evaluated\":\n",
    "        print(f\"{metric.upper()}: {value:.4f}\")\n",
    "# Log ranking metrics to Weights & Biases\n",
    "_rank_metrics = {\"ranking/users_evaluated\": ranking_results.get(\"users_evaluated\", 0)}\n",
    "for _k in [5, 10, 20, 50]:\n",
    "    _rank_metrics.update(\n",
    "        {\n",
    "            f\"ranking/precision@{_k}\": float(\n",
    "                ranking_results.get(f\"precision@{_k}\", 0.0)\n",
    "            ),\n",
    "            f\"ranking/recall@{_k}\": float(ranking_results.get(f\"recall@{_k}\", 0.0)),\n",
    "            f\"ranking/map@{_k}\": float(ranking_results.get(f\"map@{_k}\", 0.0)),\n",
    "            f\"ranking/ndcg@{_k}\": float(ranking_results.get(f\"ndcg@{_k}\", 0.0)),\n",
    "        }\n",
    "    )\n",
    "run.log(_rank_metrics)\n",
    "\n",
    "# Also log a compact table\n",
    "rank_table = wandb.Table(columns=[\"k\", \"precision\", \"recall\", \"map\", \"ndcg\"])\n",
    "for _k in [5, 10, 20, 50]:\n",
    "    rank_table.add_data(\n",
    "        _k,\n",
    "        float(ranking_results.get(f\"precision@{_k}\", 0.0)),\n",
    "        float(ranking_results.get(f\"recall@{_k}\", 0.0)),\n",
    "        float(ranking_results.get(f\"map@{_k}\", 0.0)),\n",
    "        float(ranking_results.get(f\"ndcg@{_k}\", 0.0)),\n",
    "    )\n",
    "run.log({\"ranking/metrics_table\": rank_table})\n",
    "\n",
    "# Promote commonly tracked ones to summary\n",
    "for _key in [\"precision@20\", \"recall@20\", \"map@20\", \"ndcg@20\"]:\n",
    "    run.summary[f\"ranking/{_key}\"] = float(ranking_results.get(_key, 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y59p6ibncd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "\n",
    "print(\"Feature importance analysis\")\n",
    "\n",
    "# Get feature importance from XGBoost\n",
    "feature_importance = cls.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Create importance DataFrame\n",
    "importance_df = pd.DataFrame(\n",
    "    {\"feature\": feature_names, \"importance\": feature_importance}\n",
    ").sort_values(\"importance\", ascending=False)\n",
    "\n",
    "print(\"Top 15 most important features:\")\n",
    "print(importance_df.head(15))\n",
    "\n",
    "# Plot feature importance with Plotly\n",
    "top_features = importance_df.head(20)\n",
    "\n",
    "fig = go.Figure(\n",
    "    go.Bar(\n",
    "        x=top_features[\"importance\"],\n",
    "        y=top_features[\"feature\"],\n",
    "        orientation=\"h\",\n",
    "        marker_color=\"lightblue\",\n",
    "        text=top_features[\"importance\"].round(4),\n",
    "        textposition=\"auto\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Top 20 Feature Importances (XGBoost)\",\n",
    "    xaxis_title=\"Feature Importance\",\n",
    "    yaxis_title=\"Features\",\n",
    "    height=600,\n",
    "    width=900,\n",
    "    yaxis={\"categoryorder\": \"total ascending\"},\n",
    "    showlegend=False,\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "run.log({\"feature_importance\": fig})\n",
    "\n",
    "# Save feature importance\n",
    "importance_df.to_csv(\"../data/feature_importance.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0r24w9sx2ese",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance report\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BOOK RECOMMENDATION SYSTEM - PERFORMANCE REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\"\"\n",
    "DATASET OVERVIEW:\n",
    "   • Total samples: {len(df):,}\n",
    "   • Training samples: {len(X_train):,}\n",
    "   • Test samples: {len(X_test):,}\n",
    "   • Features: {X.shape[1]}\n",
    "   • Unique users in test: {test_mapping[\"user_id\"].nunique():,}\n",
    "   • Users evaluated for ranking: {ranking_results[\"users_evaluated\"]:,}\n",
    "\n",
    "CLASSIFICATION PERFORMANCE:\n",
    "   • Accuracy: {accuracy_score(y_test, preds):.4f}\n",
    "   • Precision: {precision_score(y_test, preds):.4f}\n",
    "   • Recall: {recall_score(y_test, preds):.4f}\n",
    "   • F1-Score: {f1_score(y_test, preds):.4f}\n",
    "   • ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\n",
    "\n",
    "RANKING PERFORMANCE:\"\"\")\n",
    "\n",
    "# Format ranking metrics nicely\n",
    "metrics_by_k = {}\n",
    "for k in [5, 10, 20, 50]:\n",
    "    metrics_by_k[k] = {\n",
    "        \"Precision\": ranking_results[f\"precision@{k}\"],\n",
    "        \"Recall\": ranking_results[f\"recall@{k}\"],\n",
    "        \"mAP\": ranking_results[f\"map@{k}\"],\n",
    "        \"NDCG\": ranking_results[f\"ndcg@{k}\"],\n",
    "    }\n",
    "\n",
    "print(\"   ┌─────────┬──────────┬─────────┬─────────┬─────────┐\")\n",
    "print(\"   │    k    │ Precision│  Recall │   mAP   │  NDCG   │\")\n",
    "print(\"   ├─────────┼──────────┼─────────┼─────────┼─────────┤\")\n",
    "for k in [5, 10, 20, 50]:\n",
    "    metrics = metrics_by_k[k]\n",
    "    print(\n",
    "        f\"   │   @{k:2d}   │  {metrics['Precision']:.4f}  │ {metrics['Recall']:.4f}  │ {metrics['mAP']:.4f}  │ {metrics['NDCG']:.4f}  │\"\n",
    "    )\n",
    "print(\"   └─────────┴──────────┴─────────┴─────────┴─────────┘\")\n",
    "\n",
    "print(\"\"\"\n",
    "TOP PREDICTIVE FEATURES:\"\"\")\n",
    "for i, (feature, importance) in enumerate(importance_df.head(10).values, 1):\n",
    "    print(f\"   {i:2d}. {feature:<25} ({importance:.4f})\")\n",
    "\n",
    "print(f\"\"\"\n",
    "RECOMMENDATION QUALITY INSIGHTS:\n",
    "   • mAP@20 of {ranking_results[\"map@20\"]:.4f} indicates {\"excellent\" if ranking_results[\"map@20\"] > 0.3 else \"good\" if ranking_results[\"map@20\"] > 0.1 else \"moderate\"} ranking quality\n",
    "   • Precision@20 of {ranking_results[\"precision@20\"]:.4f} means {ranking_results[\"precision@20\"] * 100:.1f}% of top-20 recommendations are relevant\n",
    "   • Model successfully learns from {X.shape[1]} engineered features\n",
    "   • {importance_df.head(5)[\"feature\"].str.contains(\"similarity\").sum()} of top-5 features are similarity-based\n",
    "\n",
    "BUSINESS IMPACT:\n",
    "   • For every 20 books recommended, ~{ranking_results[\"precision@20\"] * 20:.0f} will be relevant to the user\n",
    "   • {ranking_results[\"recall@20\"] * 100:.1f}% of relevant books are captured in top-20 recommendations\n",
    "   • Strong classification performance (AUC: {roc_auc_score(y_test, y_proba):.3f}) enables confident ranking\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save report to file\n",
    "report_text = f\"\"\"Book Recommendation System Performance Report\n",
    "Generated: {pd.Timestamp.now()}\n",
    "\n",
    "Dataset Overview:\n",
    "- Total samples: {len(df):,}\n",
    "- Training samples: {len(X_train):,}\n",
    "- Test samples: {len(X_test):,}\n",
    "- Features: {X.shape[1]}\n",
    "- Users evaluated: {ranking_results[\"users_evaluated\"]:,}\n",
    "\n",
    "Classification Metrics:\n",
    "- Accuracy: {accuracy_score(y_test, preds):.4f}\n",
    "- Precision: {precision_score(y_test, preds):.4f}\n",
    "- Recall: {recall_score(y_test, preds):.4f}\n",
    "- F1-Score: {f1_score(y_test, preds):.4f}\n",
    "- ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\n",
    "\n",
    "Ranking Metrics:\n",
    "\"\"\"\n",
    "\n",
    "for k in [5, 10, 20, 50]:\n",
    "    report_text += f\"@{k}: Precision={ranking_results[f'precision@{k}']:.4f}, Recall={ranking_results[f'recall@{k}']:.4f}, mAP={ranking_results[f'map@{k}']:.4f}, NDCG={ranking_results[f'ndcg@{k}']:.4f}\\n\"\n",
    "\n",
    "report_text += \"\\nTop 10 Features:\\n\"\n",
    "for i, (feature, importance) in enumerate(importance_df.head(10).values, 1):\n",
    "    report_text += f\"{i}. {feature}: {importance:.4f}\\n\"\n",
    "\n",
    "# Save to file\n",
    "with open(\"../data/model_performance_report.txt\", \"w\") as f:\n",
    "    f.write(report_text)\n",
    "\n",
    "print(\"Detailed report saved to: ../data/model_performance_report.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fgqd81r4ugu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and artifacts\n",
    "\n",
    "import joblib\n",
    "\n",
    "print(\"Saving model and artifacts\")\n",
    "\n",
    "# Save the trained model\n",
    "model_path = \"../models/xgboost_recommender_model.pkl\"\n",
    "joblib.dump(cls, model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Save test results for further analysis\n",
    "test_results_path = \"../data/test_results.csv\"\n",
    "test_mapping.to_csv(test_results_path, index=False)\n",
    "print(f\"Test results saved to: {test_results_path}\")\n",
    "\n",
    "# Save ranking metrics summary\n",
    "ranking_summary = pd.DataFrame([ranking_results]).T\n",
    "ranking_summary.columns = [\"value\"]\n",
    "ranking_summary_path = \"../data/ranking_metrics_summary.csv\"\n",
    "ranking_summary.to_csv(ranking_summary_path)\n",
    "print(f\"Ranking metrics saved to: {ranking_summary_path}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "Training complete.\n",
    "\n",
    "Generated files:\n",
    "   • Model: {model_path}\n",
    "   • Test Results: {test_results_path}\n",
    "   • Feature Importance: ../data/feature_importance.csv\n",
    "   • Performance Report: ../data/model_performance_report.txt\n",
    "   • Ranking Metrics: {ranking_summary_path}\n",
    "\"\"\")\n",
    "\n",
    "# Save training data split and upload as artifact\n",
    "train_df = pd.concat([y_train.rename(\"is_read\"), X_train], axis=1)\n",
    "train_data_filename = \"train_data.csv\"\n",
    "train_data_path = os.path.join(\"..\", \"data\", train_data_filename)\n",
    "train_df.to_csv(train_data_path, index=False)\n",
    "print(f\"Training data saved to: {train_data_path}\")\n",
    "train_art = wandb.Artifact(\n",
    "    name=train_data_filename,\n",
    "    type=\"training_data\",\n",
    "    description=\"Book completion prediction training data\",\n",
    ")\n",
    "train_art.add_file(train_data_path)\n",
    "run.log_artifact(train_art)\n",
    "\n",
    "# Upload saved files as artifacts\n",
    "files_to_upload = [\n",
    "    (model_path, \"model\", \"XGBoost recommender model\"),\n",
    "    (\"../data/feature_importance.csv\", \"dataset\", \"XGBoost feature importances\"),\n",
    "    (test_results_path, \"dataset\", \"Test results CSV\"),\n",
    "    (ranking_summary_path, \"dataset\", \"Ranking metrics summary\"),\n",
    "    (\"../data/model_performance_report.txt\", \"report\", \"Text performance report\"),\n",
    "]\n",
    "for path, atype, desc in files_to_upload:\n",
    "    base = os.path.basename(path)\n",
    "    art = wandb.Artifact(name=os.path.splitext(base)[0], type=atype, description=desc)\n",
    "    art.add_file(path)\n",
    "    run.log_artifact(art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98395a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
