{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efb40310",
   "metadata": {},
   "source": [
    "# IMDB Sentiment Analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates building a sentiment classifier using simple text classification with BERT. We use the IMDb Movie Reviews dataset built into TensorFlow for our training and validation data. The output is a classifier that can take a text input and classify it as 'Positive' or 'Negative'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c42e80b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm;\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6ea350",
   "metadata": {},
   "source": [
    "### Text Encoder\n",
    "First we generate a text encoder that takes a sequence of input IDs (created via a BERT tokenizer) and outputs embeddings of a desired dimension (512 in this case).\n",
    "Some notes about the encoder implementation:\n",
    "* We use the pretrained 'distilbert-base-uncased' model to avoid training our own.\n",
    "* The projection layer takes the output of the BERT embeddings and projects them to our desired dimension.\n",
    "* We are interested in the first-token ([CLS]) embedding, as it represents sentence meaning.\n",
    "* We use L2 normalization to ensure the length of each vector is = 1.0. This prepares our vectors for cosine similarity analysis with other normalized vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cff0059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=\"distilbert-base-uncased\",\n",
    "        embedding_dim=512,\n",
    "        freeze_bert=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size, embedding_dim),\n",
    "            nn.LayerNorm(embedding_dim),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "\n",
    "        # use [CLS] token\n",
    "        cls_embedding = outputs.hidden_states[-1][:, 0]\n",
    "\n",
    "        # project to desired dimension\n",
    "        text_embedding = self.projection(cls_embedding)\n",
    "\n",
    "        # l2 normalize\n",
    "        text_embedding = F.normalize(text_embedding, p=2, dim=1)\n",
    "\n",
    "        return text_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad422d61",
   "metadata": {},
   "source": [
    "### Text Classifier\n",
    "Next we generate the text classifier. This uses the encoder we built previously to compute the text embeddings for the IMDb reviews. We finally pass these embeddings through a Dense classifier with dimension = 2 to represent Positive or Negative sentiment. The logits are returned by calling the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f845ddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=2, embedding_dim=512, **kwargs):\n",
    "        super().__init__()\n",
    "        self.text_encoder = TextEncoder(embedding_dim=embedding_dim, freeze_bert=True)\n",
    "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        text_embeddings = self.text_encoder(input_ids, attention_mask)\n",
    "        logits = self.classifier(text_embeddings)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b021561",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "Load the IMDb movie reviews dataset and prepare for consumption from our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6c8867c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imdb_data(max_train_examples=20_000, max_test_examples=500):\n",
    "    # Load dataset using Hugging Face datasets\n",
    "    dataset = load_dataset(\"imdb\")\n",
    "\n",
    "    print(\"Dataset info: IMDB Reviews - 25k train, 25k test examples\")\n",
    "    print(\"Features: text (string), label (0=negative, 1=positive)\")\n",
    "\n",
    "    # convert to lists for handling\n",
    "    train_examples = []\n",
    "    for i, example in enumerate(dataset[\"train\"]):\n",
    "        if i >= max_train_examples:\n",
    "            break\n",
    "        train_examples.append((example[\"text\"], example[\"label\"]))\n",
    "\n",
    "    test_examples = []\n",
    "    for i, example in enumerate(dataset[\"test\"]):\n",
    "        if i >= max_test_examples:\n",
    "            break\n",
    "        test_examples.append((example[\"text\"], example[\"label\"]))\n",
    "\n",
    "    print(f\"Loaded {len(train_examples)} training examples\")\n",
    "    print(f\"Loaded {len(test_examples)} test examples\")\n",
    "\n",
    "    # show examples\n",
    "    print(\"\\nExample training data:\")\n",
    "    for i in range(5):\n",
    "        text, label = train_examples[i]\n",
    "        sentiment = \"Positive\" if label == 1 else \"Negative\"\n",
    "        print(f\"{sentiment}: {text[:100]}\")\n",
    "\n",
    "    return train_examples, test_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b3238d",
   "metadata": {},
   "source": [
    "### Data Preprocessor\n",
    "We preprocess the text data by passing the text inputs through a tokenizer. Texts are contained to a max length of 128 with padding enabled. Texts over 128 are truncated, and texts shorter have a padding token added (0) to maintain the shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55905727",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self, tokenizer, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def create_dataloader(self, text_label_pairs, batch_size=32, shuffle=True):\n",
    "        texts, labels = zip(*text_label_pairs)\n",
    "\n",
    "        dataset = IMDBDataset(texts, labels, self.tokenizer, self.max_length)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "        return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d105a9d",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "Next we create the training function. This takes the tokenizer and classification model and runs it through our preprocess -> compile -> train pipeline. We return the model and the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067fc91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_simple_classifier(\n",
    "    train_data, batch_size=32, num_epochs=3, learning_rate=2e-5\n",
    "):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # split training data into train/validation\n",
    "    train_split, val_split = train_test_split(\n",
    "        train_data, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(\n",
    "        f\"Training on {len(train_split)} samples, validating on {len(val_split)} samples\"\n",
    "    )\n",
    "\n",
    "    # sSet device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Training on device: {device}\")\n",
    "\n",
    "    # initialize tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    model = TextClassifier(num_classes=2, embedding_dim=512).to(device)\n",
    "\n",
    "    # create dataloaders\n",
    "    preprocessor = TextPreprocessor(tokenizer=tokenizer, max_length=128)\n",
    "    train_dataloader = preprocessor.create_dataloader(\n",
    "        train_split, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    val_dataloader = preprocessor.create_dataloader(\n",
    "        val_split, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    # setup optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=learning_rate, weight_decay=1e-3\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # early stopping parameters\n",
    "    best_val_acc = 0\n",
    "    patience = 3\n",
    "    patience_counter = 0\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        train_progress = tqdm(\n",
    "            train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Train]\"\n",
    "        )\n",
    "        for batch in train_progress:\n",
    "            # move batch to device\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            # forward pass\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # calculate accuracy\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            train_correct += (predictions == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # update progress bar\n",
    "            current_acc = train_correct / train_total\n",
    "            train_progress.set_postfix(\n",
    "                {\"loss\": f\"{loss.item():.4f}\", \"acc\": f\"{current_acc:.4f}\"}\n",
    "            )\n",
    "\n",
    "        # validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_progress = tqdm(\n",
    "                val_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Val]\"\n",
    "            )\n",
    "            for batch in val_progress:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"label\"].to(device)\n",
    "\n",
    "                logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                val_correct += (predictions == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                current_val_acc = val_correct / val_total\n",
    "                val_progress.set_postfix(\n",
    "                    {\n",
    "                        \"val_loss\": f\"{loss.item():.4f}\",\n",
    "                        \"val_acc\": f\"{current_val_acc:.4f}\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # calculate epoch metrics\n",
    "        train_loss = total_train_loss / len(train_dataloader)\n",
    "        train_acc = train_correct / train_total\n",
    "        val_loss = total_val_loss / len(val_dataloader)\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # early stopping check\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            print(f\"  New best validation accuracy: {best_val_acc:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  No improvement. Patience: {patience_counter}/{patience}\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping after {epoch + 1} epochs\")\n",
    "            break\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d236bdde",
   "metadata": {},
   "source": [
    "### Testing our Model\n",
    "The testing function takes a set of test texts and predicts the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dc23463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(test_texts, model, tokenizer):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "\n",
    "    # tokenize test texts\n",
    "    encodings = tokenizer(\n",
    "        test_texts,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # move to device\n",
    "    input_ids = encodings[\"input_ids\"].to(device)\n",
    "    attention_mask = encodings[\"attention_mask\"].to(device)\n",
    "\n",
    "    # predict\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        probabilities = F.softmax(logits, dim=1)\n",
    "\n",
    "    print(\"Test Results:\")\n",
    "    for i, text in enumerate(test_texts):\n",
    "        pos_prob = probabilities[i][1].item()\n",
    "        sentiment = \"Positive\" if pos_prob > 0.5 else \"Negative\"\n",
    "        print(f\"'{text}' -> {sentiment} (confidence: {pos_prob:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43adb5ab",
   "metadata": {},
   "source": [
    "### Run the Test and Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "653ab618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training simple text classifier...\n",
      "Dataset info: IMDB Reviews - 25k train, 25k test examples\n",
      "Features: text (string), label (0=negative, 1=positive)\n",
      "Loaded 20000 training examples\n",
      "Loaded 1000 test examples\n",
      "\n",
      "Example training data:\n",
      "Negative: I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it w\n",
      "Negative: \"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn't matter what one's poli\n",
      "Negative: If only to avoid making this type of film in the future. This film is interesting as an experiment b\n",
      "Negative: This film was probably inspired by Godard's Masculin, féminin and I urge you to see that film instea\n",
      "Negative: Oh, brother...after hearing about this ridiculous film for umpteen years all I can think of is that \n",
      "Training on 16000 samples, validating on 4000 samples\n",
      "Training on device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/5 [Train]: 100%|██████████| 500/500 [09:31<00:00,  1.14s/it, loss=0.5323, acc=0.6532]\n",
      "Epoch 1/5 [Val]: 100%|██████████| 125/125 [01:18<00:00,  1.59it/s, val_loss=0.5182, val_acc=0.7710]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "  Train Loss: 0.6378, Train Acc: 0.6532\n",
      "  Val Loss: 0.5559, Val Acc: 0.7710\n",
      "  New best validation accuracy: 0.7710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 [Train]: 100%|██████████| 500/500 [08:23<00:00,  1.01s/it, loss=0.4757, acc=0.7827]\n",
      "Epoch 2/5 [Val]: 100%|██████████| 125/125 [01:13<00:00,  1.70it/s, val_loss=0.4722, val_acc=0.8057]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "  Train Loss: 0.5457, Train Acc: 0.7827\n",
      "  Val Loss: 0.4844, Val Acc: 0.8057\n",
      "  New best validation accuracy: 0.8057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 [Train]: 100%|██████████| 500/500 [08:22<00:00,  1.01s/it, loss=0.5280, acc=0.7999]\n",
      "Epoch 3/5 [Val]: 100%|██████████| 125/125 [01:11<00:00,  1.76it/s, val_loss=0.4235, val_acc=0.8095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "  Train Loss: 0.4975, Train Acc: 0.7999\n",
      "  Val Loss: 0.4509, Val Acc: 0.8095\n",
      "  New best validation accuracy: 0.8095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 [Train]: 100%|██████████| 500/500 [11:14<00:00,  1.35s/it, loss=0.3745, acc=0.8025]\n",
      "Epoch 4/5 [Val]: 100%|██████████| 125/125 [01:22<00:00,  1.51it/s, val_loss=0.4127, val_acc=0.8117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\n",
      "  Train Loss: 0.4716, Train Acc: 0.8025\n",
      "  Val Loss: 0.4335, Val Acc: 0.8117\n",
      "  New best validation accuracy: 0.8117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 [Train]: 100%|██████████| 500/500 [09:02<00:00,  1.09s/it, loss=0.4312, acc=0.8084]\n",
      "Epoch 5/5 [Val]: 100%|██████████| 125/125 [01:34<00:00,  1.32it/s, val_loss=0.4074, val_acc=0.8133]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:\n",
      "  Train Loss: 0.4517, Train Acc: 0.8084\n",
      "  Val Loss: 0.4238, Val Acc: 0.8133\n",
      "  New best validation accuracy: 0.8133\n",
      "\n",
      "Testing model...\n",
      "Test Results:\n",
      "'I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn't match the background, and painfully one-dimensional characters cannot be overcome with a 'sci-fi' setting. (I'm sure there are those of you out there who think Babylon 5 is good sci-fi TV. It's not. It's clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It's really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it's rubbish as they have to always say \"Gene Roddenberry's Earth...\" otherwise people would not continue watching. Roddenberry's ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.' -> Negative (confidence: 0.151)\n",
      "'Worth the entertainment value of a rental, especially if you like action movies. This one features the usual car chases, fights with the great Van Damme kick style, shooting battles with the 40 shell load shotgun, and even terrorist style bombs. All of this is entertaining and competently handled but there is nothing that really blows you away if you've seen your share before.<br /><br />The plot is made interesting by the inclusion of a rabbit, which is clever but hardly profound. Many of the characters are heavily stereotyped -- the angry veterans, the terrified illegal aliens, the crooked cops, the indifferent feds, the bitchy tough lady station head, the crooked politician, the fat federale who looks like he was typecast as the Mexican in a Hollywood movie from the 1940s. All passably acted but again nothing special.<br /><br />I thought the main villains were pretty well done and fairly well acted. By the end of the movie you certainly knew who the good guys were and weren't. There was an emotional lift as the really bad ones got their just deserts. Very simplistic, but then you weren't expecting Hamlet, right? The only thing I found really annoying was the constant cuts to VDs daughter during the last fight scene.<br /><br />Not bad. Not good. Passable 4.' -> Positive (confidence: 0.750)\n",
      "'its a totally average film with a few semi-alright action sequences that make the plot seem a little better and remind the viewer of the classic van dam films. parts of the plot don't make sense and seem to be added in to use up time. the end plot is that of a very basic type that doesn't leave the viewer guessing and any twists are obvious from the beginning. the end scene with the flask backs don't make sense as they are added in and seem to have little relevance to the history of van dam's character. not really worth watching again, bit disappointed in the end production, even though it is apparent it was shot on a low budget certain shots and sections in the film are of poor directed quality' -> Negative (confidence: 0.120)\n",
      "'STAR RATING: ***** Saturday Night **** Friday Night *** Friday Morning ** Sunday Night * Monday Morning <br /><br />Former New Orleans homicide cop Jack Robideaux (Jean Claude Van Damme) is re-assigned to Columbus, a small but violent town in Mexico to help the police there with their efforts to stop a major heroin smuggling operation into their town. The culprits turn out to be ex-military, lead by former commander Benjamin Meyers (Stephen Lord, otherwise known as Jase from East Enders) who is using a special method he learned in Afghanistan to fight off his opponents. But Jack has a more personal reason for taking him down, that draws the two men into an explosive final showdown where only one will walk away alive.<br /><br />After Until Death, Van Damme appeared to be on a high, showing he could make the best straight to video films in the action market. While that was a far more drama oriented film, with The Shepherd he has returned to the high-kicking, no brainer action that first made him famous and has sadly produced his worst film since Derailed. It's nowhere near as bad as that film, but what I said still stands.<br /><br />A dull, predictable film, with very little in the way of any exciting action. What little there is mainly consists of some limp fight scenes, trying to look cool and trendy with some cheap slo-mo/sped up effects added to them that sadly instead make them look more desperate. Being a Mexican set film, director Isaac Florentine has tried to give the film a Robert Rodriguez/Desperado sort of feel, but this only adds to the desperation.<br /><br />VD gives a particularly uninspired performance and given he's never been a Robert De Niro sort of actor, that can't be good. As the villain, Lord shouldn't expect to leave the beeb anytime soon. He gets little dialogue at the beginning as he struggles to muster an American accent but gets mysteriously better towards the end. All the supporting cast are equally bland, and do nothing to raise the films spirits at all.<br /><br />This is one shepherd that's strayed right from the flock. *' -> Negative (confidence: 0.465)\n",
      "'First off let me say, If you haven't enjoyed a Van Damme movie since bloodsport, you probably will not like this movie. Most of these movies may not have the best plots or best actors but I enjoy these kinds of movies for what they are. This movie is much better than any of the movies the other action guys (Segal and Dolph) have thought about putting out the past few years. Van Damme is good in the movie, the movie is only worth watching to Van Damme fans. It is not as good as Wake of Death (which i highly recommend to anyone of likes Van Damme) or In hell but, in my opinion it's worth watching. It has the same type of feel to it as Nowhere to Run. Good fun stuff!' -> Negative (confidence: 0.320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Training simple text classifier...\")\n",
    "train_data, test_data = load_imdb_data(\n",
    "    max_train_examples=20_000, max_test_examples=1_000\n",
    ")\n",
    "model, tokenizer = train_simple_classifier(train_data, num_epochs=5)\n",
    "\n",
    "print(\"\\nTesting model...\")\n",
    "test_texts = [d[0] for d in test_data[:5]]  # Test on first 5 test examples\n",
    "test_model(test_texts, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30df30a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
